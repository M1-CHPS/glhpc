{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GLHPC - Introduction","text":"<p>This course is intended for M1 Calcul Haute Performance et simulation (M1CHPS) students as a crash course for software engineering targetting High Performance Computing and Artificial Intelligence applications.</p> <p>This course will cover programming basics in C and shell, software engineering best practices, and will gradually move on to more complex notions such as parallelism, performance profiling, experimental design, and the implementation of a Neural Network inference engince from scratch.</p>"},{"location":"#advanced-optional-tasks","title":"Advanced optional tasks","text":"<p>This course starts from the basics as some of the students come from applied maths and physics backgrounds. For those of you, who already have a good programming background, we have included optional advanced tasks in each lab to deepen your knowledge. You are not required to do them, but you are strongly encouraged.</p>"},{"location":"lab1/","title":"Lab 1 - Prerequisites - Linux, the shell, Git","text":""},{"location":"lab1/#objective","title":"Objective","text":"<p>In this lab, you will get familiar with the very basics of using the Linux shell, installing and using a code editor, and setting up git.</p> <p>The final section of this lab will have you combine all these tools to set up a simple project for a Hello World program in C.</p>"},{"location":"lab1/#0-linux","title":"0 - Linux","text":"<p>Linux is a family of Operating Systems (like Windows or MacOS) that are suited for programming. Most high-performance clusters will run a version of Linux, and as such it is mandatory that you learn how to use it.</p> <p>If you have a personal laptop, I highly recommend you set up Linux (or MacOS) on it. Alternatively, you can use Docker or a virtual machine on Windows, but note that this is highly impractical. Lastly, you can set up and use WSL.</p> <p>Tip</p> <p>The university may be able to lend laptops while you're on-site, but you likely won't be able to bring them home for assignments.</p>"},{"location":"lab1/#1-optional-installing-fedora","title":"1  (Optional)  - Installing Fedora","text":"<p>If you want to install Linux on your personal laptop but aren\u2019t sure where to start, you can follow these instructions.</p> <p>Fedora is a modern, open-source Linux distribution sponsored by Red Hat. It ships with the GNOME 3 desktop environment by default and uses <code>dnf</code> as its package manager.</p> <p>If you're new to Linux and want something simple to use, Fedora is a great place to start.</p>"},{"location":"lab1/#2-the-linux-shell-and-bash","title":"2 - The linux Shell and Bash","text":"<p>On Windows/MacOS, you most likely use the file manager or other graphical interfaces to interact with your computer. On linux however, we use the Linux Shell via a terminal/console.</p> The Shell    <p>The shell is a very powerful tool that allows you to interact with your computer in many ways. We will only cover the basics in this lab.</p>"},{"location":"lab1/#a-first-try-starting-a-new-terminal","title":"a) First, try starting a new terminal","text":"<p>Look for an app called <code>terminal</code>, <code>console</code> or even <code>konsole</code>. In some linux distribution, <code>CTRL+ALT+T</code> will open a new terminal.</p> <ul> <li>A <code>terminal</code> is the graphical application displaying the text.</li> <li>A <code>shell</code> is the underlying program that interprets and executes commands that you provide. By default, your shell will probably be <code>bash</code>, which is one of the most basic shells available. All shells serve the same function, but some come with plugins and other tools to make your life easier.</li> </ul> <p>Warning</p> <p>To copy/paste in your terminal you must use <code>CTRL+SHIFT+C</code> and <code>CTRL+SHIFT+V</code>. Pressing <code>CTRL+C</code> will KILL (stop) the current command.</p> <p>If you press <code>CTRL+S</code>, this will put the terminal on hold. Nothing will display anymore. Press <code>CTRL+Q</code> to re-enable your terminal</p>"},{"location":"lab1/#1-basic-exercices","title":"1. Basic Exercices","text":"<p>We will now dive into the very basics of how to use the shell. Note that the exercises presented here are minimal, and there's much to discover. Lines starting with a <code>#</code> are comments and should not be executed.</p>"},{"location":"lab1/#a-try-inputing-the-following-commands-what-does-the-ls-command-do","title":"a) Try inputing the following commands. What does the <code>ls</code> command do ?","text":"<pre><code># Don't worry about this yet\ncd ~\nls\nls -lh\nls -lah\n</code></pre>"},{"location":"lab1/#b-run-the-following-commands-step-by-step-and-try-to-understand-what-is-happening","title":"b) Run the following commands step-by-step and try to understand what is happening:","text":"<p><pre><code>ls\nmkdir glhpc\nls\ncd ./glhpc\nls\nmkdir lab1\ncd ./lab1\nls\n</code></pre> What does <code>mkdir</code> do ? <code>cd</code> ?</p>"},{"location":"lab1/#c-based-on-the-previous-question-could-you-give-a-definition-for-the-term-current-working-directory-cwd","title":"c) Based on the previous question, could you give a definition for the term \"Current Working Directory\" (CWD) ?","text":"<p>Execute the following to confirm your definition:</p> <pre><code>pwd\n</code></pre>"},{"location":"lab1/#d-execute-the-following-step-by-step","title":"d) Execute the following step-by-step:","text":"<pre><code>echo \"Bonjour\"\necho \"Bonjour, mon username est $USER et mon home est dans $HOME\"\necho \"Bonjour\" &gt; bonjour.txt\nls\ncat ./bonjour.txt\nls -lah &gt; ./bonjour.txt\ncat ./bonjour.txt\n</code></pre> <ul> <li>What does <code>echo</code> do ? What is your <code>USER</code> and your <code>HOME</code> ?</li> <li>What does the <code>&gt;</code> operator do ? (Tips: Did you see the output of this command in your terminal ?)</li> <li>What does <code>cat</code> do ?</li> </ul>"},{"location":"lab1/#e-execute-the-following","title":"e) Execute the following:","text":"<pre><code>pwd\ncd ..\nmkdir lab1\n</code></pre> <ul> <li>Did the last command (<code>mkdir lab1</code>) work ? Why not ?</li> <li>What does <code>cd ..</code> do ? What does <code>..</code> mean ? </li> <li>Execute these commands: <code>pwd</code>, <code>realpath .</code>, <code>realpath ..</code>, <code>realpath ~/glhpc/lab1/..</code></li> </ul>"},{"location":"lab1/#f-run-the-following","title":"f) Run the following:","text":"<pre><code>man mkdir\n</code></pre> <p>What do you see ? Try to find the <code>mkdir</code> flag to disable errors on existing folders, so that <code>mkdir lab1</code> runs succesfully.</p> <p>Press the <code>q</code> key to exit <code>man</code>. </p> <p>Tip</p> <p>What you just saw is called a <code>man page</code>. <code>man</code> is short for <code>manual</code>. It's an offline documentation that is always available on all shells.  Some tools also provide <code>man pages</code> when installed, so that you can always search for documentation. You can even search <code>man man</code> !</p> <p>If you're ever stuck on a problem/bug (and you will), you should always read the documentation, or the man pages, for solutions. Googling a bug or an error message is not cheating. This is commonly referred to as <code>Read The F*cking Manual</code> (RTFM).</p>"},{"location":"lab1/#2-more-exercises","title":"2. More Exercises","text":""},{"location":"lab1/#a-find-what-is-a-shortcut-for","title":"a) Find what <code>~</code> is a shortcut for","text":"<pre><code>cd ~\n</code></pre>"},{"location":"lab1/#b-create-the-following-file-structure-using-only-your-terminal","title":"b) Create the following file structure using only your terminal:","text":"<pre><code>exo7/\n    readme.md # With the text \"Bonjour\"\n    dossier0/\n        test.txt  # With the text \"test0\"\n    dossier1/\n        test.txt # With the text \"test1\"\n</code></pre> <p>Where <code>exo7/</code>, <code>dossier0</code> and <code>dossier1</code> are folders/directories. This directory should be located inside <code>~/glhpc/lab1/exo7</code>.</p> <p>It should look something like this (the <code>tree</code> command may not be available on your shell):</p> Final output"},{"location":"lab1/#c-finally-run-the-following-from-glhpclab1","title":"c) Finally, run the following from <code>~/glhpc/lab1</code>","text":"<pre><code>cp -r ./exo7 ./exo7_copy\n</code></pre> <p>What does <code>cp</code> do ? Why do we use the <code>-r</code> flag ?</p> <p>The <code>rm</code> command is used to remove files, while the <code>rmdir</code> command is used to delete empty folders. In order to delete a folder, and all the files it contains, we must use the <code>--force</code> and <code>--recursive</code> flags, also known as <code>rm -rf</code>.</p> <p>Try the following: <pre><code>rm -rf ./exo7_copy\n</code></pre></p> <p>Danger</p> <p><code>rm -rf</code> is definitive: there is no way to recover your files after this. No trashbin. If you delete an important folder, it is gone forever. </p> <p>You should always be very careful when doing this.</p> <p>Thought experiment: what would happen if you were to run <code>rm -rf /</code>, where <code>/</code> is the root of your filesystem ? In modern shells, it will probably show an error, or ask for confirmation, but yes, this could instantly erase all of your files, including your operating system, and crash your computer.</p>"},{"location":"lab1/#3-cheatsheet","title":"3. Cheatsheet \ud83d\udc0d","text":"Goal Command Variants Create a directory <code>mkdir &lt;path&gt;</code> <code>mkdir -p &lt;path&gt;</code> to ignore errors Go inside a directory <code>cd &lt;path&gt;</code> <code>cd ..</code> to go up one level, <code>cd ~</code> to go to your home List all files <code>ls (&lt;path&gt;)</code> <code>ls -lah (&lt;path&gt;)</code> for pretty print with human-readable numbers. Show hidden files Print cwd <code>pwd</code> Convert to absolute path <code>realpath (&lt;path&gt;)</code> Print text <code>echo &lt;text&gt;</code> <code>echo $&lt;VARIABLE&gt;</code> to print a variable Redirect output to file <code>&gt;</code> Example: <code>echo \"Bonjour\" &gt; test.txt</code> Print file content <code>cat &lt;path&gt;</code> For big files: <code>less &lt;path&gt;</code> Delete a file <code>rm &lt;path&gt;</code> Delete a directory <code>rmdir &lt;path&gt;</code> Delete a non empty directory <code>rm -rf &lt;path&gt;</code> Create empty file <code>touch &lt;path&gt;</code> Copy a file <code>cp &lt;input&gt; &lt;output&gt;</code> <code>cp -r &lt;input&gt; &lt;output&gt;</code> to copy folders recursively"},{"location":"lab1/#4-going-further-upgrading-bash","title":"4.  (Going-Further) Upgrading bash","text":"<p>While powerful, <code>bash</code> is a very basic shell. Some shells like <code>fish</code> or <code>oh-my-zsh</code> come with extensions/plugins that can significantly improve your workflow, with auto-completion, coloring, suggestions and many other.</p> <p>In the near future, you will spend a lot of time in your programming environment. Taking a few hours making it more practical or comfortable is a worthwhile investement.</p> <p>A minimalist <code>oh-my-zsh</code> setup is described here. <code>fish</code> is very simple to install and pretty powerful, but I do not recommend it due to some <code>bash</code> incompatibilities. </p>"},{"location":"lab1/#3-optional-code-editor-vscode","title":"3 -  (Optional)  Code Editor (VSCode)","text":"<p>We are now going to see the second most critical tool you will use during the Master, second only to the shell: a code editor. Modern code editors allow you to open source files, images, pdf, or even videos. You use your editor to create programs, and the shell to execute them. </p> Visual Studio Code (VSCode) Example    <p>As a starting point, you should download <code>VSCode</code> which will cover most of your needs in the future. Do NOT listen to your obnoxious classmates telling you to \"just use vim\". They cannot be saved.</p>"},{"location":"lab1/#1-installation","title":"1. Installation:","text":""},{"location":"lab1/#a-direct-download","title":"a) Direct download","text":"<p>Go to the VSCode Website and select the option matching your OS. For Fedora, click on the <code>.rpm</code> button. </p> <p>Then double click on the downloaded <code>.rpm</code> file to automatically install <code>VSCode</code>. </p> <p>You can achieve the same effect using: <pre><code># Replace with the correct file:\nsudo dnf install ./code-1.99.3-1744761644.el8.x86_64.rpm\n</code></pre></p>"},{"location":"lab1/#b-snap-install","title":"b) Snap install","text":"<p>Snap is a very helpful application to automatically install, update, and manage third-party tools (VSCode, pycharm, Spotify, etc.)</p> <pre><code># For Fedora:\nsudo dnf install snap\nsnap install code\n</code></pre>"},{"location":"lab1/#c-usage","title":"c) Usage","text":"<p>Using your shell navigate to the directory you wish to open in VSCode:</p> <pre><code>cd ./glhpc/\ncode .\n</code></pre> <p>From there, try creating a file, installing extensions (Python, C++, cmake, etc.) and familiarize yourself with the shortcuts.</p> <p>Tip</p> <p>You can also open a terminal directly inside VSCode ! </p> <p>The shortcut should be <code>CTRL+J</code>, but you can always use the terminal menu.</p>"},{"location":"lab1/#4-getting-ready-for-git","title":"4 - Getting ready for git","text":"<p>A critical part of programming is called \"versioning\" or \"Version Control System\" (VCS). This answers the following questions:</p> <ul> <li>How can I share my code with my colleagues / classmates / friends / everyone ?</li> <li>How can I keep a history of the different versions of my code ? Say <code>version 1.0</code>, <code>v2.0</code>, <code>v3.0.1.alpha-prelease</code>, etc.</li> <li>How can multiple people work together on the same project ?</li> </ul> <p>We will dive into git later. For now, do the following:</p>"},{"location":"lab1/#a-create-a-github-account-if-you-dont-already-have-one","title":"a) Create a Github account if you don't already have one.","text":"<p>You may wish to keep this account after the master: you should use your personal email so you won't lose acces to it.</p> <p>You should setup two factor authentication (2FA) ASAP.</p>"},{"location":"lab1/#b-follow-the-official-guide-on-how-to-generate-and-add-an-ssh-key-to-your-github-account","title":"b) Follow the official guide on how to generate and add an ssh key to your github account.","text":"<p>Note</p> <p>Your github page is your portfolio. Your recruiter may look it up, or you may be able to bring it up during interviews to show projects you worked on previously. </p> <p>You should take care of it, and have a few clean projects to show !</p>"},{"location":"lab1/#5-first-c-project","title":"5 - First C Project","text":""},{"location":"lab1/#0-pulling-from-github-classroom","title":"0. Pulling from github Classroom","text":"<p>You will receive a link to GitHub classroom during this lab. Accept the invite and click on your name. This will automatically create a glhpc-lab1 repository on GitHub for you.</p> <p>First, clone this repository:</p> <pre><code>git clone &lt;repo_url&gt; \n</code></pre> <p>You should see a simple <code>Readme.md</code> and <code>.gitignore</code> files inside the newly created folder.</p>"},{"location":"lab1/#1-creating-the-project","title":"1. Creating the project","text":""},{"location":"lab1/#a-create-the-following-file-structure","title":"a) Create the following file structure:","text":"<pre><code>glhpc-lab1/\n    first_c_project/\n        build.sh # Empty text file\n        src/\n            main.c # Empty text file\n</code></pre> <p>Try to do this only using the shell. If you're using VSCode you can <code>cd</code> into <code>first_c_project</code> and run <code>code .</code> Make sure to create this structure inside the cloned repo.</p>"},{"location":"lab1/#b-modify-mainc-so-that-it-contains","title":"b) Modify <code>main.c</code> so that it contains:","text":"main.c<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main(int argc, char** argv) {\n    printf(\"Hello World !\");\n    return 0;\n}\n</code></pre>"},{"location":"lab1/#2-setup-git","title":"2. Setup git","text":"<p>Please refer to Lecture 1 for all the git commands you will need in this section.</p>"},{"location":"lab1/#a-run-git-status-then-stage-all-files-from-first_c_project-in-git","title":"a) Run <code>git status</code>, then stage all files from <code>first_c_project</code> in git.","text":""},{"location":"lab1/#b-create-a-first-commit-with-the-message-my-first-commit","title":"b) Create a first commit with the message \"My first commit\"","text":"<p>I recommend you use the command <code>git commit -m \"&lt;message&gt;\"</code> or git may open nano/vim for you to edit the commit message, which may be confusing.</p>"},{"location":"lab1/#c-ensure-the-commit-worked","title":"c) Ensure the commit worked:","text":"<p><code>git log</code> should display the previous commit, and <code>git status</code> should no longer display the content of <code>first_c_project</code>. Feel free to commit files from the previous exercises of the lab if you want.</p>"},{"location":"lab1/#3-compiling-and-running-c-code","title":"3. Compiling and running C code","text":"<p>We will now try to run our first program, but before that we need to install a few tools.</p>"},{"location":"lab1/#a-install-gcc","title":"a) Install GCC","text":"<p>First, we need a C compiler to transform the <code>main.c</code> file into an executable. We will see in future courses what this does.</p> <p>For now, install the following packages:</p> Fedora<pre><code>sudo dnf install gcc glibc-devel make gdb valgrind\n</code></pre> Ubuntu<pre><code>sudo apt update\nsudo apt install gcc libc6-dev make gdb valgrind\n</code></pre>"},{"location":"lab1/#b-check-gcc-is-working","title":"b) Check GCC is working","text":"<p>Run the following:</p> <p><pre><code>gcc --version\n</code></pre> Expected output<pre><code>gcc (GCC) 14.3.1 20250808 (Red Hat 14.3.1-3)\nCopyright (C) 2024 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n</code></pre></p>"},{"location":"lab1/#c-compiling-mainc","title":"c) Compiling <code>main.c</code>","text":"<p>You can now compile your first program by running</p> <pre><code>gcc src/main.c -o main -g \n</code></pre> <p>You should see that a <code>main</code> file has been created for you.</p>"},{"location":"lab1/#d-run-main","title":"d) Run main","text":"<p>Run the program by using <code>./main</code></p>"},{"location":"lab1/#4-first-compilation-script","title":"4. First compilation script","text":""},{"location":"lab1/#a-create-a-buildsh-script-that-contains-the-compilation-command","title":"a) Create a <code>build.sh</code> script that contains the compilation command.","text":""},{"location":"lab1/#b-try-to-run-buildsh-does-it-work","title":"b) Try to run <code>./build.sh</code>. Does it work ?","text":"<p>Linux uses a concept of file permissions: some files can be read, written to, executed, or a mix of the previous. These permissions are user dependent: you are allowed to read your own files, but this privilege should not extend to other users.</p> <p>Run the following:</p> <pre><code>ls -lah\n</code></pre> Expected output<pre><code>-rw-r--r--.  1 user user   0 Aug 26 9:48 build.sh\n</code></pre> <p><code>rw-r--r--</code> can be read as: </p> <ul> <li>The user can Read and Write</li> <li>The group can Read</li> <li>Others can Read</li> </ul>"},{"location":"lab1/#c-make-buildsh-executable","title":"c) Make <code>build.sh</code> executable","text":"<p>Run the following:</p> <pre><code>chmod +x ./build.sh\n</code></pre> <p><code>chmod</code> is a command to modify the permissions of a file. <code>x</code> designates the eXecution permission, so this command can be read as add execution permission to build.sh.</p> <p>You should see that the file now has permissions <code>rwxr-xr-x</code>. This can be read as:</p> <ul> <li>The user can Read, Write, and eXecute</li> <li>The Group can Read and eXecute</li> <li>Others can Read and eXecute</li> </ul>"},{"location":"lab1/#d-restrict-permissions-so-that-only-you-the-owner-can-read-write-and-execute-buildsh-neither-the-group-nor-others-should-have-any-permissions","title":"d) Restrict permissions so that only you (the owner) can read, write, and execute build.sh. Neither the group nor others should have any permissions.","text":"<p>In binary:</p> <ul> <li><code>rwx</code> = 111 = 7</li> <li><code>r-x</code> = 101 = 5</li> <li><code>r--</code> = 100 = 4</li> </ul> <p>One can write <code>chmod 444 ./build.sh</code> which translates to <code>r--r--r--</code> (100 100 100). </p> <p>Use this to find the command needed to only give permissions to yourself.</p>"},{"location":"lab1/#5-uploading-to-github","title":"5. Uploading to GitHub","text":""},{"location":"lab1/#a-commit-all-changes-youve-made-so-far-to-git","title":"a) Commit all changes you've made so far to git","text":"<p>Tip</p> <p>Git is recursive: it doesn't matter whether you run the git commands from <code>lab1</code> or <code>first_c_project</code>: git knows whether the <code>cwd</code> is contained inside a git repository.</p>"},{"location":"lab1/#b-push-on-github","title":"b) Push on GitHub","text":"<p>The first time you push on the repository, git might:</p> <ul> <li>Ask you to setup your email/username: Follow git instructions and make sure to use the same as the one you've used on GitHub.</li> <li>Set the upstream branch using <code>--set-upstream</code>: Follow git instructions</li> </ul>"},{"location":"lab1/#5-summary","title":"5 - Summary","text":"<p>Upon completing this first lab, you should:</p> <ul> <li> Have a working programming environment</li> <li> Know how to navigate the file system with the shell</li> <li> Know how to use basic file operations</li> <li> Know how to use VSCode to write and edit files</li> <li> Be ready to use git with Github</li> </ul>"},{"location":"lab2/","title":"Lab 2: Performance Aware C Computing","text":""},{"location":"lab2/#objective","title":"Objective","text":"<p>You are tasked with implementing an efficient image processing pipeline. A pipeline looks like this:</p> Example transformation pipeline<pre><code>0 -1 load images/image1.png\n1 0 quantize 8\n2 1 invert\n3 2 save output/image0_transformed.png\n</code></pre> <p>The line <code>1 0 quantize 8</code> declares the node 1, whose parent is the node 0, and performs a quantization transformation with 8 levels. </p> <p>Note</p> <p>This structure is a Directed Acyclic Graph (DAG), a type of graph often used in data processing and scheduling.</p> Example Processing pipeline you will have to implement"},{"location":"lab2/#provided-files","title":"Provided Files","text":"<p>The parsing and pipeline execution logic have already been implemented so you can focus on managing memory and kernel implementations. The starter codebase contains the following directories and files:</p> Path Description <code>images/</code> Image resources for the lab. <code>pipelines/</code> Sample transformation pipelines to test your implementation. <code>src/</code> C source code for this lab. Contains the files listed below: <code>src/main.c</code> Parses the transformation graph and executes it. <code>src/parser.(c|h)</code> DAG representation for the image processing pipeline. <code>src/image.(c|h)</code> Image structure and utilities for memory allocation and management. <code>src/stb_image.h</code> Public domain header-only image I/O library (from GitHub). <code>src/transformation.c/h</code> Implementation of image processing kernels. Most functions are missing and must be implemented by you."},{"location":"lab2/#1-compiling-the-program","title":"1 - Compiling the program","text":""},{"location":"lab2/#a-write-a-makefile-that","title":"a) Write a <code>Makefile</code> that:","text":"<ul> <li>Compiles <code>main.c</code>, <code>image.c</code>, <code>parser.c</code> and <code>transformation.c</code> using <code>gcc</code></li> <li>Produces a binary named <code>mytransform</code> at the root of the project</li> <li>Exposes a <code>clean</code> target that removes compiled artifacts<ul> <li>This includes any <code>.o</code>, <code>.so</code>, <code>.out</code>, as well as the <code>mytransform</code> binary</li> </ul> </li> <li>You should link to the math standard library by using the <code>-lm</code> compilation flag </li> </ul> <p>Danger</p> <p>The output binary must be named <code>mytransform</code>, or later parts of the lab may not work correctly.</p>"},{"location":"lab2/#b-define-a-cflags-variable-inside-the-makefile","title":"b) Define a <code>CFLAGS</code> variable inside the <code>Makefile</code>","text":"<p>To get started, you should use <code>-Og -g -Wall -Wextra</code> as compilation flags. We will update this later. Ensure the flags are in effect.</p>"},{"location":"lab2/#c-run-make-then-try-running-mytransform","title":"c) Run <code>make</code> then try running <code>mytransform</code>","text":"<p>Execute the following: Running a simple pipeline<pre><code>./mytransform ./pipelines/test.pipeline\n</code></pre></p> Expected Output<pre><code>create_image - Not implemented yet\nCould not allocate image for load\nconvert_to_grayscale - Not implemented yet\ninvert_image - Not implemented yet\nNode 3 requested to save an image from node 2, which did not produce an image\nquantize_image_naive - Not implemented yet\nNode 5 requested to save an image from node 4, which did not produce an image\ninvert_image - Not implemented yet\nNode 7 requested to save an image from node 6, which did not produce an image\n</code></pre> <p>This indicates that your build works correctly and you can continue the lab. If needed, fix your <code>Makefile</code> until you obtain the same results.</p>"},{"location":"lab2/#2-allocating-manipulating-memory","title":"2 - Allocating &amp; Manipulating memory","text":"<p>Look at <code>image.c</code> and <code>image.h</code>, and try to understand the provided structure.</p> <ul> <li>Pixels are stored as <code>unsigned char* pixels[3]</code>: what does that mean in practice ? How many arrays do we have to allocate to store an RGB Image ? What's the size of each array ? What happens if we have a grayscale image (black &amp; white).</li> <li>How do we distinguish between grayscale and RGB images ?</li> </ul>"},{"location":"lab2/#1-implement-memory-allocation","title":"1. Implement Memory Allocation","text":""},{"location":"lab2/#a-implement-image-allocation","title":"a) Implement image allocation","text":"<p>You must implement the function <code>image.c:create_image(...)</code>.</p> <p>This function should allocate memory buffers big enough to hold an image of size \\(\\text{width} \\cdot \\text{height}\\). Note for this lab, channels can either be 1 (grayscale) or 3 (RGB).</p>"},{"location":"lab2/#b-implement-image-deallocation","title":"b) Implement image deallocation","text":"<p>You must implement <code>image.c:free_image(...)</code>.</p> <p>Make sure you can free both grayscale and RGB images. Note: you must also free the <code>Image</code> structure itself.</p>"},{"location":"lab2/#c-execute-the-memory-implementation-test","title":"c) Execute the memory implementation test","text":"<p>Compile your program and fix any errors until none are left. Run the following: Memory Test<pre><code>./mytransform --memory-check\n</code></pre></p> <p>You should get the following: Expected Output<pre><code>Memory Allocations tests completed successfully\n&lt;Lots of error about copy failure&gt;\n</code></pre></p>"},{"location":"lab2/#2-image-copying","title":"2. Image Copying","text":""},{"location":"lab2/#a-implement-image-copy","title":"a) Implement image copy","text":"<p>You must implement <code>Image* copy_image(const Image* image)</code> inside <code>src/image.c</code></p> <p>This function receives an <code>Image*</code>, and produces a deepcopy, meaning that we are not copying the pointers, but allocating new pixels buffers and duplicating the image in memory.</p> <p>You should not use <code>memcpy</code> or <code>strcpy</code> for this exercise: perform a manual copy.</p>"},{"location":"lab2/#b-execute-the-memory-implementation-test","title":"b) Execute the memory implementation test","text":"<p>Compile again and run: Memory Test<pre><code>./mytransform --memory-check\n</code></pre></p> <p>You should get the following (Exact results may vary depending on your machine): <pre><code>Memory Allocations tests completed in 5 seconds\nCopy Test: 3000x3000x3 image -&gt; 3524.74 MIOPS, 3.52 GB/s\n</code></pre></p>"},{"location":"lab2/#c-implement-2d-copy-loops","title":"c) Implement 2d copy loops","text":"<p>When copying, we have to deal with three dimensions: the channels, the width and the height.</p> <p>In which order does your current implementation traverse the pixel buffer ?</p> <p>Implement the following loop structure in C: 3D loop traversal<pre><code>for c in channels\n  for x in width\n    for y in height\n      # Do the copy here\n</code></pre></p> <p>Tip</p> <p>You should make a copy of your current implementation before implementing this structure, so you can compare the different versions.  Optionally, make multiple versions of <code>copy_image</code> with different names (e.g. <code>copy_image_cxy</code>, <code>copy_image_xyc</code>), and have <code>copy_image</code> call the version you want to test.</p> <p>Now, try swapping out the loop. First iterate on <code>y</code>, then <code>x</code>, then<code>c</code>. Try all possible combinations to find which one is faster. Can you explain why ?</p>"},{"location":"lab2/#d-implement-the-linear-versions","title":"d) Implement the linear versions","text":"<p>While images are 3D structures (When taking the channels into account), you may have noticed that we have stored them as 2D arrays (One linear array per channel).  Each channel is stored in row-major order.</p> <ul> <li>Implement two nested loops: the upper levels iterating on the channels, the inner loop iterating over each pixel.</li> <li>Implement one loop: iterate only over the pixel, copying RGB in a single loop.</li> </ul> <p>Compare the performance by running the <code>--memory-check</code> option. Which loops gives you the best performance ? Do you understand why ?</p>"},{"location":"lab2/#3-executing-a-transformation-pipeline","title":"3 - Executing a transformation pipeline","text":"<p>Look at <code>src/parser.h</code> and try to understand the different structures of the parser. Do the same for <code>src/transformation.c</code></p>"},{"location":"lab2/#1-implementing-grayscale","title":"1. Implementing Grayscale","text":"<p>The grayscale transform receives an RGB image and converts it into a black-and-white, single channel image. The formula for the transformation is:</p> \\[ C_{out} = 0.299 \\cdot R_{in} + 0.587 \\cdot G_{in} + 0.114 \\cdot B_{in} \\] <p>Where C is the grayscale channel, and R,G,B are the corresponding components of the RGB image. Note: If the grayscale transform receives a grayscale image as input, it should output a (deep) copy of the input.</p>"},{"location":"lab2/#a-implement-this-transformation","title":"a) Implement this transformation","text":"<p>You should read data from <code>node.input</code>, and allocate and write data to <code>node.output</code>.  You do not need to free the <code>input</code> and <code>output</code> buffer of any of the nodes: it will be handled for you.</p> <p>Test using Test grayscale<pre><code>mkdir -p ./output\n./mytransform ./pipelines/test.pipeline\n</code></pre></p> <p>At this stage, <code>output/test_grayscale.png</code> should contain the grayscale of <code>images/test.png</code></p>"},{"location":"lab2/#2-implementing-inversion","title":"2. Implementing Inversion","text":"<p>The inversion transform is simple:</p> \\[ C_{out} = 255 - C_{in} \\] <p>Where C is one of the input image channels (RGB or Grayscale). Implement this kernel in <code>transformation.c:invert_image(...)</code>.</p>"},{"location":"lab2/#3-implementing-quantization","title":"3. Implementing Quantization","text":"<p>We will implement a uniform quantization transform, which uniformly reduces the number of possible values in each component of the image.</p> \\[\\begin{aligned} \\text{step} &amp;= \\frac{255}{\\text{levels} - 1} \\\\\\\\ C_{out} &amp;= \\text{round}\\left(\\frac{C_{in}}{\\text{step}}\\right) \\cdot \\text{step} \\end{aligned}\\] <p>Where \\(\\text{levels}\\) is the target number of discrete values per component. This maps each input color channel value \\(C_{in} \\in [0, 255]\\) to a quantized output \\(C_{out}\\) in the same range.</p>"},{"location":"lab2/#a-implement-this-transformation-using-the-formula-provided-above","title":"a) Implement this transformation using the formula provided above.","text":"<p>You must implement this function in <code>transformation.c:quantize_image_naive(...)</code></p>"},{"location":"lab2/#b-optimize-using-a-lookup-table","title":"b) Optimize using a lookup table","text":"<p>Division and floating-point operations can be costly. Since \\(C_{in} \\in [0, 255]\\), we can easily precompute all possible outputs in a lookup table (LUT) of size 256,  then convert each pixel using:</p> \\[ C_{out} = LUT[C_{in}] \\] <p>We are trading memory overhead for performance, which is a very common pattern.</p> <p>Implement this function in <code>transformation.c:quantize_image_lut(...)</code>, and be sure to correctly allocate and free the LUT.</p>"},{"location":"lab2/#c-compare-performance","title":"c) Compare performance","text":"<p>Modify <code>transformation.c:quantize_image(...)</code> to either use the LUT or the naive version, and run the following code: Benchmarking<pre><code>time ./mytransform ./pipelines/quantize_benchmark.pipeline\n</code></pre></p> <p>Which version is faster ? What's the speedup of the LUT algorithm over the naive version ?</p>"},{"location":"lab2/#4-validation","title":"4) Validation","text":"<p>At this stage, your code should be able to execute all pipelines in the <code>pipelines/</code> directory.  Validate your implementation by checking that output images are generated and appear visually correct.</p>"},{"location":"lab2/#1-improving-performance","title":"1. Improving performance","text":"<p>Once your implementation is functionally correct, your next goal is to optimize performance.</p> <p>Use the provided script:</p> <pre><code># run_all.sh &lt;run_label&gt;\n./run_all.sh first_version\n</code></pre> <p>Tip</p> <p>You should run the previous script with both the LUT and naive implementation of quantization to compare.</p> <p>Check the resulting plots in <code>./results/first_version/</code>. Take a look at <code>./results/first_version/pipeline.pipeline</code> to understand which operations are being evaluated. Re-run the benchmark after every meaningful optimization, and check <code>./results/comparisons.png</code> to see if you improved performance or not. Tip: play around with compilation flags.</p> <p>To remove a version, remove the corresponding folder in <code>./results</code></p> <p>Important</p> <p>You need to have python installed for <code>run_all.sh</code> to work. On Fedora: Installing python on fedora<pre><code>sudo dnf install python3 python3-pip python3-virtualenv\n</code></pre></p>"},{"location":"lab2/#5-summary","title":"5 - Summary","text":"<p>Upon completing this second lab, you should be able to:</p> <ul> <li> Create a makefile, link to a dynamic library.</li> <li> Explore compilation flags for performance</li> <li> Allocate and free memory in C.</li> <li> Explore the effect of memory layout and loop order on performance.</li> <li> Implement loop-based algorithms.</li> <li> Rearrange your algorithm to avoid costly operations.</li> <li> Understand the basics of the optimization loop.</li> </ul>"},{"location":"lab3/","title":"Lab 3: CMake, Unit Tests, and Debugging","text":""},{"location":"lab3/#objectives","title":"Objectives","text":"<ul> <li>Learn how to use CMake for building C projects.</li> <li>Write and run unit tests using the Unity testing framework.</li> <li>Use <code>valgrind</code> to detect memory-related issues and pinpoint invalid memory accesses.</li> <li>Learn how to use <code>gdb</code> to debug logical errors in C programs.</li> <li>Practice setting breakpoints, inspecting variables, and stepping through code in <code>gdb</code>.</li> </ul>"},{"location":"lab3/#provided-files","title":"Provided Files","text":"<p>This lab is a continuation of lab 2. The structure of the project is the same. A new transformation <code>rotate_image_90_clockwise</code>, which you will analyze in the third part of this lab, has been added to the <code>transformations.h</code> and <code>transformations.c</code> files.</p>"},{"location":"lab3/#1-cmake","title":"1 - CMake","text":"<p>In this first part, you will learn how to write a <code>CMakeLists.txt</code> file for a C project, starting from a provided <code>Makefile</code>. The goal is to progressively build a robust and maintainable CMake configuration for an HPC project.</p>"},{"location":"lab3/#1-minimal-build","title":"1. Minimal Build","text":"<p>Create a minimal <code>CMakeLists.txt</code> that builds the shared library <code>libimage.so</code> and the executable <code>mytransform</code>.</p>"},{"location":"lab3/#a-set-the-minimum-required-cmake-version-and-project-name","title":"a) Set the minimum required CMake version and project name","text":"CMakeLists.txt<pre><code>cmake_minimum_required(VERSION 3.15)\nproject(parser LANGUAGES C)\n</code></pre>"},{"location":"lab3/#b-add-include-directories","title":"b) Add include directories","text":"CMakeLists.txt<pre><code>include_directories(src include)\n</code></pre> <p>The <code>include_directories</code> command specifies the directories to search for header files during compilation. Here, <code>include</code> contains the public header of the image library, and <code>src</code> contains the private headers used internally by the library and the executable.</p>"},{"location":"lab3/#c-add-the-shared-library-target","title":"c) Add the shared library target","text":"CMakeLists.txt<pre><code>add_library(image SHARED src/image.c)\n</code></pre> <p>In Linux a shared library has the extension <code>.so</code> (shared object). The <code>add_library</code> command creates a target named <code>image</code> that builds a shared library from the source file <code>src/image.c</code>. The final library will be named <code>libimage.so</code> by default.</p>"},{"location":"lab3/#d-add-the-executable-target","title":"d) Add the executable target","text":"CMakeLists.txt<pre><code>add_executable(mytransform src/main.c src/transformation.c src/image.c)\n</code></pre> <p>The <code>add_executable</code> command builds an executable <code>mytransform</code> from the specified source files. Header files were included before.</p>"},{"location":"lab3/#e-link-the-shared-library-to-the-executable","title":"e) Link the shared library to the executable","text":"<pre><code>target_link_libraries(mytransform PRIVATE image m)\n</code></pre> <p>This command ensures that the <code>mytransform</code> executable is linked against the <code>image</code> shared library and the math library <code>m</code>. </p> <p>Note</p> <p>The <code>PRIVATE</code> keyword indicates that the dependency is only required for building the <code>mytransform</code> target and does not propagate to other targets that may link against <code>mytransform</code>.</p>"},{"location":"lab3/#f-generate-the-build-system","title":"f) Generate the build system","text":"<pre><code>$ cmake -B build .\n</code></pre> <p>Here <code>-B</code> specifies the build directory (a new directory named <code>build</code>).</p>"},{"location":"lab3/#g-build-the-project","title":"g) Build the project","text":"<pre><code>$ make -C build/\n</code></pre> <p>Note</p> <p>By default, CMake generates a Makefile as the build system on Unix-like systems. Sometimes, it can be useful to call directly the <code>make</code> command to build the project. You can also use <code>cmake --build build</code> to build the project, which is more portable across different platforms and build systems.</p>"},{"location":"lab3/#h-answer-the-following-questions","title":"h) Answer the following questions","text":"<ul> <li>Where are the generated files located?</li> <li>Can you find the <code>libimage.so</code> library? the <code>mytransform</code> executable?</li> <li>Can you run the program? </li> </ul>"},{"location":"lab3/#2-build-configurations","title":"2. Build Configurations","text":"<p>We want to enable different build configurations (e.g., Debug, Release) and set appropriate compiler options.</p>"},{"location":"lab3/#a-configure-the-c-standard-used","title":"a) Configure the C standard used","text":"CMakeLists.txt<pre><code>set(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED ON)\n</code></pre> <p>This ensures that the C11 standard is used for compiling the project.</p>"},{"location":"lab3/#b-enable-different-build-types","title":"b) Enable different build types","text":"<p>Define common compiler flags for different build types:</p> CMakeLists.txt<pre><code>set(COMMON_COMPILE_FLAGS\n    $&lt;$&lt;CONFIG:Debug&gt;:-Wall -Wextra -g&gt;\n    $&lt;$&lt;CONFIG:Release&gt;:-Wall -Wextra -O3 -DNDEBUG&gt;\n)\n</code></pre> <p>Apply the flags to the targets:</p> CMakeLists.txt<pre><code>target_compile_options(image PRIVATE ${COMMON_COMPILE_FLAGS})\ntarget_compile_options(mytransform PRIVATE ${COMMON_COMPILE_FLAGS})\n</code></pre> <p>Rebuild the project and test different configurations:</p> <pre><code>$ cmake -B build -DCMAKE_BUILD_TYPE=Debug .\n</code></pre> <p>You can check that the debug symbols are included in the binary using <code>file</code>:</p> <pre><code>$ file build/mytransform \nbuild/mytransform: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=0d2cbaa0cf08a42b916e2edffe9940ce828b2bd9, for GNU/Linux 3.2.0, with debug_info, not stripped\n</code></pre>"},{"location":"lab3/#3-enable-installation","title":"3. Enable Installation","text":"<p>Now we will add installation rules to install the shared library, executable, and headers.</p>"},{"location":"lab3/#a-give-your-project-a-version-number","title":"a) Give your project a version number","text":"<p>Modify the <code>project</code> command as below:</p> CMakeLists.txt<pre><code>project(parser VERSION 1.0.0 LANGUAGES C)\n</code></pre>"},{"location":"lab3/#b-include-the-gnuinstalldirs-module","title":"b) Include the <code>GNUInstallDirs</code> module","text":"CMakeLists.txt<pre><code>include(GNUInstallDirs)\n</code></pre> <p>This module provides standard installation directory variables like <code>CMAKE_INSTALL_BINDIR</code>, <code>CMAKE_INSTALL_LIBDIR</code>, and <code>CMAKE_INSTALL_INCLUDEDIR</code>.</p>"},{"location":"lab3/#c-add-installation-rules-for-the-shared-library","title":"c) Add installation rules for the shared library","text":"CMakeLists.txt<pre><code>install(TARGETS image \n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n)\n</code></pre>"},{"location":"lab3/#d-add-installation-rules-for-the-executable","title":"d) Add installation rules for the executable","text":"CMakeLists.txt<pre><code>install(TARGETS mytransform\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n)\n</code></pre>"},{"location":"lab3/#e-test-the-installation","title":"e) Test the installation","text":"<pre><code>mkdir install_dir\n$ cmake -B build -DCMAKE_INSTALL_PREFIX=install_dir . \n$ make -C build/ install\n</code></pre> <p>Since <code>install_dir</code> is not a standard system directory, you need to set the <code>LD_LIBRARY_PATH</code> environment variable to include the path to the installed shared library before running the program.</p> <p>Note</p> <p>We can also call <code>cmake --install build --prefix &lt;install_directory&gt;</code> to install the project.</p>"},{"location":"lab3/#f-install-the-public-header-file","title":"f) Install the public header file","text":"<p>As you can see, the public header file <code>image.h</code> is not installed. To inform CMake about the public headers, you should add the following command:</p> CMakeLists.txt<pre><code>set_target_properties(image PROPERTIES\n    VERSION ${PROJECT_VERSION}\n    SOVERSION ${PROJECT_VERSION_MAJOR}\n    PUBLIC_HEADER include/image.h\n)\n</code></pre> <p>Rebuild and install the project again, you should see the <code>image.h</code> file in the <code>include</code> directory of the installation prefix. Additionally, the shared library should now have a versioned name like <code>libimage.so.1.0.0</code>.</p>"},{"location":"lab3/#3-better-handling-of-include-directories","title":"3. Better handling of include directories","text":"<p>Our current way of handling include directories is not ideal. We will improve it by using <code>target_include_directories</code> which keeps the include directories scoped to each target.</p>"},{"location":"lab3/#a-remove-the-global-include_directories-command","title":"a) Remove the global <code>include_directories</code> command","text":""},{"location":"lab3/#b-add-the-following-commands-to-specify-include-directories-for-each-target","title":"b) Add the following commands to specify include directories for each target","text":"CMakeLists.txt<pre><code>target_include_directories(image\n    PUBLIC \n        $&lt;BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/include&gt;\n        $&lt;INSTALL_INTERFACE:include&gt;\n    PRIVATE\n        ${PROJECT_SOURCE_DIR}/src\n    )\n\ntarget_include_directories(mytransform PRIVATE ${PROJECT_SOURCE_DIR}/src)\n</code></pre> <p>The library target disinguishes between <code>PUBLIC</code> and <code>PRIVATE</code> include directories. <code>PUBLIC</code> directories are needed both when building the library and when using it, while <code>PRIVATE</code> directories are only needed when building the library itself.</p> <p>The <code>BUILD_INTERFACE</code> generator expression specifies the include directory to use when building the project, while the <code>INSTALL_INTERFACE</code> generator expression specifies the include directory to use when the library is installed. This ensures that users of the installed library can include the header files correctly.</p>"},{"location":"lab3/#2-unit-tests","title":"2 - Unit Tests","text":"<p>In this part, you will learn how to integrate unit tests into your CMake project using the Unity testing framework.</p>"},{"location":"lab3/#1-fetch-the-unity-framework","title":"1. Fetch the Unity framework","text":"<p>Fetch the Unity framework:</p> CMakeLists.txt<pre><code># Fetch and build the Unity testing framework\ninclude(FetchContent)\nFetchContent_Declare(\n    unity\n    GIT_REPOSITORY  https://github.com/ThrowTheSwitch/Unity.git\n    GIT_TAG         v2.6.1\n    GIT_SHALLOW TRUE # Only download the specific tag, not full history\n)\n\n# Make Unity available but don't add to ALL target by default\n# This is important to avoid installing unity dependencies which are only needed for testing \n# but are not required in the release version of the project\nFetchContent_GetProperties(unity)\nif(NOT unity_POPULATED)\n    FetchContent_Populate(unity)\n    add_subdirectory(${unity_SOURCE_DIR} ${unity_BINARY_DIR} EXCLUDE_FROM_ALL)\nendif()\n</code></pre> <p>CMake fetches and builds the Unity framework, making it available for use in your project.</p>"},{"location":"lab3/#2-write-a-first-unit-test","title":"2. Write a first unit test","text":"<p>We currently have a set of hard-coded tests in <code>main.c</code>:</p> <ul> <li><code>check_grayscale</code></li> <li><code>check_rgb</code> </li> <li><code>check_copy</code></li> </ul> <p>Read carefully these tests to understand what they do.</p> <p>To convert them into unit tests, we will create a new source file <code>tests/test_image.c</code> and move the test functions there. The <code>check_memory</code> function which ran all the files will be replaced by a test runner in <code>tests/test_runner.c</code>.</p> <p>We will start first by writing the test runner. Create a new file <code>tests/test_runner.c</code> with the following content:</p> tests/test_runner.c<pre><code>#include \"unity.h\"\n\nextern void test_grayscale_image_creation(void);\n\nvoid setUp(void) {}\nvoid tearDown(void) {}\n\nint main(void)\n{\n    UNITY_BEGIN();\n    RUN_TEST(test_grayscale_image_creation);\n    return UNITY_END();\n}\n</code></pre> <p>Note</p> <p>The <code>setUp</code> and <code>tearDown</code> functions are called before and after each test, respectively. They can be used to set up and clean up test fixtures if needed.</p> <p>Now create the <code>tests/test_image.c</code> file and move the <code>check_grayscale</code> function there, renaming it to <code>test_grayscale_image_creation</code>:</p> tests/test_image.c<pre><code>#include \"unity.h\"\n#include \"transformation.h\"\n#include &lt;stdlib.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\nvoid test_grayscale_image_creation(void)\n{\n    Image *img = create_image(100, 100, 1);\n    TEST_ASSERT_NOT_NULL(img);\n    TEST_ASSERT_EQUAL(100, img-&gt;width);\n    TEST_ASSERT_EQUAL(100, img-&gt;height);\n    TEST_ASSERT_EQUAL(1, img-&gt;channels);\n    TEST_ASSERT_NOT_NULL(img-&gt;pixels[0]);\n    TEST_ASSERT_NULL(img-&gt;pixels[1]);\n    TEST_ASSERT_NULL(img-&gt;pixels[2]);\n    free_image(img);\n}\n</code></pre> <p>As you can see, we are using systematically the Unity assertion macros to check conditions.</p>"},{"location":"lab3/#3-add-the-test-runner-executable","title":"3. Add the test runner executable","text":"<p>To run our tests, we need to create a new executable target for the test runner in our <code>CMakeLists.txt</code> file.</p> CMakeLists.txt<pre><code>add_executable(test_runner tests/test_runner.c tests/test_image.c src/transformation.c src/parser.c)\ntarget_include_directories(test_runner \n    PRIVATE \n        ${PROJECT_SOURCE_DIR}/src\n)\ntarget_link_libraries(test_runner unity m image)\n</code></pre> <p>Observe that we link the <code>test_runner</code> target against the <code>unity</code> library, the math library <code>m</code>, and our <code>image</code> library.</p>"},{"location":"lab3/#4-add-a-custom-target-to-run-the-tests","title":"4. Add a custom target to run the tests","text":"<p>To facilitate running the tests, we can add a custom target in our <code>CMakeLists.txt</code> file that will execute the <code>test_runner</code> executable.</p> CMakeLists.txt<pre><code>add_custom_target(test test_runner \n    DEPENDS $&lt;TARGET_FILE:test_runner&gt;\n    WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}\n    COMMENT \"Running unit tests...\"\n)\n</code></pre> <p>Check that everything works by building the project and running the tests:</p> <pre><code>$ cmake --build build\n$ make -C build test\n</code></pre>"},{"location":"lab3/#5-add-the-remaining-tests","title":"5. Add the remaining tests","text":"<p>Write unit tests for the <code>check_rgb</code> and <code>check_copy</code> functions in the <code>tests/test_image.c</code> file. </p> <p>Tip</p> <p>For bonus point, separate the logic of the <code>check_copy</code> test into two different tests: <code>test_image_copy</code>, that check the code validity, and <code>test_image_copy_performance</code> that measures and displays the performance.</p>"},{"location":"lab3/#3-debugging-with-gdb-and-valgrind","title":"3 - Debugging with GDB and Valgrind","text":"<p>In this last part, you will learn how to debug two types of bugs in a C program using <code>gdb</code> and <code>valgrind</code>. These bugs are intentionally introduced in the <code>rotate_image_90_clockwise</code> function. The goal is to identify, understand, and fix these bugs while reflecting on the debugging process.</p>"},{"location":"lab3/#1-build-the-program-with-debug-symbols","title":"1. Build the Program with Debug Symbols","text":"<p>Debugging symbols are metadata embedded in a program's binary during compilation, providing detailed information about the source code, such as variable names, function names, and line numbers. These symbols allow tools like gdb and valgrind to map the program's execution back to the original source code, making it easier to inspect variables, set breakpoints, and trace errors. Without debugging symbols, these tools would only display raw memory addresses and machine-level details, making debugging significantly harder.</p> <p>Note</p> <p>To enable debugging symbols, you need to configure CMake to include the <code>-g</code> flag in the compilation process. This can be achieved by setting the <code>CMAKE_BUILD_TYPE</code> to <code>Debug</code>.</p>"},{"location":"lab3/#a-run-the-cmake-command-with-the-debug-build-type","title":"a) Run the <code>cmake</code> command with the <code>Debug</code> build type","text":"<pre><code>$ cmake -B build/ -DCMAKE_BUILD_TYPE=Debug .\n</code></pre>"},{"location":"lab3/#b-build-the-project","title":"b) Build the project","text":"<pre><code>$ make -C build/\n</code></pre>"},{"location":"lab3/#c-run-the-program-using-the-rotate-transformation","title":"c) Run the program using the rotate transformation","text":"<pre><code>$ build/mytransform pipelines/rotate.pipeline\nLoaded image: images/image0.bmp (259x194, 3 channels)\nSegmentation fault (core dumped)\n</code></pre> <p>You should get an error as above.</p>"},{"location":"lab3/#d-analyze-carefully-the-error-message","title":"d) Analyze carefully the error message","text":"<ul> <li>What does <code>Segmentation fault</code> mean?</li> <li>What does <code>(core dumped)</code> mean?</li> <li>What could be the possible causes of this error?</li> </ul>"},{"location":"lab3/#2-running-the-program-with-gdb","title":"2. Running the program with GDB","text":"<p>GDB is the GNU Project Debugger, a powerful tool for debugging programs. It allows you to run your program step by step, inspect variables, set breakpoints, and analyze the program's flow to identify and fix bugs.</p>"},{"location":"lab3/#a-start-gdb-with-the-program-and-its-arguments","title":"a) Start gdb with the program and its arguments","text":"<pre><code>$ gdb --args build/mytransform pipelines/rotate.pipeline\n\nGNU gdb (Ubuntu 15.0.50.20240403-0ubuntu1) 15.0.50.20240403-git\n... [output truncated] ...\n(gdb)\n</code></pre> <p>Note</p> <p>The <code>--args</code> option allows you to pass the program's arguments directly to gdb, so you don't have to type them again after starting gdb. <code>(gdb)</code> is the gdb prompt, where you can enter gdb commands.</p>"},{"location":"lab3/#b-run-the-program-inside-gdb","title":"b) Run the program inside gdb","text":"<pre><code>(gdb) run\nStarting program: lab3/build/mytransform pipelines/rotate.pipeline\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nLoaded image: images/image0.bmp (259x194, 3 channels)\n\nProgram received signal SIGSEGV, Segmentation fault.\nrotate_image_90_clockwise (node=0x5555555802f0)\n    at lab3/src/transformation.c:105\n105 node-&gt;output-&gt;pixels[c][x * width + (height - y - 1)] = node-&gt;input-&gt;pixels[c][y * width + x];\n</code></pre> <p>GDB has caught the segmentation fault and shows you the exact line where the error occurred.</p>"},{"location":"lab3/#c-backtrace-the-function-calls","title":"c) Backtrace the function calls","text":"<p>You can use the <code>backtrace</code> command to see the function call stack leading to the crash:</p> <pre><code>(gdb) backtrace\n#0  rotate_image_90_clockwise (node=0x5555555802f0)\n    at lab3/src/transformation.c:105\n#1  0x0000555555577885 in execute_node (node=0x5555555802f0)\n    at lab3/src/transformation.c:222\n#2  0x00007ffff7fb9c7f in execute_graph (graph=0x5555555802a0)\n    at lab3/src/parser.c:174\n#3  0x0000555555555e4a in main (argc=2, argv=0x7fffffffdae8)\n    at lab3/src/main.c:165\n</code></pre> <p>Here everything appears normal.</p> <p>Note</p> <p>It's possible to change the frame using <code>up</code> and <code>down</code> commands to navigate through the call stack and inspect their variables.</p>"},{"location":"lab3/#d-inspect-the-variables","title":"d) Inspect the variables","text":"<p>You can inspect the values of variables at the point of the crash. For example, to check the values of <code>x</code>, <code>y</code>, <code>c</code>, <code>width</code>, and <code>height</code>, you can use the <code>print</code> command:</p> <pre><code>(gdb) print x \n$1 = 0\n</code></pre> <p>Print each of the variables, do you see anything suspicious at the point of crash?</p>"},{"location":"lab3/#e-fix-and-explain-the-first-bug","title":"e) Fix and explain the first bug","text":"<p>Tip</p> <p>The first bug is a logical error in the loop exit condition at line 109.</p> <p>Once you have identified and understood the first bug, you can fix it directly in the source code. Commit the fix to git and explain the bug and how you fixed it in the commit message.</p> <p>Unfortunately, there is still a second bug that we will fix in the next section.</p>"},{"location":"lab3/#3-using-valgrind-to-detect-memory-issues","title":"3. Using Valgrind to Detect Memory Issues","text":""},{"location":"lab3/#a-run-the-program-with-gdb-again","title":"a) Run the program with GDB again","text":"<pre><code>(gdb) run\nStarting program: lab3/build/mytransform pipelines/rotate.pipeline\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nLoaded image: images/image0.bmp (259x194, 3 channels)\nmalloc(): corrupted top size\n\nProgram received signal SIGABRT, Aborted.\n__pthread_kill_implementation (no_tid=0, signo=6, threadid=&lt;optimized out&gt;)\n    at ./nptl/pthread_kill.c:44\nwarning: 44 ./nptl/pthread_kill.c: No such file or directory\n(gdb) backtrace\n#0  __pthread_kill_implementation (no_tid=0, signo=6, threadid=&lt;optimized out&gt;)\n    at ./nptl/pthread_kill.c:44\n#1  __pthread_kill_internal (signo=6, threadid=&lt;optimized out&gt;) at ./nptl/pthread_kill.c:78\n#2  __GI___pthread_kill (threadid=&lt;optimized out&gt;, signo=signo@entry=6)\n    at ./nptl/pthread_kill.c:89\n#3  0x00007ffff7c4527e in __GI_raise (sig=sig@entry=6) at ../sysdeps/posix/raise.c:26\n#4  0x00007ffff7c288ff in __GI_abort () at ./stdlib/abort.c:79\n#5  0x00007ffff7c297b6 in __libc_message_impl (fmt=fmt@entry=0x7ffff7dce8d7 \"%s\\n\")\n    at ../sysdeps/posix/libc_fatal.c:134\n#6  0x00007ffff7ca8ff5 in malloc_printerr (\n    str=str@entry=0x7ffff7dcc6f7 \"malloc(): corrupted top size\") at ./malloc/malloc.c:5772\n#7  0x00007ffff7cac2fc in _int_malloc (av=av@entry=0x7ffff7e03ac0 &lt;main_arena&gt;, bytes=150738)\n    at ./malloc/malloc.c:4447\n#8  0x00007ffff7cad7f2 in __GI___libc_malloc (bytes=&lt;optimized out&gt;)\n    at ./malloc/malloc.c:3328\n#9  0x0000555555576fd3 in save_image (node=0x555555580320)\n    at lab3/src/transformation.c:70\n#10 0x0)\n    at lab3/src/transformation.c:225\n#11 0x0a0)\n    at lab3/src/parser.c:174\n#12 0x0)\n    at lab3/src/main.c:165\n</code></pre> <p>The program crashes again, but this time with a different error message: <code>malloc(): corrupted top size</code>. This indicates a memory corruption issue.</p> <p>The backtrace does not point to the exact line in your code where the corruption occurred. Why?</p>"},{"location":"lab3/#b-use-valgrind-to-pinpoint-the-memory-issue","title":"b) Use Valgrind to pinpoint the memory issue","text":"<p>Valgrind is a programming tool for memory debugging, memory leak detection, and profiling. It can help you identify memory-related issues in your program, such as invalid memory accesses, memory leaks, and uninitialized memory usage.</p> <pre><code>valgrind build/mytransform pipelines/rotate.pipeline\n\n==241843== Memcheck, a memory error detector\n==241843== Copyright (C) 2002-2022, and GNU GPL'd, by Julian Seward et al.\n==241843== Using Valgrind-3.22.0 and LibVEX; rerun with -h for copyright info\n==241843== Command: build/mytransform pipelines/rotate.pipeline\n==241843== \nLoaded image: images/image0.bmp (259x194, 3 channels)\n==241843== Invalid write of size 1\n==241843==    at 0x12B188: rotate_image_90_clockwise (transformation.c:105)\n==241843==    by 0x12B87D: execute_node (transformation.c:222)\n==241843==    by 0x485BC7E: execute_graph (parser.c:174)\n==241843==    by 0x109E49: main (main.c:165)\n==241843==  Address 0x4bf26f7 is 119 bytes inside an unallocated block of size 3,729,760 in arena \"client\"\n... [output truncated] ...\n</code></pre> <p>How does Valgrind work? Why is it able to provide more detailed information about memory issues than gdb?</p> <p>Here valgrind provides a detailed report of the memory error, including the exact line in your code where the invalid write occurred. Now we will use gdb again to inspect the variables at the point of the invalid write.</p>"},{"location":"lab3/#c-set-a-breakpoint-at-the-faulty-line","title":"c) Set a breakpoint at the faulty line","text":"<pre><code>(gdb) break transformation.c:105\n(gdb) run\n... [output truncated] ...\nBreakpoint 1, rotate_image_90_clockwise (node=0x5555555802f0)\n    at lab3/src/transformation.c:105\n105 node-&gt;output-&gt;pixels[c][x * width + (height - y - 1)] = node-&gt;input-&gt;pixels[c][y * width + x];\n(gdb) \n</code></pre> <p>Observe that gdb stops at the breakpoint you set and allows inspecting the point of the invalid write.</p>"},{"location":"lab3/#d-inspect-the-values-of-x-y-width-height-and-the-computed-index","title":"d) Inspect the values of <code>x</code>, <code>y</code>, <code>width</code>, <code>height</code>, and the computed index:","text":"<p><pre><code>print x\nprint y\nprint width\nprint height\nprint x * width + (height - y - 1)\n</code></pre> GDB allows you to perform arithmetic operations directly in the <code>print</code> command, so you can compute the index and check if it is within bounds. Do you see anything suspicious?</p>"},{"location":"lab3/#e-set-a-conditional-breakpoint","title":"e) Set a conditional breakpoint","text":"<p>We start to suspect that the index calculation is incorrect. To catch the invalid memory write, set a conditional breakpoint that triggers when the computed index is out of bounds. Start gdb again and run the following commands:</p> <pre><code>(gdb) break transformation.c:105\n(gdb) condition 1 x * width + (height - y - 1) &gt;= height * width\n</code></pre> <p><code>condition</code> 1 sets a condition on breakpoint 1, so it only triggers when the condition is true. The condition will trigger when the computed index is greater than or equal to the total number of pixels in the image, which indicates an out-of-bounds access.</p> <p>Run the program again, do you hit the breakpoint?</p>"},{"location":"lab3/#f-analyze-and-fix-the-bug","title":"f) Analyze and fix the bug","text":"<p>Tip</p> <p>Check carefully that the dimensions used in the index calculation are correct. The bug is a mix-up between <code>width</code> and <code>height</code>.</p> <p>As before once you have identified and understood the second bug, you can fix it directly in the source code. Commit the fix to git and explain the bug and how you fixed it in the commit message.</p> <p>Check that the program runs correctly now!</p>"},{"location":"lab3/#4-non-regression-tests","title":"4. Non-regression tests","text":"<p>Now that you have fixed both bugs, it's important to ensure that the bugs will not reappear in the future. To do this, you will write non-regression tests using the Unity testing framework.</p> <p>Add tests that specifically target the scenarios that led to the bugs you fixed. For example, you can create tests that rotate images of various sizes and shapes, including non-square images, to ensure that the <code>rotate_image_90_clockwise</code> function behaves correctly.</p>"},{"location":"lab3/#5-gcov-llvm-cov-optional","title":"5. GCov / llvm-cov (Optional)","text":"<p>GCov analyzes and reports on the code coverage of your tests. It helps you identify which parts of your code are being executed during testing and which parts are not, allowing you to improve your test suite.</p> <p>GCov (or its variant llvm-cov for LLVM/Clang) works by instrumenting your code during compilation to collect coverage data. </p> <p>To use GCov with CMake, you need to enable coverage flags during the build process. When compiling with GCC, you can use the <code>--coverage -O0 -g</code> flags, and link with <code>--coverage</code> flag.</p> <p>Fortunately, CMake provides a convenient way to set these flags using the CodeCoverage module. Retrieve the module cmake file:</p> <pre><code>$ mkdir CMakeModules &amp;&amp; cd CMakeModules\n$ wget https://github.com/bilke/cmake-modules/raw/refs/heads/master/CodeCoverage.cmake\n</code></pre> <p>Then, include the module in your <code>CMakeLists.txt</code> file and add the coverage flags to the test target:</p> CMakeLists.txt<pre><code>set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/CMakeModules)\ninclude(CodeCoverage)\n\n... [truncated] ...\n\nappend_coverage_compiler_flags_to_target(test_runner)\n</code></pre> <p>Then, you can build your project with the <code>Debug</code> build type:</p> <pre><code>$ cmake -B build -DCMAKE_BUILD_TYPE=Debug .\n$ make -C build/ test \n</code></pre> <p>You should see files with the <code>.gcda</code> extension generated in the build directory. These files contain the coverage data collected during the execution of your tests.</p> <p>Finally, you can generate the coverage report using the <code>gcovr</code> command:</p> <pre><code>$ gcovr -r .\n</code></pre> <p>This will generate a coverage report for the <code>transformation.c</code> file, showing which lines of code were executed during the tests.</p> <p>Tip</p> <p>You can also generate an HTML report using the <code>--html</code> option:</p> <pre><code>$ gcovr -r . --html --html-details -o coverage_report.html\n</code></pre> <p>This will create a detailed HTML report that you can open in a web browser.</p> <p>What is the percentage of code coverage achieved by your tests? Are there any parts of the code that are not covered by the tests? If so, consider adding more tests to cover those areas.</p>"},{"location":"lab3/#4-summary","title":"4 - Summary","text":"<p>Upon completing this third lab, you should know how to:</p> <ul> <li> Use CMake to build and install a shared library and an executable.</li> <li> Configure CMake for different build types and compiler options.</li> <li> Integrate and run unit tests using the Unity testing framework.</li> <li> Debug segmentation faults and memory issues using GDB and Valgrind.</li> <li> Write non-regression tests to prevent reintroducing fixed bugs.</li> </ul>"},{"location":"lab4/","title":"Lab 4: Monte-Carlo Sampling","text":"<p>Monte-Carlo methods are useful for approximating quantities that are difficult or computationally expensive to determine analytically or through deterministic algorithms. These methods introduce randomness into the computations in order to obtain a statistically meaningful estimate over multiple repetitions of the same routine.</p> <p>In this lab, we explore the approximation of \u03c0 using Monte-Carlo sampling. To do this, we consider a unit circle (radius 1) inscribed within a square of side length 2, centered at the origin. By generating random points uniformly within the square and counting how many fall inside the circle, we can estimate the ratio of the areas of the two shapes. </p> <p>Since the area of the circle is \\(\\pi \\cdot r^2 = \\pi\\), and the area of the square is 4, we expect the ratio of points falling inside the circle to converge to \\(\\pi / 4\\).</p> Monte-Carlo Pi"},{"location":"lab4/#1-implementing-the-mc-method","title":"1 - Implementing the MC Method","text":"<p>Implement your own version of the \\(\\pi\\) estimator inside <code>src/compute_pi.c</code> using the Monte-Carlo method. This method receives \\(n\\) the number of Monte-Carlo samples to take as arguments, and must return the approximation of \\(\\pi\\) in <code>double</code> precision.</p> <p>You will have to setup a CMake for this lab. The program can be run using: Run the estimator<pre><code># piestimator &lt;nsamples&gt;\npiestimator 1000000\n</code></pre></p>"},{"location":"lab4/#2-timing-and-serialization","title":"2 - Timing and serialization","text":""},{"location":"lab4/#1-implementing-timing","title":"1. Implementing timing","text":""},{"location":"lab4/#a-modify-the-function-srcmaincmc_harness-to-measure-the-execution-time-of-your-method","title":"a) Modify the function <code>src/main.c:mc_harness(...)</code> to measure the execution time of your method.","text":"<p>You must repeat the measurements <code>nmeta</code> times, and record the values of Pi as well as the execution time for each execution.</p> <p>What function did you use to measure time? How accurate is it? Is it monotonic?</p>"},{"location":"lab4/#b-modify-the-function-srcmaincprint_results-to-print-a-table-with-the-following-values","title":"b) Modify the function <code>src/main.c:print_results(...)</code> to print a table with the following values:","text":"Avg. Pi Std Pi Avg. Time Std Time Min Time Max Time Average Value of Pi Standard Deviation of Pi Average execution time Standard Deviation of execution time Min execution time Max Execution time <p>You may need to modify other functions or the provided structures to achieve this.</p>"},{"location":"lab4/#c-implement-csv-serialization-inside-the-program","title":"c) Implement csv serialization inside the program.","text":"<p>You must exactly match this format and header:</p> output.csv<pre><code>NMeta,Pi,Time\n1,3.145584,0.025\n2,3.13547,0.028\n</code></pre> <p>Print at least 10 decimals, and ensure that the file is saved in the path provided by the user.</p> <p>Check that you can run the following: Expected API<pre><code># Run MC Pi Estimator with 1 Million sample, 2048 meta-repetitions and save the results in results.csv\n./piestimator 1000000 2048 results.csv\n</code></pre></p> <p>Warning</p> <p>The following questions will reuse this CSV serialization. Be sure that you match exactly the prescribed format. Be careful not to introduce blank spaces in the header name (e.g. <code>NMeta,Pi,Time</code> rather than <code>NMeta,   Pi,   Time</code>)</p>"},{"location":"lab4/#2-run-the-provided-experiments","title":"2. Run the provided experiments","text":"<p>If you correctly implemented csv serialization, this will generate plots inside the <code>results</code> folder.</p> First sequential run<pre><code># ./run_all.sh &lt;run_label&gt;\n./run_all.sh sequential\n</code></pre>"},{"location":"lab4/#a-first-look-at-the-top-figure-in-convergencepng","title":"a) First, look at the top figure in <code>convergence.png</code>","text":"<p>What do you observe? How does the error evolve when increasing the number of Monte-Carlo samples? If needed, fix your program so that the relative error converges toward zero.</p>"},{"location":"lab4/#b-look-at-the-bottom-figure-how-does-execution-time-evolve-when-increasing-the-number-of-samples","title":"b) Look at the bottom figure: how does execution time evolve when increasing the number of samples?","text":"<p>If needed, fix your program so that the execution time scales linearly with the number of samples.</p>"},{"location":"lab4/#c-look-at-the-top-figure-in-stabilitypng","title":"c) Look at the top figure in <code>stability.png</code>","text":"<p>How are the values of the Pi estimations distributed? Is there any bias, and if so, why? If needed, fix your program so that the Pi estimations are normally distributed around 3.14.</p>"},{"location":"lab4/#d-look-at-the-bottom-figure-how-is-the-execution-time-distributed","title":"d) Look at the bottom figure: how is the execution time distributed?","text":"<p>Check whether the timings are stable, and if not, propose an explanation. Do you observe any measurement noise? How would you characterize it?</p> <p>If needed, fix your measurements so that the execution time is mostly normally distributed, and the measurement noise is tolerable.</p> <p>Tip</p> <p>The <code>results/expected_results</code> folder contains examples of plots that were run on a stable machine, with a correct implementation of the Monte-Carlo estimator.</p>"},{"location":"lab4/#25-going-further-understanding-the-scripts","title":"2.5 -  (Going-Further) Understanding the scripts","text":""},{"location":"lab4/#1-look-at-run_allsh-and-try-to-understand-each-line","title":"1. Look at <code>run_all.sh</code>, and try to understand each line.","text":""},{"location":"lab4/#a-what-does-set-e-set-o-pipefail-and-21-tee-do","title":"a) What does <code>set -e</code>, <code>set -o pipefail</code> and <code>2&gt;&amp;1 | tee ...</code> do?","text":""},{"location":"lab4/#b-what-is-the-purpose-of-the-run_label-argument-why-should-we-label-our-data","title":"b) What is the purpose of the <code>run_label</code> argument (Why should we label our data)?","text":"<p>What files are generated, and what's the purpose of every one of them?</p>"},{"location":"lab4/#2-look-at-scriptsanalysepy-and-try-to-understand-how-each-plot-is-built","title":"2. Look at <code>scripts/analyse.py</code> and try to understand how each plot is built.","text":"<p>Try to link every components of <code>convergence.png</code> and <code>stability.png</code> (The titles, the axis label, the axis ticks, the distributions, the grid, ...) with the code that generates it.</p>"},{"location":"lab4/#3-optimization","title":"3 - Optimization","text":""},{"location":"lab4/#1-setting-up-makefile","title":"1) Setting up Makefile","text":""},{"location":"lab4/#a-modify-the-makefile-and-play-around-with-compilation-flags-and-different-compilers","title":"a) Modify the <code>makefile</code> and play around with compilation flags and different compilers.","text":"<p>Remember that you can run <code>run_all.sh &lt;run_label&gt;</code> to compare the different runs later.  </p> <p>What configuration gives you the fastest execution time? Can you understand why?</p>"},{"location":"lab4/#2-use-openmp-to-parallelize-your-monte-carlo-algorithm","title":"2. Use OpenMP to parallelize your Monte-Carlo algorithm.","text":"<p>You may need to modify the <code>makefile</code> to link to OpenMP.  </p>"},{"location":"lab4/#a-how-are-you-generating-random-numbers-in-the-sampling-algorithm","title":"a) How are you generating random numbers in the sampling algorithm?","text":"<ul> <li>Read the fourth paragraph in the description section of the <code>rand</code>/<code>srand</code> man page using <code>man 3 srand</code> or the online version.</li> <li>If necessary, research thread-safe alternatives.</li> </ul>"},{"location":"lab4/#b-where-did-you-implement-parallelization","title":"b) Where did you implement parallelization?","text":"<ul> <li>Are you averaging multiple Pi values from separate runs, or</li> <li>Are you summing the counts of points inside the circle across threads?<ul> <li>Does each thread accumulate its count in a private variable?</li> <li>If you accumulate in a shared variable, do you protect it using mutexes, locks, or OpenMP critical sections?</li> </ul> </li> </ul>"},{"location":"lab4/#c-do-you-see-any-performance-gains-compared-to-the-sequential-version","title":"c) Do you see any performance gains compared to the sequential version?","text":""},{"location":"lab4/#d-rerun-the-provided-experiments-and-compare-with-your-previous-sequential-result","title":"d) Rerun the provided experiments, and compare with your previous sequential result.","text":"Running the experiment(s)<pre><code>./run_all.sh parallel\n</code></pre> <p>Is your program faster using OpenMP? If not, ensure you are correctly running multiple threads.</p>"},{"location":"lab4/#e-is-the-performance-stable-is-there-any-impact-on-the-accuracy-of-your-estimator","title":"e) Is the performance stable? Is there any impact on the accuracy of your estimator?","text":"<p>If performance is unstable, investigate thread affinity and how to bind threads to specific cores.</p> <p>On Intel CPUs, check if your processor has Performance (P) and Efficiency (E) cores, as this can affect timing consistency.</p> Checking your CPU Model Name<pre><code>lscpu | grep \"Model name\"\n</code></pre>"},{"location":"lab4/#e-bonus-for-students-with-an-heterogeneous-cpu-efficiency-e-cores-and-performance-p-cores","title":"e - Bonus) For students with an heterogeneous CPU (Efficiency (E) Cores and Performance (P) Cores)","text":"<p>Using both E and P cores can introduce instabilities in your measurements, load balancing issues, and should be avoided. Run the following to check your CPU topology: Checking CPU Topology<pre><code>lscpu --all --extended\n\n# CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE    MAXMHZ   MINMHZ       MHZ\n#   0    0      0    0 0:0:0:0          yes 5100.0000 800.0000  800.0000\n#   1    0      0    0 0:0:0:0          yes 5100.0000 800.0000  800.0000\n#   2    0      0    1 4:4:1:0          yes 5100.0000 800.0000  800.0000\n#   3    0      0    1 4:4:1:0          yes 5100.0000 800.0000  800.0000\n#   4    0      0    2 8:8:2:0          yes 5100.0000 800.0000  800.0000\n#   5    0      0    2 8:8:2:0          yes 5100.0000 800.0000  800.0000\n#   6    0      0    3 12:12:3:0        yes 5100.0000 800.0000  800.0000\n#   7    0      0    3 12:12:3:0        yes 5100.0000 800.0000  800.0000\n#   8    0      0    4 16:16:4:0        yes 5300.0000 800.0000  843.1520\n#   9    0      0    4 16:16:4:0        yes 5300.0000 800.0000  800.0000\n#  10    0      0    5 20:20:5:0        yes 5300.0000 800.0000 2329.4590\n#  11    0      0    5 20:20:5:0        yes 5300.0000 800.0000  800.0000\n#  12    0      0    6 24:24:6:0        yes 5100.0000 800.0000  800.0000\n#  13    0      0    6 24:24:6:0        yes 5100.0000 800.0000  800.0000\n#  14    0      0    7 28:28:7:0        yes 5100.0000 800.0000  800.0000\n#  15    0      0    7 28:28:7:0        yes 5100.0000 800.0000  800.0000\n#  16    0      0    8 32:32:8:0        yes 3800.0000 800.0000  800.0000\n#  17    0      0    9 33:33:8:0        yes 3800.0000 800.0000  842.6700\n#  18    0      0   10 34:34:8:0        yes 3800.0000 800.0000  799.7680\n#  19    0      0   11 35:35:8:0        yes 3800.0000 800.0000  800.0000\n#  20    0      0   12 40:40:10:0       yes 3800.0000 800.0000  800.0000\n#  21    0      0   13 41:41:10:0       yes 3800.0000 800.0000  800.0000\n#  22    0      0   14 42:42:10:0       yes 3800.0000 800.0000  800.0000\n#  23    0      0   15 43:43:10:0       yes 3800.0000 800.0000  800.0000\n#  24    0      0   16 44:44:11:0       yes 3800.0000 800.0000  799.0440\n#  25    0      0   17 45:45:11:0       yes 3800.0000 800.0000  800.0000\n#  26    0      0   18 46:46:11:0       yes 3800.0000 800.0000  800.0000\n#  27    0      0   19 47:47:11:0       yes 3800.0000 800.0000  800.0000\n</code></pre></p> <p>You should be able to see that some cores have a higher Max Frequency (MAXMHZ) than other: those are typically performance cores. If you have hyperthreading enabled, you should also see that some cores share the same \"core id\": classically, only performance cores have hyperthreading enabled. From the previous output, we can deduce that cores 0-15 are P-cores and that hyperthreading is enabled. As such, we can run the following command to change the thread affinity of our shell to only use physical cores:</p> Restricting Thread Affinity<pre><code># Adjust the core list based on your actual topology\ntaskset -cp 0,2,4,6,8,10,12,14 $$\n# You can also use `hwloc-ls` or `lstopo` (from hwloc) for a visual map of your CPU topology.\n</code></pre> <p>From now on, all the executable run in this shell will inherit this thread affinity. Feel free to create an alias for this command inside your <code>.bashrc</code> for future use.</p> <p>You can also use the following variable for OpenMP: OMP Places for E/P cores<pre><code>OMP_PLACES=\"{0, 2, 4, 6, 8, 10, 12, 14}\"\n</code></pre></p>"},{"location":"lab4/#f-verify-your-implementations-strong-scaling","title":"f) Verify your implementation's strong scaling:","text":"Strong Scaling<pre><code>OMP_NUM_THREADS=1 ./piestimator 1000000\nOMP_NUM_THREADS=2 ./piestimator 1000000\nOMP_NUM_THREADS=4 ./piestimator 1000000\nOMP_NUM_THREADS=8 ./piestimator 1000000\n</code></pre> <p>Does using 2 threads instead of 1 make your code twice as fast? Propose an explanation.</p>"},{"location":"lab4/#g-verify-your-implementations-weak-scaling","title":"g) Verify your implementation's weak-scaling:","text":"Weak scaling<pre><code>OMP_NUM_THREADS=1 ./piestimator 1000000\nOMP_NUM_THREADS=2 ./piestimator 2000000\nOMP_NUM_THREADS=4 ./piestimator 4000000\nOMP_NUM_THREADS=8 ./piestimator 8000000\n</code></pre> <p>What results do you expect to see? Does that match your empirical observations? Propose an explanation.</p>"},{"location":"lab4/#5-summary","title":"5 - Summary","text":"<p>Upon completing this third lab, you should be able to:</p> <ul> <li> Explain the principle of Monte-Carlo algorithms and apply them to numerical estimation</li> <li> Implement and benchmark a Monte-Carlo estimator with statistical analysis (mean, stddev, min/max)</li> <li> Account for variability and noise in timing benchmarks</li> <li> Use OpenMP to parallelize a minimal compute-bound code and understand implications for thread safety</li> <li> Perform simple analysis of scaling behavior (strong/weak scaling)</li> <li> Understand thread affinity and hardware topology impacts on performance and timing stability</li> </ul>"},{"location":"lab5/","title":"Lab 5: Experimental Methodology and Scientific Reporting","text":"<p>The Kepler space telescope monitors the variation in the luminosity of distant stars using a photometer. The datasets are freely available online, and we will use them as a case study for this lab.</p> Kepler space telescope shortly after the assembly      (NASA/Troy Cryder)    <p>Kepler generates relatively simple datasets: a photon flux (the intensity of the received light) at specific dates for a particular stars. However, by carefully preprocessing this data and using specialized signal analysis techniques, we can make major discoveries.</p>"},{"location":"lab5/#0-report","title":"0 - Report","text":"<p>Take a look at <code>report.md</code>. You should complete this report as you go along the lab. Below is a short description on what is expected in each section:</p> <ul> <li>1) Environment and context<ul> <li>Give details on the machine you used for the experiments: CPU/Memory specifications, compiler version, python version, OS name and version, and any other details that helps characterize your setup.<ul> <li>You can use <code>lscpu</code>, <code>free -h</code>, <code>python --version</code>, <code>gcc --version</code>, ...</li> </ul> </li> <li>A brief description of the context of BLS, the kepler datasets, etc.</li> </ul> </li> <li> <p>2) Kepler result</p> <ul> <li>Include both a lightcurve plot and a phase-folding plot for Kepler 8.</li> <li>A single figure that includes a periodogram for all the provided Kepler datasets.</li> </ul> </li> <li> <p>3) Profiling results</p> <ul> <li>Generate stability plots using <code>./scripts/stability.py</code> and include them in the report.</li> <li>If your machine supports RAPL measurement: Give the approximate energy consumption of the BLS algorithm on the Kepler 8 dataset. </li> <li>If your machine does not support RAPL measurement:<ul> <li>State this explicity in your report.</li> <li>Try on another machine if possible</li> <li>... OR replace this experiment by a weak scaling plot if you can't get RAPL to work.</li> </ul> </li> <li>Give the perf results for <code>instructions,cycles,cache-references,cache-misses</code> of <code>bls(...)</code> on the Kepler-8 dataset.</li> <li>Include the strong scaling plot</li> </ul> </li> </ul>"},{"location":"lab5/#provided-files","title":"Provided files","text":"Path Description <code>data/</code> Pre-processed Kepler dataset for this lab <code>libbls/</code> Box Least Square (BLS) Python library for transit detection. (CMake) <code>scripts/</code> Python/bash scripts for plotting and data analysis that you will have to complete during the lab <code>setup_env.sh</code> Helper script to setup the python environment and various env. variables <code>build_library.sh</code> Helper script to run CMake for the BLS library"},{"location":"lab5/#1-plotting-and-data-analysis","title":"1 - Plotting and data analysis","text":"<p>Kepler generates time-series, that is data indexed by a timestep. First, look at the data inside <code>data/Kepler-8_light_curve.csv</code>. The <code>time</code> column denotes the time in days since the satellite reference. The <code>flux</code> column is the normalized measured luminosity of the Kepler 8 star at a given date.</p>"},{"location":"lab5/#a-setup-python","title":"a) Setup python","text":"<p>Run the following: Setup bash environment<pre><code>source ./setup_env.sh\n</code></pre></p>"},{"location":"lab5/#b-plot-the-evolution-of-luminosity","title":"b) Plot the evolution of luminosity","text":"<p>Write a <code>scripts/plot_luminosity.py</code> script that:</p> <ul> <li>Can be called with <code>./scripts/plot_luminosity.py ./results kepler-*</code> where * is an id (i.e., kepler-8, kepler-17, etc.)</li> <li>Fetches the corresponding dataset in <code>data/</code></li> <li>Plots the dataset using <code>matplotlib</code> (x: Time (days), y: Flux)</li> <li>Save the plots as <code>results/luminosity_kepler-*.png</code></li> </ul> <p>Ensure the script is executable using <code>chmod +x &lt;file&gt;</code> and that the file starts with the shebang <code>#!/usr/bin/env python3</code> </p> <p>Tip</p> <p>Ensure that the <code>results</code> folder exists before saving to it. You can use <code>os.makedirs(&lt;path&gt;, exists_ok=True)</code> in your script.</p>"},{"location":"lab5/#c-run-the-previous-script-for-the-kepler-8-dataset-what-do-you-observe","title":"c) Run the previous script for the Kepler 8 dataset. What do you observe ?","text":""},{"location":"lab5/#d-refine-your-previous-plot","title":"d) Refine your previous plot","text":"<p>Make sure that:</p> <ul> <li>The axes are clearly labeled</li> <li>The x and y ticks are easily readable and properly spaced (<code>np.linspace</code>)</li> <li>The plot includes a title, legend, and uses a <code>tight</code> or <code>constrained</code> layout.</li> <li>The figure has an appropriate aspect ratio (width to height)</li> </ul> <p>The final plot could look something like this:</p> Kepler 8 Light curve"},{"location":"lab5/#e-give-a-possible-explanation-for-the-periodic-dips-in-luminosity","title":"e) Give a possible explanation for the periodic dips in luminosity","text":"<p>On the previous light curve, we observe that the luminosity appears to \"dip\" sharply at regular intervals. What could cause this periodic phenomenon ?</p> <p>Note</p> <p>Remember that the \"flux\" variable is the observed light intensity for a given star, from the telescope point of view.</p>"},{"location":"lab5/#f-implement-phase-folding-light-curve","title":"f) Implement phase folding light curve","text":"<p>Phase folding is a simple technique to visualize periodic signals: we fold the data over a given period so that the signals overlap, highlighting patterns.</p> Phase Folding<pre><code># Load the data here using pandas, store in a `data` variable\n# Period to fold over\nperiod = 0.8\n\n # We phase by the period, and divide by period to go in the [0, 1] range\nphase = (data[\"time\"] % period) / period\nphase = phase - 0.5 # Center the phase\nsort_idx = np.argsort(phase)\nphase_sorted = phase[sort_idx]\nflux_sorted = data[\"flux\"].iloc[sort_idx]\n\nphase = np.concatenate([phase_sorted, phase_sorted+1]) # Double plotting to improve visualization\nflux = np.concatenate([flux_sorted, flux_sorted])\n\n# Combine everying back to a DataFrame for plotting !\ndf = pd.DataFrame({\"phase\": phase, \"flux\": flux})\n</code></pre> <p>Implement a <code>scripts/phase_folding.py</code> script that plots the phase-folded light curve (x: phase, y: flux). </p> <p>It should be used like so: <code>./scripts/phase_folding.py ./results kepler-* &lt;period&gt;</code>.</p> <p>Optionally, you can also plot a binned mean on top of the phase-folded light curve:</p> Phase folding: Binning<pre><code>from scipy import stats\nbins = 200\n# Here, we bin the data using 200 bins. In each bin, we compute the mean flux.\nbin_means, bin_edges, _ = stats.binned_statistic(phase, flux, statistic='mean', bins=bins)\n# We compute the x coordinate of each bins, by taking the center point\nbin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\nax.plot(bin_centers, bin_means, color=\"red\", lw=1.5)\n</code></pre>"},{"location":"lab5/#h-run-the-previous-script-by-phase-folding-over-the-kepler-8b-period-koi_period","title":"h) Run the previous script by phase folding over the Kepler 8b Period (<code>koi_period</code>).","text":"<p>Check the file <code>data/kepler-8_known_planets.json</code>. This json contains information about the lonely Kepler 8b exoplanet, in the Kepler 8 star system. This exoplanet orbits its parent star every 3.52 days.</p> <ul> <li>What do you observe ? </li> <li>Does the light \"dips\" overlap or is your plot noisy?</li> <li>What can we say about the relationship between the light \"dips\" and Kepler 8b orbit ?</li> <li>Draw a simple diagram describing what's happening during the light dips.<ul> <li>(Optionnal) Draw a sad emoji face on Kepler 8b, because she's alone, in a vast, vast universe.</li> </ul> </li> </ul> Phase folded Kepler 8 Light curve"},{"location":"lab5/#2-box-least-square","title":"2 - Box Least Square","text":"<p>The Box Least Square (BLS) signal processing algorithm is used to detect the transit of exoplanets in front of their stars by searching for characteristic box-shaped dips in the luminosity at regular frequency.</p> <p>The provided library implements a Python &lt;-&gt; C interface so that you can call BLS from a Python script. It also simplifies the loading and manipulation of the data, which can be done in Python, while the C code focuses on high-performance analysis.</p>"},{"location":"lab5/#a-run-the-provided-build_librarysh-script","title":"a) Run the provided <code>build_library.sh</code> script","text":""},{"location":"lab5/#b-write-a-scriptsrun_blspy-script-for-kepler-data","title":"b) Write a <code>scripts/run_bls.py</code> script for kepler data","text":"<p>The BLS library can be used like so: run_bls.py<pre><code>import bls\n# ... Load data here\nmatch = bls.bls(data[\"time\"].values.astype(np.float64), data[\"flux\"].values.astype(np.float64),\n    1.0, 4, 250, 0.02, 0.15, 100)\n</code></pre> BLS is very sensitive to the hyperparameters, so you must use the ones provided here.</p> <p>Ensure that your script can be called with <code>scripts/run_bls.py kepler-*</code> where * is an id (i.e., kepler-8, kepler-17, etc.), and that it reports the BLS match. Check that the BLS output is consistent with the characteristics of the Kepler 8b exoplanet.</p>"},{"location":"lab5/#c-build-a-periodogram","title":"c) Build a periodogram","text":"<p>The BLS algorithm scans through a range of Orbital Periods, and computes a Power score for each candidates. The higher the power, the better the candidate. </p> <p>The function <code>bls.bls_periodogram(...)</code> returns a 2D array containing all the scores, which allows us to build a Periodogram. Pairs are stored as (Power, Period).</p> <ul> <li>Take a look at <code>scripts/periodogram.py</code> and understand the provided code snippets</li> <li>Plot a periodogram for Kepler 8, 17, 45 and 785. You are required to use subplots so that all periodograms are on the same figure. Feel free to lookup \"BLS Periodogram\" online to get an idea of the target results.</li> <li>Note that you should use the same arguments for <code>bls.bls_periodogram(...)</code> that the ones used for <code>bls.bls(...)</code>.</li> <li>Save the resulting plots as <code>results/all_periodograms.png</code></li> </ul> Principle behind the Kepler exoplanet detection system      Hannah R. Wakeford, Laura C. Mayorga    Characterizing the Dynamics and Chemistry of Transiting Exoplanets with the Habitable World Observatory (2025)"},{"location":"lab5/#d-look-at-libblssrcpyblsc-and-try-to-understand-how-we-are-interfacing-python-with-c-code","title":"d) Look at <code>libbls/src/pybls.c</code> and try to understand how we are interfacing Python with C code","text":"py_bls interface<pre><code>static PyObject *py_bls(PyObject *self, PyObject *args) {\n  PyArrayObject *t_arr, *f_arr;\n  double Pmin, Pmax, qmin, qmax;\n  int Np, Nq;\n\n  if (!PyArg_ParseTuple(args, \"O!O!ddiddi\", &amp;PyArray_Type, &amp;t_arr, &amp;PyArray_Type, &amp;f_arr, &amp;Pmin,\n                        &amp;Pmax, &amp;Np, &amp;qmin, &amp;qmax, &amp;Nq))\n    return NULL;\n\n  int N = (int)PyArray_DIM(t_arr, 0);\n  double *t = (double *)PyArray_DATA(t_arr);\n  double *f = (double *)PyArray_DATA(f_arr);\n\n  BLSResult res = bls(t, f, N, Pmin, Pmax, Np, qmin, qmax, Nq);\n\n  return Py_BuildValue(\"{s:d,s:d,s:d,s:d,s:d}\", \"period\", res.period, \"duration\", res.duration,\n                       \"phase\", res.phase, \"depth\", res.depth, \"power\", res.power);\n}\n</code></pre> <p>When we call <code>bls.bls(...)</code> from Python, all arguments are redirected to this C function. The <code>PyObject</code> self and args pointers are structures containing the arguments of the function.</p> <p>The <code>PyArg_ParseTuple(...)</code> call is used to \"unfold\" the Python arguments into C variables. The string <code>O!O!ddiddi</code> defines how the arguments should be interpreted:</p> <ul> <li><code>d</code> for double</li> <li><code>i</code> for integers (32bits <code>int</code>)</li> <li><code>O!</code> for objects (Here numpy arrays). Note that the <code>!</code> qualifiers adds a type check: the runtime would raise an error if the received object wasn't a <code>PyArray_Type</code>.</li> </ul> <p><code>PyArray_DIM</code> and <code>PyArray_DATA</code> are used to convert Python/Numpy arrays to C usable pointers and size pairs.</p> <p>The <code>Py_BuildValue(...)</code> call does the exact inverse process and builds Python values from C variables.</p> <p>Finally:</p> Hooking up C with Python<pre><code>static PyMethodDef BlsMethods[] = {\n    {\"bls\", py_bls, METH_VARARGS, \"Run the BLS algorithm.\"},\n    {\"bls_periodogram\", py_periodogram_bls, METH_VARARGS, \"Compute the BLS periodogram.\"},\n    {NULL, NULL, 0, NULL}};\n\nstatic struct PyModuleDef blsmodule = {PyModuleDef_HEAD_INIT, \"pybls\", NULL, -1, BlsMethods};\n\nPyMODINIT_FUNC PyInit_bls(void) {\n  import_array();\n  return PyModule_Create(&amp;blsmodule);\n}\n</code></pre> <p>Here, we create an array containing all the functions we want to make available from python. We then declare a Python module and a  <code>PyInit_bls(void)</code> function, that will be called by Python to load/initialize the module at runtime.</p>"},{"location":"lab5/#3-profiling-for-energy-and-performance-characterization","title":"3 - Profiling for energy and performance characterization","text":""},{"location":"lab5/#1-measuring-energy","title":"1. Measuring Energy","text":""},{"location":"lab5/#a-check-the-value-of-perf-event-paranoid","title":"a) Check the value of perf event paranoid","text":"<p>By default, the Linux kernel restricts access to some perf counters to prevent malicious usage. Check the value of the paranoid flag:</p> Perf event paranoid<pre><code>cat /proc/sys/kernel/perf_event_paranoid\n</code></pre> <p>Possible values are:</p> <ul> <li>-1 -&gt; unrestricted. All counters can be used system-wise.</li> <li>0 -&gt; All counters can be used on your own processes</li> <li>1 -&gt; Limited access to CPU counters</li> <li>2 -&gt; Only basic access</li> </ul> <p>For this lab, we want <code>perf_event_paranoid</code> to be set to -1 so we can access the RAPL counters.</p>"},{"location":"lab5/#b-disable-perf-event-paranoid","title":"b) Disable perf event paranoid","text":"<p>The following command will set <code>perf_event_paranoid</code> to be set to -1.</p> <p>However, you need <code>sudo</code> (admin rights) on your machine.</p> Perf event paranoid<pre><code>sudo sh -c 'echo -1 &gt; /proc/sys/kernel/perf_event_paranoid'\n</code></pre> <p>If the command fails (because you don't have sudo permissions) and <code>perf_event_paranoid</code> is not set to -1, you will not be able to proceed in this section. Skip to section 3.2 (Measuring performance metrics).</p>"},{"location":"lab5/#c-checking-availability-of-rapl-counters","title":"c) Checking availability of RAPL counters","text":"<p>Run the following: Checking for RAPL perf events<pre><code>perf list | grep \"energy-pkg\"\n  power/energy-pkg/                                  [Kernel PMU event]\n</code></pre></p> <p>If you don't see the <code>power/energy-pkg</code> line, this probably means that RAPL energy measurement isn't supported on your machine.  Skip to section 3.2 (Measuring performance metrics).</p>"},{"location":"lab5/#c-first-energy-measure","title":"c) First energy measure","text":"<p>What does the following command do ? Idle Energy Consumption<pre><code>perf stat -a -j -e power/energy-pkg/,power/energy-cores/ sleep 60\n</code></pre></p> <p>Ensure your machine is mostly idle and execute this command.</p> <p>Danger</p> <p>Classical hardware counters, like <code>instructions</code>, are per-process, whereas RAPL is system-wide: RAPL captures energy consumption for all processes currently running, not specifically by our application !</p>"},{"location":"lab5/#d-what-does-energy-pkg-measure-and-whats-the-unit-what-about-the-other-events","title":"d) What does <code>energy-pkg</code> measure, and what's the unit ? What about the other event(s) ?","text":"<p>Compute your machine idle power consumption: </p> \\[ P_{idle} = \\frac{\\mathrm{energy{\\text -}pkg}_{idle}}{t} \\] <p>Where \\(\\mathrm{energy{\\text -}pkg}_{idle}\\) is in Joules, and \\(t\\) is in seconds. This is the average power consumed by the CPU package while the machine is idle, in watts</p>"},{"location":"lab5/#e-measure-the-bls-algorithm-energy-consumption","title":"e) Measure the BLS algorithm energy consumption","text":"Workload Consumption<pre><code>time perf stat -r 5 -a -j -e power/energy-pkg/,power/energy-cores/ \\\n    ./scripts/run_bls.py kepler-8\n</code></pre> <p>Note that the <code>-r 5</code> flag causes <code>perf</code> to perform five meta-repetitions. The <code>time</code> command reports the sum of the timings for all runs. Calculate the effective power and energy consumption for BLS.</p> \\[ P_{effective} = \\frac{\\mathrm{energy{\\text -}pkg_{BLS}}}{t_{BLS}} - P_{idle} \\]"},{"location":"lab5/#2-measuring-performance-metrics","title":"2. Measuring performance metrics","text":"<p>What does the following command do ? Measuring performance<pre><code>time perf stat -r 5 -e instructions,cycles,cache-references,cache-misses \\\n    ./scripts/run_bls.py kepler-8\n</code></pre></p> <p>Execute this command and answer the following questions.</p>"},{"location":"lab5/#a-whats-the-observed-variability-in-the-execution-time","title":"a) What's the observed variability in the execution time ?","text":""},{"location":"lab5/#b-whats-the-mean-instructions-cycle","title":"b) What's the mean instructions / cycle ?","text":""},{"location":"lab5/#c-is-the-application-compute-or-memory-intensive","title":"c) Is the application compute or memory intensive ?","text":"<p>In summary:</p> <ul> <li>Memory-intensive applications have low instructions per cycle and high memory metrics</li> <li>Compute-intensive applications are vectorized (high instructions per cycle) and fully utilize threads.   Memory usage is relatively low because the arithmetic density is high.</li> </ul>"},{"location":"lab5/#d-what-is-costlier-running-the-bls-algorithm-or-loading-the-dataset","title":"d) What is costlier: running the BLS algorithm, or loading the dataset ?","text":"<p>Run the following: Perf Record<pre><code>perf record -g -- python3 ./scripts/run_bls.py kepler-8\nperf report\n</code></pre></p> <p>You can move around the perf report using the arrow keys, and you can press <code>+</code> to expand a particular call tree.</p> <p>How much time is spent in the BLS algorithm ?</p> <p>Which would be more time-efficient for an engineer: optimizing the data loading process or optimizing the BLS algorithm?</p>"},{"location":"lab5/#3-strong-scaling-analysis","title":"3. Strong scaling analysis","text":"<p>Take a look at <code>scripts/strong_scaling.py</code>. It contains a code snippet for running the <code>scripts/run_bls.py</code> script with a given number of threads. Our goal is now to build a strong scaling plot.</p>"},{"location":"lab5/#a-look-and-try-to-understand-the-purpose-of-the-scriptsstabilitypy-script","title":"a) Look and try to understand the purpose of the <code>scripts/stability.py</code> script","text":"<p>What are we measuring ? What information does this script provide about our environment?</p>"},{"location":"lab5/#b-modify-stabilitypy-to-measure-the-stability-of-run_blspy","title":"b) Modify <code>stability.py</code> to measure the stability of <code>run_bls.py</code>","text":"<p>To further assess the stability of our setup, we should try to measure the distribution of multiple runs of <code>run_bls.py</code></p> <p>Modify the script to:</p> <ul> <li>Load the kepler 8 Dataset, and subsample it (Reduce the size to ~2k randomly selected samples)<ul> <li>Be sure to sort the dataset after subsampling by using <code>df.sort_values(by=\"time\")</code> !</li> </ul> </li> <li>Save the previous dataset, and execute <code>run_bls.py</code> on the subsampled dataset, measuring the time.</li> <li>Repeat the previous measurements approximately 100 times, and generate a distribution plot using seaborn.     You can use a boxplot, kdeplot, violin plot, histogram, etc.     Save the raw data to <code>results/stability_bls.csv</code></li> </ul> <p>If your machine is stable, the performance distribution should follow a normal distribution.</p> Examples of different distribution plots.      The stability results here were gathered on a laptop that was in-use, thus the measures are quite unstable."},{"location":"lab5/#c-modify-scriptsstrong_scalingpy-to-build-a-strong-scaling-plot","title":"c) Modify <code>scripts/strong_scaling.py</code> to build a strong scaling plot","text":"<p>Save the plot to <code>results/&lt;date&gt;/strong_scaling.png</code> where date is obtained via: Formatting a timestamp<pre><code>import time\ndate = time.strftime(\"%Y_%m_%d-%H_%M_%S\") # e.g. 2025_08_28-12_08_40\nos.makedirs(f\"results/{date}/\", exist_ok=True)\n</code></pre></p>"},{"location":"lab5/#5-summary","title":"5 - Summary","text":"<p>Upon completing this fifth lab, you should know how to:</p> <ul> <li> Build simple plots using python and matplotlib</li> <li> Improve a plot with titles, labels, formatting</li> <li> Manipulate simple data formats (csv, json)</li> <li> Use <code>perf</code> for measuring energy and performance hardware counters</li> <li> Use <code>perf</code> sampling profiler to understand an application hotspots</li> <li> Organise a report around data analysis</li> </ul>"},{"location":"lab6/","title":"Lab 6 - AI Project (1) SGEMM Kernel Optimization","text":""},{"location":"lab6/#1-implementing-a-naive-sgemm","title":"1 - Implementing a naive SGEMM","text":"<p>We work on multiplication of dense matrices. This algorithm is everywhere (ML, data analysis, 3D engines, physics).  Here we focus on single-precision floating point SGEMM as used in many ML workloads.</p> <p>The operation we want to implement is:</p> \\[ \\text{RES} = A \\times B + C \\] <p></p> sgemm.h<pre><code>/**\n * @brief SGEMM (Single-precision General Matrix Multiply)\n *\n * Performs the matrix multiplication operation:\n * RES = A*B + C\n *\n * @param A Input matrix A with dimensions M x K (row-major order)\n * @param B Input matrix B with dimensions K x N\n * @param C Input matrix C with dimensions M x N\n * @param M Number of rows in matrix A and C\n * @param N Number of columns in matrix B and C\n * @param K Number of columns in matrix A and rows in matrix B\n * @param RES Output matrix with dimensions M x N, where the result will be stored\n */\nvoid sgemm(\n    const float *A,\n    const float *B,\n    const float *C,\n    float *RES,\n    size_t M,\n    size_t N,\n    size_t K);\n</code></pre> <p>Matrices are stored in row-major order (C-style). That is, the elements of each row are stored in contiguous memory locations. For example, a 3x3 matrix:</p> \\[ \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{pmatrix} \\] <p>is stored in memory as:</p> \\[ [a_{11}, a_{12}, a_{13}, a_{21}, a_{22}, a_{23}, a_{31}, a_{32}, a_{33}] \\]"},{"location":"lab6/#1-creating-random-matrices","title":"1. Creating random matrices","text":"<p>Inside <code>src/</code> you find starter files: <code>sgemm.h</code>, <code>sgemm.c</code>, <code>main.c</code>. Inside <code>tests/</code> you will find <code>test_runner.c</code> that contains a starter test harness.</p>"},{"location":"lab6/#a-implement-random_matrix-inside-sgemmc-to-generate-random-matrices","title":"a) Implement <code>random_matrix</code> inside <code>sgemm.c</code> to generate random matrices.","text":""},{"location":"lab6/#b-add-a-test-in-test_sgemmc-that-checks-that-random_matrix-correctly-generates-matrices-with-values-in-the-range-10-10-call-the-test-from-test_runnerc","title":"b) Add a test in <code>test_sgemm.c</code> that checks that <code>random_matrix</code> correctly generates matrices with values in the range \\([-1.0, 1.0]\\). Call the test from <code>test_runner.c</code>.","text":""},{"location":"lab6/#c-create-a-cmakelisttxt","title":"c) Create a <code>CMakeList.txt</code>","text":"<p>It should:</p> <ul> <li> <p>build an executable <code>gemm</code> from <code>main.c</code> and <code>sgemm.c</code>.</p> </li> <li> <p>build an executable <code>test_runner</code> from <code>test_runner.c</code> and <code>sgemm.c</code> using the <code>unity</code> testing framework as shown in previous labs.</p> </li> <li> <p>allow changing the build type (Debug/Release) from the command line with different sets of flags.</p> </li> </ul> <p>Check that your code compiles and that the test passes.</p>"},{"location":"lab6/#2-naive-sgemm-implementation","title":"2. Naive SGEMM implementation","text":""},{"location":"lab6/#a-implement-the-naive-version-of-sgemm-in-sgemmc-using-three-nested-loops-in-the-order-i-j-k","title":"a) Implement the naive version of <code>sgemm</code> in <code>sgemm.c</code> using three nested loops in the order (i, j, k).","text":""},{"location":"lab6/#b-add-various-tests-in-test_sgemmc-that-checks-that-your-sgemm-implementation-correctly-computes-the-product-of-small-matrices-with-variying-sizes-and-shapes-eg-2x2-3x1-1x3-3x3","title":"b) Add various tests in <code>test_sgemm.c</code> that checks that your <code>sgemm</code> implementation correctly computes the product of small matrices with variying sizes and shapes (e.g. 2x2, 3x1, 1x3, 3x3).","text":""},{"location":"lab6/#c-check-that-your-tests-pass","title":"c) Check that your tests pass.","text":"<p>Now that you have a working implementation, you can call <code>sgemm</code> from <code>main.c</code> with random matrices of different sizes passed as command line arguments.</p>"},{"location":"lab6/#2-performance-measurement-harness","title":"2 - Performance measurement harness","text":"<p>Create a new directory <code>performance/</code> and add two files: <code>performance.sh</code> and <code>plot.py</code>. For both the measurement harness and plotting script follow best practices taught in lecture 4.</p>"},{"location":"lab6/#1-measure-performance-and-energy-consumption","title":"1. Measure performance and energy consumption","text":"<p>We will use <code>perf</code> to measure the elapsed time and energy consumption of our SGEMM implementation.</p> <p>Example command to measure duration_time and energy consumption of a run of <code>sgemm</code> with M=512, K=512, N=512: <pre><code>perf stat -r 3 -a -e duration_time,power/energy-pkg/ ./sgemm naive 512 512 512\n</code></pre></p> <p>The <code>-r 3</code> option tells <code>perf</code> to repeat the measurement 3 times and report the mean and variance.</p> <p>Tip</p> <ul> <li> <p>You can use <code>LC_NUMERIC=C</code> to ensure that the decimal separator is a dot (<code>.</code>) instead of a comma (<code>,</code>) in the output of <code>perf</code>, which is useful for parsing the output.</p> </li> <li> <p>The <code>-a</code> option can be used to measure system-wide events, which is needed for energy measurements.</p> </li> <li> <p>For added stability you can use <code>taskset</code> to pin the process to a specific CPU core and disable frequency scaling with <code>cpufreq-set</code>.</p> </li> </ul>"},{"location":"lab6/#2-write-a-script-compare-sizessh","title":"2. Write a script <code>compare-sizes.sh</code>","text":"<p>The script should:</p> <ul> <li>run <code>sgemm</code> with N=512, K=512 and M with increasing sizes (e.g. 100, 200, 300, ..., 3000).</li> <li>measure the time and energy consumption using <code>perf stat</code> for each run.</li> <li>define a variable <code>REPETITIONS</code> that controls how many times each configuration is repeated. <code>perf</code> supports the <code>-r</code> option to repeat measurements. It outputs the mean and variance of the measurements, allowing to plot error bars.</li> <li>define a variable <code>EVENTS</code> to specify the events to measure (e.g. <code>duration_time,power/energy-pkg/</code>).</li> <li>aggregate all measures into a <code>.json</code> file or <code>.csv</code> file.</li> </ul> <p>Tip</p> <p>If you are using <code>json</code>, you can use <code>jq</code> to process the output of <code>perf stat</code> and aggregate the results together with <code>jq -s 'flatten(1)'</code>.</p>"},{"location":"lab6/#3-write-a-script-plot-sizespy","title":"3. Write a script <code>plot-sizes.py</code>","text":"<p>The script should:</p> <ul> <li>read the output of the previous script and produces a plot with two y-axes:</li> <li>left y-axis: time (with error bars)</li> <li>right y-axis: energy consumption (with error bars)</li> </ul>"},{"location":"lab6/#a-run-your-scripts-and-produce-an-initial-plot","title":"a) Run your scripts and produce an initial plot.","text":""},{"location":"lab6/#b-inspect-your-plot-do-you-see-any-knee-points-can-you-explain-them-hint-consider-cache-sizes","title":"b) Inspect your plot. Do you see any knee points? Can you explain them? (Hint: consider cache sizes.)","text":"<p>Tip</p> <p>You can use the following commands to inspect your CPU and memory topology:</p> <ul> <li> <p><code>lstopo</code> to view memory/core topology (if available).</p> </li> <li> <p><code>cat /proc/cpuinfo</code> to inspect core model, cache sizes.</p> </li> <li> <p>do not forget to include the units in your plot (e.g. ns, ms, s, mJ, J, ...).</p> </li> </ul>"},{"location":"lab6/#3-optimizations","title":"3 - Optimizations","text":"<p>Note</p> <p>In this section we will implement different optimizations step by step. After each optimization, you should run your performance measurement harness to see the impact of the optimization on performance and energy consumption.</p> <p>Each new version of the code should be implemented in a new function (e.g. <code>sgemm_ikj</code>, <code>sgemm_blocked</code>, <code>sgemm_omp</code>), and you should add a command line option to <code>main.c</code> to select which version to run.</p> <p>Do not forget to add tests for each new version of <code>sgemm</code> in <code>test_sgemm.c</code>.</p>"},{"location":"lab6/#1-vectorization","title":"1. Vectorization","text":"<p>Modern CPUs have SIMD units (SSE/AVX/AVX512). The compiler can generate SIMD code, but the code and data layout must be friendly.</p>"},{"location":"lab6/#a-compiler-flags","title":"a) Compiler flags","text":"<ol> <li>Compile with <code>-O3 -march=native</code> and inspect compiler vectorization reports (GCC/Clang flags to tell whether loops were vectorized: <code>-fopt-info-vec-optimized</code> or <code>-Rpass=loop-vectorize</code>).</li> <li>Measure runtime/energy after changes. Observe and explain differences.</li> </ol> <p>Tip</p> <p>If the compiler does not vectorize your loops, try to understand why. Common reasons include:</p> <ul> <li> <p>Data dependencies that prevent reordering.</p> </li> <li> <p>Pointer aliasing (use <code>restrict</code> keyword if applicable).</p> </li> <li> <p>Complex control flow inside loops.</p> </li> </ul>"},{"location":"lab6/#b-loop-order-stride","title":"b) Loop order / stride","text":"<p>Ensure the innermost loop has contiguous memory accesses.</p> <ol> <li>Create a new function <code>sgemm_ikj</code> and call it from <code>main.c</code>. Add a command line option to select which version to run.</li> <li>Change the loop order to (i, k, j).</li> <li>Measure runtime/energy and compare to the naive version. Explain differences.</li> </ol>"},{"location":"lab6/#c-add-two-new-scripts-to-performance","title":"c) Add two new scripts to <code>performance/</code>","text":"<ul> <li><code>compare-optimizations.sh</code>: compares the naive and <code>ikj</code> implementations for a chosen matrix size.</li> <li><code>plot-optimizations.py</code>: plots the performance and energy consumption of each implementation. Include error bars as before.</li> </ul> <p>Tip</p> <p>You can use the same structure as <code>compare-sizes.sh</code> and <code>plot-sizes.py</code>, but modify them to compare different implementations for a fixed matrix size. Make the scripts generic enough to easily add new implementations in the future.</p>"},{"location":"lab6/#2-cache-blocking","title":"2. Cache blocking","text":"<p>Access to <code>B</code> in the naive algorithm typically strides through columns \u2014 poor locality for row-major storage. Blocking (tiling) improves cache reuse.</p>"},{"location":"lab6/#a-measure-last-level-cache-events-for-the-naive-implementation","title":"a) Measure last-level-cache events for the naive implementation.","text":"<p>Use <code>perf stat</code> with <code>LLC-loads</code> and <code>LLC-stores</code> events. Example:</p> <pre><code>perf stat -e LLC-loads,LLC-stores ./sgemm sgemm_ikj 1280 512 512\n</code></pre>"},{"location":"lab6/#b-implement-a-blocked-matrix-multiplication","title":"b) Implement a blocked matrix multiplication","text":"<p>Define a block size <code>BLOCK_SIZE</code> and implement <code>sgemm_blocked</code> in <code>sgemm.c</code>:</p> sgemm.h<pre><code>#define BLOCK_SIZE 512  /* block size, tune for your CPU cache */\n/* sgemm_blocked has the same prototype as sgemm and sgemm_ikj */ \nvoid sgemm_blocked( ... );\n</code></pre> <p>Do not forget to call it from <code>main.c</code> and add a command line option to select it.</p> <p>Tip</p> <ul> <li> <p>Ensure <code>n</code> is a multiple of <code>BLOCK</code> for simplicity; either restrict inputs or pad matrices (padding can be an exercise).</p> </li> <li> <p>Inside blocks, use simple triple loops over block elements (ensure indices map correctly to row-major layout).</p> </li> </ul>"},{"location":"lab6/#c-measure-llc-loadsstores-for-the-blocked-version-and-compare-to-naive","title":"c) Measure LLC loads/stores for the blocked version and compare to naive.","text":""},{"location":"lab6/#d-measure-energy-and-time-for-blocked-version-and-compare","title":"d) Measure energy and time for blocked version and compare.","text":"<p>Modify your <code>compare-optimizations.sh</code> script to include the blocked version in the comparisons.</p>"},{"location":"lab6/#e-why-does-blocking-reduce-llc-misses-and-energytime","title":"e) Why does blocking reduce LLC misses and energy/time?","text":""},{"location":"lab6/#3-parallelization-with-openmp","title":"3 - Parallelization with OpenMP","text":"<p>In this section, we focus on parallelizing the blocked matrix multiplication using OpenMP. While we will only target the CPU for now, other parallel runtimes (e.g., MPI, TBB) could also be explored in the future.</p>"},{"location":"lab6/#1-implement-a-parallel-version-using-openmp","title":"1. Implement a parallel version using OpenMP","text":"<ul> <li>Use OpenMP to parallelize the blocked version of <code>sgemm</code>.</li> <li> <p>Parallelize over blocks of rows for better load balancing.</p> <p>Add <code>#include &lt;omp.h&gt;</code> at the top of your <code>sgemm.c</code> file. Use the following pragma to parallelize the outer loop: <pre><code>#pragma omp parallel for schedule(static)\n</code></pre></p> </li> </ul> <p>Tip</p> <p>Static scheduling is often a good choice for matrix multiplication since the workload is evenly distributed, but you can experiment with other scheduling strategies (e.g., dynamic, guided) to see their impact on performance.</p>"},{"location":"lab6/#2-compile-and-measure-performance","title":"2. Compile and measure performance","text":"<p>You can set the number of threads with the <code>OMP_NUM_THREADS</code> environment variable. For example, to use 8 threads:</p> <pre><code>export OMP_NUM_THREADS=8  # Adjust based on your machine\n</code></pre> <p>Plot the performance and energy consumption of your OpenMP implementation for different numbers of threads (e.g., 1, 2, 4, 8, 16). Add your scripts to <code>performance/</code> and produce a scalability plot.</p>"},{"location":"lab6/#3-tune-openmp-settings","title":"3. Tune OpenMP settings","text":"<p>You can experiment with different OpenMP settings to optimize performance and energy consumption. For example, you can set the thread affinity to bind threads to specific cores: <pre><code>export OMP_NUM_THREADS=6\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close        # or 'spread' to experiment\n</code></pre></p>"},{"location":"lab6/#4-analyze-results","title":"4. Analyze results","text":"<ul> <li>Compare energy consumption and runtime to the single-threaded version.</li> <li>Experiment with different numbers of threads and observe the impact on energy and time.</li> <li>Is energy per run higher or lower with more threads? Why?</li> <li>How does memory bandwidth affect scaling as more threads are added?</li> </ul> <p>Warning</p> <p>Keep all your performance results, plots, and analysis for your final report that you will submit at the end of lab 7. </p>"},{"location":"lab6/#4-making-a-library","title":"4 - Making a Library","text":"<p>Now that you have optimized your SGEMM implementation, you can package it as a library. In the next lab, we will use your library to implement a simple Neural Network inference engine. </p> <p>To do so, modify your <code>CMakeLists.txt</code> to build a static library <code>libsgemm.a</code> from <code>sgemm.c</code> and <code>sgemm.h</code>. Ensure the prototypes in the public header <code>sgemm.h</code> are well commented in Doxygen style. Modify the <code>CMakeLists.txt</code> so that users of the library know where to find the header files.</p>"},{"location":"lab6/#5-for-further-study","title":"5 - For further study","text":"<p>The matrix product we have implemented is efficient, but it is possible to push optimizations even further. Here are some references and avenues if this work interests you:</p> <ul> <li> <p>Algorithmic improvements such as Strassen allow to trade multiplications for additions, which in some cases can be beneficial.</p> </li> <li> <p>Cache-oblivious algorithms (recursive tiling based on space-filling curves). Instead of using blocking techniques, which must be parameterized by a fixed block size; it is possible to implement matrix multiplication to preserve locality regardless of scale. This is called a cache-oblivious algorithm in English. For matrix multiplication, such an algorithm can be obtained by reordering the elements according to the order given by the Lebesgue curve. This allows obtaining very efficient implementations for matrices whose dimension is a power of two.  </p> </li> <li> <p>Advanced BLAS. You can compare your implementation with <code>cblas_sgemm</code> from optimized vendor libraries (OpenBLAS, Intel MKL). You can even try using <code>cublas</code> library to target GPUs.</p> </li> </ul>"},{"location":"lab6/#7-summary","title":"7 - Summary","text":"<p>Upon completing this lab, you should know how to:</p> <ul> <li> Write a naive implementation of the SGEMM algorithm.</li> <li> Measure performance and energy consumption using <code>perf</code>.</li> <li> Optimize the SGEMM algorithm using vectorization, loop reordering, cache blocking, and parallelization with OpenMP.</li> <li> Analyze the impact of optimizations on performance and energy consumption.</li> </ul>"},{"location":"lab7/","title":"Lab 7: Projet IA - Part 2 - Neural Networks Inference Engine","text":"<p>In lab 6, you implemented and optimized an sgemm library for matrix-matrix multiplication. In this lab, you will use this library to implement a simple neural network inference engine. This lab is inspired by Michal Pitr inference engine project released under the MIT license. Michal Pitr offers a C++ implementation and a series of excellent blog posts explaining the implementation. Feel free to read them, but you must implement the lab in C and produce your own code.</p>"},{"location":"lab7/#1-design-of-the-inference-engine","title":"1 - Design of the inference engine","text":"<p>We want to recognize handwritten digits from the MNIST dataset using a neural network. The neural network has been trained using PyTorch and the ONNX format is used to export the model.</p> <p></p> <p>The model takes as input a 28x28 grayscale image (784 pixels) and outputs a vector of 10 probabilities, one for each digit (0 to 9). The predicted digit is the one with the highest probability.</p> <p>The model architecture is described as a computation graph. Each node in the graph represents a layer in the neural network. The edges represent the flow of data between layers. Our inference engine must support the following four operations:</p> <ul> <li>Flatten: This layer reshapes the input tensor into a 2D matrix. The 28x28 image is flattened into a 784x1 vector.</li> <li>Add: This layer performs element-wise addition of two matrices. The operation can be expressed as <code>Y = A + B</code>, where <code>A</code> and <code>B</code> are the input matrices.</li> <li>Gemm: This is a fully connected layer. It performs a linear transformation of the input data using weights and biases. The operation can be expressed as <code>Y =  A * B + C</code>, where <code>A</code> is the input matrix, <code>B</code> is the weight matrix, <code>C</code> is the bias vector, and <code>alpha</code> and <code>beta</code> are scaling factors.</li> <li>Relu: This layer applies the ReLU (Rectified Linear Unit) activation function, which sets all negative values to zero. The operation can be expressed as <code>Y = max(0, X)</code>, where <code>X</code> is the input matrix.</li> </ul> <p>Multiple pre-trained models are available in the <code>models/</code> directory. They differ by their architecture (number and type of layers and number of neurons per layer). You can use the web application Netron to visualize the models.</p> <p>Your inference engine must be able to execute any of these models. It should be as efficient as possible. You have already optimized the sgemm library in the previous lab. Now you must implement and optimize the other operations (add, relu, flatten) and the overall scheduling of the model.</p>"},{"location":"lab7/#2-projet-structure","title":"2 - Projet structure","text":"<p>Here are the contents of the project starting directory:</p> <ul> <li> <p><code>models/</code>: Directory containing pre-trained ONNX models.</p> </li> <li> <p><code>image-test-set/</code>: Directory containing test images in binary format.</p> <ul> <li> <p><code>image_0.ubyte</code> to <code>image_99.ubyte</code>: 100 test images of handwritten digits in binary format; extracted from the MNIST database. Each file contains 784 bytes (28x28 pixels) representing a grayscale image.</p> </li> <li> <p><code>labels.csv</code>: Text file containing the expected labels for the test images separated by commas. Each value corresponds to the label of the image with the same index (e.g., the first label is the label for image_0.ubyte).</p> </li> </ul> </li> <li> <p><code>external/onnx/</code>: Directory containing an ONNX protobuf low-level parser. We use this auto-generated parser to read the ONNX format. You don't need to modify anything in this directory.</p> </li> <li> <p><code>src/</code>: Directory containing the source code.</p> <ul> <li> <p><code>main.c</code>: Minimal main program.</p> </li> <li> <p><code>parse_model.h</code> and <code>parse_model.c</code>: High level code for parsing input images and ONNX models.</p> </li> </ul> </li> <li> <p><code>tests/</code>: Directory containing unit tests for <code>parse_model.c</code>.</p> </li> <li> <p><code>CMakeLists.txt</code>: CMake build configuration file.</p> </li> </ul>"},{"location":"lab7/#3-the-parse_model-module","title":"3 - The parse_model module","text":"<p>The parse_model module provides utilities to load MNIST input images and read ONNX models, exposing a small C API to access model graph info and weight tensors.</p>"},{"location":"lab7/#data-types","title":"Data types","text":"<ul> <li> <p><code>struct matrix_t</code>: represents a matrix (weights or data) with fields:</p> <ul> <li><code>bool weights</code>, true if the <code>matrix</code> represents weights, false for data. </li> <li><code>size_t M, N</code>, dimensions of the matrix (<code>MxN</code>).</li> <li><code>float *data</code>, pointer to the matrix data in row-major order.</li> </ul> </li> <li> <p><code>struct model_t</code>: opaque model handle (internal representation hidden).</p> </li> <li> <p>The caller is responsible for freeing returned objects using <code>free_matrix()</code> and <code>free_model()</code>.</p> </li> </ul>"},{"location":"lab7/#core-functions","title":"Core functions","text":"<ul> <li><code>struct matrix_t* read_mnist_input(char *filename)</code>: load a 28\u00d728 MNIST image as a 784\u00d71 matrix.</li> <li><code>void free_matrix(struct matrix_t *m)</code>: free <code>a matrix_t</code>.</li> <li><code>struct model_t* read_onnx_model(const char *filename)</code>: parse an ONNX file and return a model handle.</li> <li><code>void free_model(struct model_t *model)</code>: free the model handle.</li> <li><code>struct matrix_t* get_weight_matrix(struct model_t *model, char *name)</code>: retrieve an initializer (weight/bias) by name.</li> </ul>"},{"location":"lab7/#graph-inspection-helpers","title":"Graph inspection helpers","text":"<ul> <li><code>char * get_graph_input_name(struct model_t *model)</code>: graph input name.</li> <li><code>char * get_graph_output_name(struct model_t *model)</code>: graph output name.</li> <li><code>size_t get_number_nodes(struct model_t *model)</code>: number of nodes in the graph.</li> <li><code>char * get_node_name(struct model_t *model, size_t node_pos)</code>: node name.</li> <li><code>char * get_op_type(struct model_t *model, size_t node_pos)</code>: node operator type (e.g. \"Gemm\", \"Relu\", \"Add\", or \"Flatten\").</li> <li><code>char * get_output_name(struct model_t *model, size_t node_pos)</code>: node output name.</li> <li><code>size_t get_number_inputs(struct model_t *model, size_t node_pos)</code>: number of inputs for a node.</li> <li><code>char * get_input_name(struct model_t *model, size_t node_pos, size_t input_pos)</code>: name of a node input.</li> </ul>"},{"location":"lab7/#example-click-on-the-image-to-see-it-in-full-size","title":"Example (Click on the image to see it in full size)","text":"<p>For the model <code>models/mnist.onnx</code>,</p> <ul> <li> <p><code>get_number_nodes(model)</code> returns <code>15</code>.</p> </li> <li> <p>The first node is a Flatten layer named \"/Flatten\", </p> </li> <li> <p><code>get_node_name(model, 0)</code> returns <code>\"/Flatten\"</code> and <code>get_op_type(model, 0)</code> returns <code>\"Flatten\"</code>. </p> </li> <li> <p>Flatten has one input : <code>get_number_inputs(model, 0)</code> returns <code>1</code>.</p> <ul> <li>The input comes from the graph input named <code>\"onnx::Flatten_0\"</code>: <code>get_input_name(model, 0, 0)</code> returns <code>\"onnx::Flatten_0\"</code>.</li> </ul> </li> <li> <p>Flatten has one output. In our models all layers have a single output, but it can be consumed by multiple nodes (as is the case for the first /Relu layer in our example).</p> <ul> <li>The output of the Flatten layer is named <code>\"/Flatten_output_0\"</code>: <code>get_output_name(model, 0)</code> returns <code>\"/Flatten_output_0\"</code>.</li> </ul> </li> <li> <p>The second node is a fully connected layer named \"/l1/Gemm\",</p> <ul> <li> <p><code>get_node_name(model, 1)</code> returns <code>\"/l1/Gemm\"</code> and <code>get_op_type(model, 1)</code> returns <code>\"Gemm\"</code>.</p> </li> <li> <p>Gemm has three inputs (the input data, the weights, and the biases): <code>get_number_inputs(model, 1)</code> returns <code>3</code>.</p> <ul> <li> <p>The first input comes from the output of the previous Flatten layer: <code>get_input_name(model, 1, 0)</code> returns <code>\"/Flatten_output_0\"</code>.</p> </li> <li> <p>The second input is the weight matrix named <code>\"l1.weight\"</code>: <code>get_input_name(model, 1, 1)</code> returns <code>\"l1.weight\"</code>.</p> </li> <li> <p>The third input is the bias vector named <code>\"l1.bias\"</code>: <code>get_input_name(model, 1, 2)</code> returns <code>\"l1.bias\"</code>.</p> </li> </ul> </li> <li> <p>The output of the Gemm layer is named <code>\"/l1/Gemm_output_0\"</code>: <code>get_output_name(model, 1)</code> returns <code>\"/l1/Gemm_output_0\"</code>.</p> </li> </ul> </li> </ul> <p>Note</p> <p>In the provided ONNX models weight matrices are stored in column-major order.  The <code>get_weight_matrix()</code> function takes care of this for you by transposing the data and returning a <code>matrix_t</code> structure in row-major order.</p> <p>We can avoid this transposition by changing the sgemm library to support column-major order in the second matrix, but this is not required for this project.</p>"},{"location":"lab7/#4-implementation-tasks","title":"4 - Implementation tasks","text":""},{"location":"lab7/#1-implementing-the-four-layer-operations","title":"1. Implementing the four layer operations","text":"<p>Implement functions to execute the four operations (Flatten, Add, Gemm, Relu) using the <code>matrix_t</code> data structure. You should create a new source file (e.g., <code>inference.c</code>).</p> <p>Some hints:</p> <ul> <li> <p>Flatten is straightforward: it just reshapes the input matrix. You can create a new <code>matrix_t</code> with the appropriate dimensions and copy the data.</p> </li> <li> <p>Add and Relu can be implemented using simple loops. You can optimize them using techniques like loop unrolling or SIMD intrinsics.</p> </li> <li> <p>For Gemm, you can use the optimized sgemm library you implemented in the previous lab. You should copy the relevant files and modify the CMakeLists.txt to build and link against the sgemm library.</p> </li> </ul> <p>Tip</p> <p>Make sure to add relevant unit tests for each operation in the <code>tests/</code> directory.</p>"},{"location":"lab7/#2-implementing-a-scheduler-for-the-model-graph","title":"2. Implementing a scheduler for the model graph","text":"<p>Implement a function to execute the model graph. The function should take a <code>model_t</code> and an input <code>matrix_t</code>, and return the output <code>matrix_t</code>.</p> <p>You can use the graph inspection helpers to traverse the graph and execute each node. You will need to maintain a mapping from node output names to <code>matrix_t</code> objects to keep track of intermediate results.</p> <p>The order of execution must respect the dependencies between nodes. Thus you need to ensure that the execution follows a topological order.</p> <p>Tip</p> <p>Include tests for the scheduler in the <code>tests/</code> directory.</p>"},{"location":"lab7/#3-testing-and-benchmarking","title":"3. Testing and benchmarking","text":"<p>Use the provided test images and labels to validate your implementation. Include tests in the <code>tests/</code> directory to verify the correctness of each operation and the overall model execution.</p> <p>Measure the accuracy of each model by comparing the predicted labels with the expected labels from <code>labels.txt</code>. Report the accuracy for each model.</p> <p>Note</p> <p>To compute a simple accuracy metric, you can count the number of correct predictions and divide by the total number of images. More advanced metrics (precision, recall, F1-score) are not required for this lab.</p> <p>You can use the <code>main.c</code> file to implement a simple command-line interface to load a model, load an input image, execute the model, and print the predicted label.</p>"},{"location":"lab7/#4-optimization-and-performance-analysis","title":"4. Optimization and performance analysis","text":"<p>Optimize the implementation for performance and energy efficiency. You can experiment with parallel execution of the graph nodes, pipelining, and other optimization techniques. o Benchmark the execution time and energy consumption of your implementation. Include your benchmarking scripts in a <code>performance/</code> directory.</p> <p>Tip</p> <p>Keep the performance measurements for each version of your code so you can compare them in the final report. Do not forget to track versions with git commits.</p> <p>The project is open-ended and there are many possible approaches to implement and optimize the inference engine. The goal is to learn about neural networks, ONNX format, and performance optimization techniques. Feel free to explore and experiment with different ideas. If you have access to a GPU, you can also try to implement parts of the inference engine using CUDA or OpenCL.</p>"},{"location":"lab7/#5-submission","title":"5 - Submission","text":"<p>Submit your implementation in GitHub Classroom. Make sure to include a <code>README.md</code> file with instructions on how to build and run your code, as well as any dependencies required.</p> <p>Include a report (in PDF format, <code>Report.pdf</code>) describing your implementation, the optimizations you applied, and the performance results you obtained. Be sure to include the graphs and analysis of the SGEMM optimizations from the previous lab as well. The report should be around 3000 words (about 5 pages in 11pt without figures, tables and code excerpts).</p>"},{"location":"lecture1/","title":"Introduction to Software Engineering for HPC and AI","text":"<p> Download as slides \ud83d\udce5 </p>"},{"location":"lecture1/#syllabus","title":"Syllabus","text":"<ul> <li>Lecture 1: Introduction &amp; Development Environment</li> <li>Lecture 2: Performance Aware C Computing </li> <li>Lecture 3: Building, Testing and Debugging Scientific Software</li> <li>Lecture 4: Experimental Design, Profiling and Performance/Energy Optimization</li> <li>Lecture 5: HPC for AI &amp; Environmental Impact of Computation</li> </ul> <p>Project: Inference Engine for a Deep Network</p>"},{"location":"lecture1/#introduction-development-environment","title":"Introduction &amp; Development Environment","text":"<ul> <li>Principles of software engineering applied to HPC and AI.</li> <li>Introduction to computing architectures.</li> <li>Development tools: shell scripts, package management, Git, IDEs, etc.</li> </ul>"},{"location":"lecture1/#analytical-solution-to-the-2-body-problem","title":"Analytical solution to the 2-Body Problem","text":"<p>Consider two particles with masses \\(m_1\\) and \\(m_2\\) at positions \\(x_1\\) and \\(x_2\\) under gravitational interaction.</p> \\[m_1.a_1 = -\\frac{G.m_1.m_2}{\\|x_1 - x_2\\|^3} (x_2 - x_1)\\] \\[m_2.a_2 = -\\frac{G.m_1.m_2}{\\|x_1 - x_2\\|^3} (x_1 - x_2)\\] <p>Solved by Bernoulli in 1734, \\(x_1\\) and \\(x_2\\) can be expressed as simple equations that depend on time, masses, and initial conditions.</p>"},{"location":"lecture1/#why-simulate-the-n-body-problem","title":"Why Simulate the n-Body Problem?","text":"<ul> <li>For \\(n=3\\) or more, no practical analytical solution exists.</li> <li>Even advanced mathematical solutions (e.g., Sundman, 1909) are too slow for real use.</li> <li>Computer simulations allow us to study the motion of many interacting particles.<ul> <li>Efficient algorithms (e.g., Barnes-Hut, Fast Multipole) make large-scale simulations possible.</li> </ul> </li> <li>HPC is essential to simulate realistic systems in physics, astronomy, and AI.<ul> <li>Simulation + HPC = understanding complex systems!</li> </ul> </li> </ul>"},{"location":"lecture1/#naive-n-body-simulation-in-c","title":"Naive n-Body Simulation in C","text":"<pre><code>// Compute accelerations based on gravitational forces\nfor (int i = 0; i &lt; num_particles; i++) {\n  double ax = 0.0, ay = 0.0, az = 0.0;\n  for (int j = 0; j &lt; num_particles; j++) {\n    if (i == j) continue;\n    double dx = p[j].x - p[i].x;\n    double dy = p[j].y - p[i].y;\n    double dz = p[j].z - p[i].z;\n    double d_sq = dx * dx + dy * dy + dz * dz;\n    double d = sqrt(d_sq);\n    double f = G * p[i].m * p[j].m / (d_sq * d);\n    ax += f * dx / p[i].m;\n    ay += f * dy / p[i].m;\n    az += f * dz / p[i].m;\n  }\n  p[i].ax = ax;\n  p[i].ay = ay;\n  p[i].az = az;\n}\n</code></pre>"},{"location":"lecture1/#naive-n-body-simulation-in-c_1","title":"Naive n-Body Simulation in C","text":"<p>Introduce a small time step <code>dt</code> and update positions based on gravitational forces.</p> <pre><code>// Update velocity and positions based on computed accelerations\nfor (int i = 0; i &lt; num_particles; i++) {\n  p[i].vx += p[i].ax * dt;\n  p[i].vy += p[i].ay * dt;\n  p[i].vz += p[i].az * dt;\n\n  p[i].x += p[i].vx * dt;\n  p[i].y += p[i].vy * dt;\n  p[i].z += p[i].vz * dt;\n}\n</code></pre>"},{"location":"lecture1/#high-performance-computing","title":"High Performance Computing","text":"<p>Fugaku (2020, 442 petaflops, 7.3 million cores)</p> <ul> <li>n-body: integrates 1.45 trillion particules per second.</li> </ul> <p>How to achieve such performance?</p> <ul> <li>Algorithmic improvements:<ul> <li>Use tree-based methods (Barnes-Hut) to reduce complexity from \\(O(n^2)\\) to \\(O(n \\log n)\\) or better.</li> </ul> </li> <li>Parallelization: distribute computation accross many cores.</li> <li>Vectorization: use SIMD instructions to process multiple data points in parallel.</li> <li>Data locality: optimize data access patterns to minimize memory latency and maximize cache usage.</li> </ul> <p>Compiler optimizations, performance tuning, hardware acceleration are also crucial.</p>"},{"location":"lecture1/#hpc-architectures","title":"HPC Architectures","text":""},{"location":"lecture1/#cpu-instruction-set-isa-quick-review","title":"CPU &amp; Instruction Set (ISA) \u2014 quick review","text":"<ul> <li>CPU core executes instructions; machine state = registers, program counter and flags.</li> <li>Assembly encodes the instructions; compilers translate high-level code into the ISA.<ul> <li>types: arithmetic/logical, load/store (memory), control flow (branches, calls), system calls</li> </ul> </li> <li>Registers are the fastest storage; register pressure influence performance.</li> </ul>"},{"location":"lecture1/#example-intel-core2-architecture","title":"Example: Intel Core2 Architecture","text":""},{"location":"lecture1/#pipeline-memory-hierarchy-interrupts","title":"Pipeline, Memory Hierarchy &amp; Interrupts","text":"<ul> <li>Pipeline increases instruction throughput, classic 5-stages:<ul> <li>Fetch \u2192 Decode \u2192 Execute \u2192 Memory \u2192 Write-back</li> </ul> </li> <li>Hazards: data hazards (dependencies), control hazards (branch prediction), resource conflicts.</li> <li>Memory hierarchy: registers \u2192 L1/L2/L3 caches \u2192 DRAM \u2192 persistent storage; spatial and temporal locality drive cache effectiveness.</li> <li>Buses, coherence and NUMA: cross-socket memory access has higher latency; cache coherence and memory bandwidth limit scalability.</li> <li>Interrupts and exceptions: asynchronous interrupts signal external events; exceptions/traps handle synchronous faults; the OS performs context switching and servicing.</li> </ul>"},{"location":"lecture1/#multicore-memory-hierarchy-more-in-next-lecture","title":"Multicore memory hierarchy (more in next lecture ...)","text":""},{"location":"lecture1/#system-hierarchy-physical-view","title":"System hierarchy (physical view)","text":"<ul> <li>Chassis \u2192 rack \u2192 node \u2192 socket \u2192 core \u2192 hardware thread: a multi-level physical organization.</li> <li>Nodes often include accelerators (GPUs, TPUs, FPGAs) and have their own memory (DRAM, sometimes HBM).</li> <li>Heterogeneous hardware and multi-level parallelism are the norm in modern HPC systems.</li> </ul>"},{"location":"lecture1/#interconnects-and-io","title":"Interconnects and I/O","text":""},{"location":"lecture1/#interconnects","title":"Interconnects","text":"<ul> <li>Two key metrics: latency (small-message cost) and bandwidth (sustained transfer rate).</li> <li>Fabrics: Ethernet, InfiniBand, Omni-Path; features to note: RDMA, kernel bypass, hardware offloads.</li> <li>Network topology affects routing, contention and scalability.</li> </ul>"},{"location":"lecture1/#storage-and-io","title":"Storage and I/O","text":"<ul> <li>Parallel file systems provide shared high-throughput storage for HPC jobs.</li> <li>Design I/O to avoid bottlenecks and to fit checkpoint/analysis cadence (collective I/O, buffer in NVMe).</li> </ul>"},{"location":"lecture1/#levels-of-parallelism-mapping","title":"Levels of parallelism &amp; mapping","text":"<ul> <li>Inter-node (distributed memory) via MPI; intra-node threading via OpenMP/pthreads; SIMD/vector units for data-level parallelism.</li> <li>Accelerator offload (CUDA/HIP/OpenCL) creates hybrid MPI+X application patterns.</li> <li>Choose mapping to match algorithm characteristics (communication-heavy vs compute-dense).</li> </ul>"},{"location":"lecture1/#software-stack-operations-current-trends","title":"Software stack, operations &amp; current trends","text":"<ul> <li>Typical stack: compilers, MPI/libfabric, math libraries, system libs</li> <li>Job schedulers (Slurm/PBS) handle resource allocation, queues and batch workflows</li> </ul>"},{"location":"lecture1/#shell-basics-and-scripting","title":"Shell Basics and Scripting","text":""},{"location":"lecture1/#what-is-the-shell","title":"What is the Shell?","text":"<ul> <li>Definition: A shell is a command-line interface to interact with the operating system.</li> <li>Purpose: Execute commands, run programs, and automate tasks.</li> <li>Common Shells: <code>bash</code>, <code>zsh</code>, <code>fish</code>, <code>sh</code>.</li> <li>Why Learn It?<ul> <li>Essential for HPC environments.</li> <li>Enables automation and efficient system interaction.</li> </ul> </li> </ul>"},{"location":"lecture1/#basic-shell-commands","title":"Basic Shell Commands","text":"<ul> <li>File and Directory Management:<ul> <li><code>ls</code>: List files and directories.</li> <li><code>cd &lt;directory&gt;</code>: Change directory.</li> <li><code>pwd</code>: Print current working directory.</li> <li><code>mkdir &lt;directory&gt;</code>: Create a new directory.</li> <li><code>rm &lt;file&gt;</code>: Remove a file.</li> </ul> </li> <li>File Viewing:<ul> <li><code>cat &lt;file&gt;</code>: Display file contents.</li> <li><code>less &lt;file&gt;</code>: View file contents interactively.</li> <li><code>head &lt;file&gt;</code>: Show the first 10 lines.</li> <li><code>tail &lt;file&gt;</code>: Show the last 10 lines.</li> </ul> </li> </ul>"},{"location":"lecture1/#redirections","title":"Redirections","text":"<ul> <li>Standard Input/Output:<ul> <li><code>&lt;</code>: Redirect input from a file.</li> <li><code>&gt;</code>: Redirect output to a file (overwrite).</li> <li><code>&gt;&gt;</code>: Append output to a file.</li> </ul> </li> <li>Examples:<ul> <li><code>cat file.txt &gt; output.txt</code>: Save contents of <code>file.txt</code> to <code>output.txt</code>.</li> <li><code>grep \"error\" log.txt &gt;&gt; errors.txt</code>: Append lines containing \"error\" to <code>errors.txt</code>.</li> </ul> </li> </ul>"},{"location":"lecture1/#pipes","title":"Pipes","text":"<ul> <li>Definition: Pipes (<code>|</code>) connect the output of one command to the input of another.</li> <li>Examples:<ul> <li><code>ls | grep \".txt\"</code>: List <code>.txt</code> files.</li> <li><code>cat file.txt | wc -l</code>: Count the number of lines in <code>file.txt</code>.</li> </ul> </li> <li>Why Use Pipes?<ul> <li>Combine simple commands to perform complex tasks.</li> <li>Avoid creating intermediate files.</li> </ul> </li> </ul>"},{"location":"lecture1/#variables-and-environment","title":"Variables and Environment","text":"<ul> <li>Variables:<ul> <li><code>VAR=value</code>: Define a variable.</li> <li><code>$VAR</code>: Access the variable's value.</li> </ul> </li> <li> <p>Environment Variables:</p> <ul> <li><code>echo $HOME</code>: Display the home directory.</li> <li><code>export PATH=$PATH:/new/path</code>: Add a directory to the <code>PATH</code>.</li> </ul> </li> <li> <p>Example:</p> </li> </ul> <pre><code>NODES=4\nPROGRAM=\"my_hpc_program\"\necho \"Running $PROGRAM on $NODES MPI nodes...\"\nmpirun -np $NODES ./$PROGRAM\n</code></pre>"},{"location":"lecture1/#writing-a-simple-script","title":"Writing a Simple Script","text":"<ul> <li>What is a Script?<ul> <li>A file containing a sequence of shell commands.</li> </ul> </li> <li> <p>Creating a Script:</p> <ol> <li>Create a file: <code>vim script.sh</code>.</li> <li> <p>Add commands:</p> <pre><code>#!/bin/bash\necho \"Hello, World!\"\n</code></pre> </li> <li> <p>Make it executable: <code>chmod +x script.sh</code>.</p> </li> <li>Run it: <code>./script.sh</code>.</li> </ol> </li> </ul>"},{"location":"lecture1/#conditional-statements","title":"Conditional Statements","text":"<pre><code>if [ -f \"config.json\" ]; then\n  echo \"config.json exists. Running the HPC program...\"\n  ./my_hpc_program --config=config.json\nelse\n  echo \"Error: config.json does not exist.\"\nfi\n</code></pre>"},{"location":"lecture1/#loops","title":"Loops","text":"<pre><code>for i in {1..5}; do\n  echo \"Running simulation with parameter set $i...\"\n  ./my_hpc_program --config=config_$i.json\ndone\n</code></pre>"},{"location":"lecture1/#functions-in-shell-scripts","title":"Functions in Shell Scripts","text":"<pre><code>run_simulation() {\n  echo \"Starting with config file: $1 and $2 nodes...\"\n  mpirun -np $2 ./simulation_program --config=$1\n  echo \"Simulation completed.\"\n}\nrun_simulation \"simulation_config.json\" 8\n</code></pre>"},{"location":"lecture1/#debugging-and-best-practices","title":"Debugging and Best Practices","text":"<ul> <li>Debugging:<ul> <li>Run with <code>bash -x script.sh</code> to trace execution.</li> <li>Use <code>set -e</code> to exit on errors as the first command.</li> </ul> </li> <li>Best Practices:<ul> <li>Use comments (<code>#</code>) to explain code.</li> <li>Write reusable functions.</li> <li>Check for errors (<code>if [ $? -ne 0 ]; then</code>).</li> <li>Test scripts on small inputs before scaling up.</li> </ul> </li> </ul>"},{"location":"lecture1/#package-management","title":"Package Management","text":""},{"location":"lecture1/#package-management-overview","title":"Package Management: Overview","text":"<ul> <li>Problem Solved: Simplifies software installation, updates, and dependency management.</li> <li>Ensures compatibility between libraries and applications.</li> <li>Tracks installed software versions for easy upgrades or rollbacks.</li> <li>Examples: <code>dnf</code> (Fedora/RHEL), <code>apt</code> (Debian/Ubuntu).</li> </ul>"},{"location":"lecture1/#package-managers-for-hpc","title":"Package Managers for HPC","text":"<ul> <li>Cluster-Specific Tools: <code>spack</code>, <code>guix</code> enable software installation without root privileges.</li> <li>Useful in HPC environments where users lack admin rights.</li> <li>Manage multiple versions of libraries and tools for reproducibility.</li> <li>Facilitate deployment of complex scientific software stacks.</li> </ul>"},{"location":"lecture1/#language-specific-containers","title":"Language-Specific &amp; Containers","text":"<ul> <li>Language-Specific Managers: <code>pip</code> (Python), <code>cargo</code> (Rust) simplify language ecosystem management.</li> <li>Containers: Tools like Docker/Singularity encapsulate software and dependencies.</li> <li>Enable portability across systems and reproducible environments.</li> <li>Virtualization/containerization is becoming popular in modern HPC workflows.</li> </ul>"},{"location":"lecture1/#version-control-systems","title":"Version Control Systems","text":""},{"location":"lecture1/#what-is-version-control","title":"What is Version Control?","text":"<p>Version control involves tracking and managing the changes made to project files.</p> <p>Each version is associated with a date, an author, and a message. Developers can work on a copy corresponding to a specific version.</p>"},{"location":"lecture1/#objectives","title":"Objectives","text":"<ul> <li>Enhance communication among developers (track code evolution, messages).</li> <li>Isolate experimental developments (work branches).</li> <li>Ensure code stability (stable version on the main branch, ability to revert to a stable version).</li> <li>Manage releases (tags for specific versions).</li> </ul>"},{"location":"lecture1/#vocabulary-for-versions","title":"Vocabulary for Versions","text":"<ul> <li>Version \u2014 a recorded state or revision in the project's history.</li> <li>Commit \u2014 a snapshot of the project at a given version with metadata.</li> <li>Branch \u2014 an independent line of development (use one branch per feature/experiment).</li> <li>Tag \u2014 a stable label pointing to a specific commit (e.g., releases).</li> <li>Diff / Patch \u2014 textual representation of changes between versions.</li> <li>Conflict \u2014 incompatible concurrent edits that must be resolved manually.</li> </ul>"},{"location":"lecture1/#vocabulary-for-storage","title":"Vocabulary for Storage","text":"<ul> <li>Repository \u2014 storage of the project's history (local <code>.git</code> and metadata).</li> <li>Clone \u2014 a full local copy of the repository including history.</li> <li>Working copy \u2014 editable files checked out from a repository.</li> <li>Index / Staging area \u2014 area to stage selected changes for the next commit.</li> <li>Remote \u2014 hosted repository (e.g., <code>origin</code> on GitHub/GitLab) for collaboration.</li> </ul>"},{"location":"lecture1/#distributed-vcs","title":"Distributed VCS","text":"<p>Distributed Version Control System (DVCS)</p>"},{"location":"lecture1/#advantages","title":"Advantages","text":"<ul> <li>Multiple repositories can exist.</li> <li>Version control can be performed locally.</li> <li>No need for network connectivity.</li> </ul>"},{"location":"lecture1/#examples","title":"Examples","text":"<ul> <li>Mercurial (2005) (Mozilla, Python, OpenOffice.org)</li> <li>Bazaar (2005) (Ubuntu, MySQL)</li> <li>Git (2005) (Linux Kernel, Debian, VLC, Android, Gnome, Qt)</li> </ul>"},{"location":"lecture1/#introduction-to-dvcs-git","title":"Introduction to DVCS: Git","text":""},{"location":"lecture1/#history","title":"History","text":"<ul> <li>Git was created in 2005 to version the development of the Linux kernel.</li> <li>Designed as a distributed version control system (replacing BitKeeper).</li> </ul>"},{"location":"lecture1/#context","title":"Context","text":"<ul> <li>Widely used by projects: Linux Kernel, Debian, VLC, Android, Gnome, Qt, etc.</li> <li>Accessible via command-line interface.</li> <li>Graphical tools available: gitk, qgit.</li> </ul>"},{"location":"lecture1/#core-principles-of-git","title":"Core Principles of Git","text":"<ul> <li>Git does not store differences between commits (unlike SVN).</li> <li>Instead, Git stores snapshots of the project's file hierarchy at each commit.</li> <li>These snapshots are based on hierarchical structures of objects.</li> <li>Git operations revolve around manipulating these objects.</li> </ul>"},{"location":"lecture1/#hash","title":"Hash","text":"<ul> <li>Each object has a unique hash (SHA1).</li> <li>Git identifies identical objects by comparing their hashes.</li> <li>The same content stored in different repositories will always have the same hash.</li> </ul>"},{"location":"lecture1/#git-objects","title":"Git Objects","text":"<p>Object types include:</p> <ul> <li>Blob: Stores file data.</li> <li>Tree: References a list of other trees or blobs.</li> <li>Commit: Points to a single tree, representing a project snapshot. Includes metadata like timestamp, author, and parent commits.</li> <li>Tag: Labels a specific commit for easy reference.</li> </ul>"},{"location":"lecture1/#commit-representation","title":"Commit Representation","text":"<p>(Git Community Book, p13) </p>"},{"location":"lecture1/#commit-structure","title":"Commit Structure","text":"<p>(Git Community Book, p14) </p>"},{"location":"lecture1/#git-repository","title":"Git Repository","text":"<ul> <li>.git directory:  <ul> <li>Stores the project's history.  </li> <li>Contains metadata for version control.  </li> <li>Located at the root of the project.</li> </ul> </li> </ul>"},{"location":"lecture1/#working-directory","title":"Working Directory","text":"<ul> <li>Current version of project files.  </li> <li>Files are replaced or removed by Git during branch or version changes.</li> </ul>"},{"location":"lecture1/#index-staging-area","title":"Index / Staging Area","text":"<ul> <li>Bridge between the working directory and the repository.  </li> <li>Used to group changes for a single commit.  </li> <li>Only the index content is committed, not the working directory.</li> </ul>"},{"location":"lecture1/#basic-commands","title":"Basic Commands","text":"<ul> <li><code>git init</code>: Initialize a Git repository.</li> <li><code>git clone &lt;repository&gt;</code>: Clone a repository.</li> <li><code>git status</code>: Check the status of the working directory and staging area.</li> <li><code>git add &lt;file&gt;</code>: Stage changes for commit.</li> <li><code>git commit</code>: Commit staged changes.</li> </ul>"},{"location":"lecture1/#basic-commands-continued","title":"Basic Commands (Continued)","text":"<ul> <li><code>git pull</code>: Update local repository from remote.</li> <li><code>git push</code>: Push local commits to remote repository.</li> <li><code>git log</code>: View commit history.</li> <li><code>git checkout &lt;hash&gt;</code>: Switch to a specific commit using its SHA1 hash.</li> <li><code>git branch &lt;branchName&gt;</code>: Create a new branch.</li> </ul>"},{"location":"lecture1/#branches-purpose","title":"Branches: Purpose","text":"<ul> <li>Work on changes that diverge from the main branch or another branch.</li> <li>Isolate experimental developments.</li> <li>Avoid disrupting shared development efforts.</li> <li>Version parallel developments with the option to merge later.</li> </ul>"},{"location":"lecture1/#branches-commands","title":"Branches: Commands","text":"<ul> <li><code>git branch</code> or <code>git checkout -b &lt;branchName&gt;</code>: Create a new branch.</li> <li><code>git checkout &lt;branchName&gt;</code>: Switch to an existing branch.</li> <li><code>git merge &lt;branchName&gt;</code>: Merge a branch into the current branch.</li> <li><code>git branch -d &lt;branchName&gt;</code>: Delete a branch.</li> <li><code>git branch</code>: List all branches and show the current branch.</li> </ul>"},{"location":"lecture1/#conflict-management","title":"Conflict Management","text":"<ul> <li> <p>Conflict: Occurs during branch merging when two changes affect the same lines.</p> </li> <li> <p>Resolution Steps:</p> </li> <li>Merge is paused.</li> <li>Conflict zones are marked in the file.</li> <li>Edit the file to resolve conflicts by choosing one version or combining changes.</li> <li>Verify and validate the resolution.</li> <li>Commit the resolved conflict.</li> </ul>"},{"location":"lecture1/#correction-methods","title":"Correction Methods","text":"<ul> <li>Undo Changes: Use <code>git reset</code> to discard modifications.</li> <li>Amend Last Commit: Use <code>git commit --amend</code> to modify the previous commit.</li> <li>Branch-Based Correction: Create a new branch from a specific version and work from there.</li> <li>Rewrite History: Use <code>git rebase</code> to edit commits and history.</li> </ul>"},{"location":"lecture1/#warning","title":"Warning","text":"<ul> <li>Rewriting History: Interactive rebasing is risky. Only rewrite commits that haven't been pushed to a remote repository. Prefer branch-based corrections for safer handling.</li> </ul>"},{"location":"lecture1/#centralized-collaboration","title":"Centralized Collaboration","text":"<p>(image from Joomla's documentation)</p>"},{"location":"lecture1/#decentralized-with-central-repository","title":"Decentralized with Central Repository","text":"<p>(image from Joomla's documentation)</p>"},{"location":"lecture1/#fully-decentralized-collaboration","title":"Fully Decentralized Collaboration","text":"<p>(image from Joomla's documentation)</p>"},{"location":"lecture1/#best-practices-for-collaborative-development","title":"Best Practices for Collaborative Development","text":""},{"location":"lecture1/#before-development","title":"Before Development","text":"<ul> <li> <p>Define a developer charter:</p> <ul> <li>Naming conventions for files, functions, variables.</li> <li>Standards for technical documentation and comments.</li> <li>Indentation rules (tabs vs spaces).</li> </ul> </li> <li> <p>Establish a version control strategy.</p> </li> </ul>"},{"location":"lecture1/#during-development","title":"During Development","text":"<ul> <li>Create isolated commits (one commit = one coherent change).</li> <li>Write concise commit messages (max 60 characters summarizing the change).</li> <li>Add detailed commit descriptions if necessary.</li> <li>Regularly update your working copy.</li> <li>Share updates with team members.</li> </ul>"},{"location":"lecture1/#references","title":"References","text":"<ul> <li>The Art of HPC by Victor Eijkhout</li> <li>What Every Programmer Should Know About Memory by Ulrich Drepper</li> <li>The Git Community Book</li> <li>Tech Talk: Linus Torvalds on Git (YouTube)</li> <li>TOP500 Supercomputers</li> <li>Modern Operating Systems by Andrew S. Tanenbaum</li> <li>GIT Lecture Notes by Thomas Dufaud (IUT V\u00e9lizy - UVSQ)</li> </ul>"},{"location":"lecture2/","title":"C for High Performance","text":"<p> Download as slides \ud83d\udce5 </p>"},{"location":"lecture2/#why-c-c-python","title":"Why C, C++, Python ?","text":"<p>Programming occurs at several abstraction levels from the hardware</p> <p></p>"},{"location":"lecture2/#why-c-c-python_1","title":"Why C, C++, Python ?","text":"<ul> <li>Layers close to metal are harder to program...<ul> <li>But they offer maximum control and performance</li> </ul> </li> <li>High-level abstraction maximize productivity...<ul> <li>But have significant overhead, less control over performance</li> </ul> </li> </ul> <p>In practice; we often combine multiple languages</p> <ul> <li>C for performance critical sections, python for higher level APIs</li> </ul>"},{"location":"lecture2/#why-c-c-python_2","title":"Why C, C++, Python ?","text":""},{"location":"lecture2/#c-programming-operations-and-typing","title":"C Programming - Operations and Typing","text":"<p>C is a strongly typed imperative language:</p> <pre><code>int main() {\n  int a = 5;\n  int b = 10;\n\n  int c = a + b;\n  float d = c / a;\n  float e = (float)c / a;\n\n  int f = a * a * a * a;\n\n  return 0;\n}\n</code></pre> <p><code>main</code> is the program entry point.</p>"},{"location":"lecture2/#c-programming-functions","title":"C Programming - Functions","text":"<pre><code>#include &lt;stdio.h&gt; // For printf(...)\n\nint sum_and_square(int a, int n) {\n  int tmp = a + n;\n  return tmp * tmp;\n}\n\nint main() {\n  int a = 5;\n  int b = 4;\n  int c = sum_and_square(a, b);\n  int d = sum_and_square(3, 9);\n\n  // Print the result to the console\n  printf(\"(5+4)**2: %d\\n\", c);\n  printf(\"(3+9)**2: %d\\n\", d);\n  return 0;\n}\n</code></pre>"},{"location":"lecture2/#c-programming-loops","title":"C Programming - Loops","text":"<p>Implementation C de \\(\\sum_{i=1}^{100}{i}\\)</p> <p><pre><code>#include &lt;stdio.h&gt; // For printf(...)\n\nint sum_range(const int start, const int end) {\n  int sum = 0;\n  // Consider start = 0; end = 100\n  // For i starting at 0; while i &lt;= 100; increment i by one\n  for (int i = start; i &lt;= end; i = i + 1) {\n    sum += i;\n  }\n  return sum;\n}\n\nint main() {\n  printf(\"Result: %d\\n\", sum_range(1, 100));\n  return 0;\n}\n</code></pre> <code>const</code> qualified variable cannot be modified. This may enable optimizations during compilation.</p>"},{"location":"lecture2/#c-programming-conditions","title":"C Programming - Conditions","text":"<p>Numbers of multiple of 3 inside \\([0, 99]\\) (i.e. \\(i \\mod 3 = 0\\)) <pre><code>void count_multiples_of_three() {\n  unsigned int count = 0;\n  // For i starting at 0; while i &lt; 100; increment i by one\n  for (unsigned int i = 0; i &lt; 100; i++) {\n    // if i % 3 (Remainder of the integer division) is equal to 0\n    if (i % 3 == 0) {\n      count++;\n    }\n  }\n  printf(\"Result: %d\\n\", count);\n}\n</code></pre></p>"},{"location":"lecture2/#note","title":"Note","text":"<p>Here we could also do <code>for (unsigned int i = 0; i &lt; 100; i += 3)</code></p>"},{"location":"lecture2/#c-programming-basic-pointers","title":"C Programming - Basic Pointers","text":"<p><pre><code>int a = 0;\nint b = 5;\n\nint* c = &amp;a;\n*c = *c + b;\nprintf(\"a: %d; b: %d; c: %d\\n\", a, b , *c);\n</code></pre> <code>c</code> contains the address of <code>a</code>; so <code>*c = *c + b</code> write in <code>a</code> the sum of <code>a</code> and <code>b</code>.</p> Adress Value Variable 0x004 0 a 0x008 5 b 0x00c 0x004 c ... ... ..."},{"location":"lecture2/#c-programming-arrays","title":"C Programming - Arrays","text":"<pre><code>int main() {\n  char morpion[9] = {'X', 'O',  '\\0',\n                     'O', 'X',  '\\0',\n                     'O', '\\0', '\\0'};\n  morpion[8] = 'X'; // The player clicked on the bottom-right cell !\n}\n</code></pre>"},{"location":"lecture2/#c-programming-structures","title":"C Programming - Structures","text":"<p>Structures are user-defined composite types:</p> <pre><code>typedef struct {\n  char* first_name;\n  char* last_name;\n  int age;\n  float mean_grade;\n  char gender;\n} Student;\n</code></pre> <pre><code>Student e1 = {\"Dupont\", \"Pierre\", 22, 13, 'm'};\nStudent e2 = {\"Major\", \"Major\", 22, 13.5, 'a'};\nStudent e3 = {\"Martin\", \"Evelynne\", 24, 14, 'f'};\n\nif (e1.mean_grade &gt; 10) {\n  printf(\"(%s %s) is a pretty good student !\\n\", \n         e1.first_name, e1.last_name);\n}\n</code></pre> <p>C has no concept of <code>class</code>, <code>object</code>, or <code>method</code> !</p>"},{"location":"lecture2/#c-programming-structures-2","title":"C programming - Structures 2","text":"<pre><code>void display_student(Student* s) {\n  // s-&gt;age is equivalent to (*s).age\n  printf(\"%s %s (%i): %f\\n\", s-&gt;first_name, \n                             s-&gt;last_name, s-&gt;age,\n                             s-&gt;mean_grade);\n}\n\n// We can have arrays of any types !\nStudent students[3] = {{\"Dupont\", \"Pierre\", 22, 13, 'm'}, ...};\nfor (int i = 0; i &lt; 3; i++)\n  display_student(&amp;students[i]);\n</code></pre>"},{"location":"lecture2/#c-programming-trading-abstraction-for-performance","title":"C Programming - Trading Abstraction for performance","text":""},{"location":"lecture2/#in-c-we-must-manually-take-care-of-very-low-level-concepts","title":"In C, we must manually take care of very low level concepts","text":"<ul> <li>We care about data layout, memory addresses, pointers, etc.</li> <li>The language doesn't provide linked lists, dynamic arrays, dictionaries, etc.</li> <li>No basic algorithms like sorting</li> </ul>"},{"location":"lecture2/#on-the-flip-side-we-can","title":"On the flip side, we can","text":"<ul> <li>Manually lay out data to maximize efficiency</li> <li>Remove any abstractions and overhead to maximize performance</li> <li>Generate code  that runs as close to the metal as possible</li> <li>Optimize our program for the hardware</li> </ul>"},{"location":"lecture2/#c-programming-trading-abstraction-for-performance-example","title":"C Programming - Trading Abstraction for performance (Example)","text":"<p>Consider the following python and C code:</p> <pre><code>sum = 0\nfor i in range(ub):\n  sum += i\nprint(sum)\n</code></pre> <pre><code>unsigned long long sum = 0;\nfor (unsigned int i = 0; i &lt; ub; i++){\n    sum += i;\n}\nprintf(\"Sum of first %llu integers is: %llu\\n\", ub, sum);\n</code></pre> <p>Where ub is a very large number (100 Millions in this example). Which one is faster, and by how much ?</p>"},{"location":"lecture2/#c-programming-trading-abstraction-for-performance-example_1","title":"C Programming - Trading Abstraction for performance (Example)","text":"<p>Results:</p> <ul> <li><code>C</code> version: 0.024s</li> <li><code>Python</code> version: 5.650s</li> </ul> <p>That's a speedup of \\(\\times 235\\). </p> <p>We will see later in this course how this is possible.</p>"},{"location":"lecture2/#numpy-and-other-libraries","title":"Numpy and other libraries","text":"<p>Note that we could use <code>numpy</code> or the <code>sum</code> python function: but those are actually implemented in <code>C</code> !</p>"},{"location":"lecture2/#managing-memory","title":"Managing Memory","text":""},{"location":"lecture2/#managing-memory-concept","title":"Managing Memory - Concept","text":""},{"location":"lecture2/#in-high-level-languages","title":"In High-level languages","text":"<ul> <li>We operate on abstracted data structures (lists, dictionaries, etc.)</li> <li>Memory is managed automatically (allocation, resizing, deallocation)</li> <li>We don't care about memory alignment, stack vs. heap, page size, Numa effects, etc.</li> </ul>"},{"location":"lecture2/#in-c","title":"In C","text":"<ul> <li>We perform directly with primitive data and raw memory</li> <li>We are responsible for allocation, layout, and cleanup</li> <li>We can only request chunks of raw memory, and fill it however we choose</li> <li>This is critical for performance</li> </ul> <p>This low level control is critical for performance; hence we must understand how memory works under the hood !</p>"},{"location":"lecture2/#managing-memory-memory-types","title":"Managing Memory - Memory Types","text":"<p>We can distinguish two types of memory</p> <ul> <li>Memory automatically allocated by the compiler on the stack.<ul> <li>Stores variables, functions arguments, etc.</li> <li>Fast but limited in size</li> </ul> </li> <li>Memory that is (manually) dynamically allocated on the heap <ul> <li>Must be allocated and freed by the developer !</li> </ul> </li> </ul> <p>The kernel (Linux / Windows) allocates memory pages and operates at a coarse grain level. The standard library (<code>libc</code>) manipulates pages on a finer scale and provides memory to the user.</p>"},{"location":"lecture2/#managing-memory-allocation","title":"Managing Memory - Allocation","text":"<pre><code>#include &lt;time.h&gt; // for time\n#include &lt;stdlib.h&gt; // For malloc,  srand, rand\n\nint do_the_thing(int n) {\n  // We allocate n numbers\n  float* numbers = malloc(sizeof(float) * n);\n\n  // We seed the random number generator\n  srand(time(NULL));\n\n  // We generate nsamples random numbers\n  for (int i = 0; i &lt; n; i++) {\n    numbers[i] = (float) rand() / RAND_MAX; // Generate a number in [0, 1]\n  }\n  ... // Do something complicated here\n  free(numbers); // Release memory back to the kernel\n  return 0;\n}\n</code></pre>"},{"location":"lecture2/#managing-memory-allocation_1","title":"Managing Memory - Allocation","text":"<p><code>malloc</code> returns a pointer to the beginning of the allocated memory range</p>"},{"location":"lecture2/#managing-memory-deallocation","title":"Managing Memory - Deallocation","text":"<p>Memory is not infinite ! </p> <p>In Python (and Java, C#, etc.); memory is managed by the garbage collector (GC):</p> <ul> <li>The runtime tracks all memory allocations; and all reference(s)</li> <li>When a memory block is not referenced by the program; the GC will release the memory back to the kernel.</li> </ul> <p>In C/C++, the user must deallocate memory using <code>free(ptr)</code>. </p>"},{"location":"lecture2/#memory-leak","title":"Memory leak","text":"<p>If memory is not freed (memory leak) the computer can run out:</p> <ul> <li>The kernel can kill the program</li> <li>The OS can crash</li> <li>Other applications requesting memory can crash or fail</li> </ul>"},{"location":"lecture2/#virtual-and-physical-memory-problem","title":"Virtual And Physical Memory - Problem","text":"<ul> <li>How can the kernel guarantee that memory is always contiguous?</li> <li>Can I acess memory from another program and steal their data?</li> <li>How can multiple applications share the same memory?</li> <li>Some variables have hard-coded addresses!</li> <li>How to handle (Internal/External) fragmentation (Empty slot)? </li> </ul>"},{"location":"lecture2/#virtual-and-physical-memory-concept","title":"Virtual And Physical Memory - Concept","text":"<p>We separate Physical Addresses (locations in memory) from Virtual Addresses (Logic locations) seen by each program !</p> <ul> <li>Physical memory is divided into small fixed-size blocks called pages (typically ~4KB).</li> <li>The CPU includes a Memory Management Unit (MMU) that translates virtual addresses into physical addresses.</li> <li>Each program is given its own isolated virtual address space.</li> <li>The kernel maintains a page table for each program that tells the MMU how to translate addresses.</li> </ul>"},{"location":"lecture2/#the-illusion-of-contiguity","title":"The Illusion of contiguity","text":"<p>Each process believes it has acces to a large, contiguous block of memory; while it can be physically fragmented or shared.</p>"},{"location":"lecture2/#virtual-and-physical-memory-diagram","title":"Virtual And Physical Memory - Diagram","text":"<p>*Note that this is a simplified representation.</p>"},{"location":"lecture2/#memory-hierarchy","title":"Memory Hierarchy","text":"<p>Which memory are we talking about ?</p> <p></p> <p>Note that GPU(s) also have their own separate memory !</p>"},{"location":"lecture2/#memory-hierarchy_1","title":"Memory Hierarchy","text":"<ul> <li>CPU computations are extremely fast, and memory access can be a bottleneck</li> <li>Registers have the lowest latency</li> <li>CPU caches (L1, L2, L3) act as fast buffers for memory</li> <li>DRAM (main memory) is much slower, but cheaper and larger</li> <li>Accessing DRAM causes significant delays compared to cache</li> </ul> <p>To achieve high performance, we must maximize data reuse in registers or caches, and minimize DRAM access.</p>"},{"location":"lecture2/#cpu-caches","title":"CPU Caches","text":"<p>Most CPU have 3 levels of cache</p> <ul> <li>L1d - First Level Cache (Very fast)</li> <li>L2 - Second Level Cache (Fast)</li> <li>L3 (Last Level Cache - LLC) (Larger but slower than L1/L2)</li> </ul> <p>Some cache level are per-core (L1, often L2) whereas others are shared between multiple cores (L3).</p>"},{"location":"lecture2/#instruction-cache","title":"Instruction Cache","text":"<p>The assembly instructions are stored in a separate (L1i) instruction cache</p>"},{"location":"lecture2/#cpu-caches_1","title":"CPU Caches","text":"<p>We speak of Heterogeneous Memory Hierarchy: the same memory accesses can have different latency depending on where the data resides !</p> <p>[Live example: LSTOPO]</p>"},{"location":"lecture2/#cpu-caches-in-practice","title":"CPU Caches - In practice","text":"<pre><code>for (int i = 0; i &lt; n; i++) {\n  T[i] = A[i] * B[i];\n}\n</code></pre> <ol> <li>The cache controller looks-up the data inside the CPU cache (L1 -&gt; L2 -&gt; L3)</li> <li>If available, data is sent to register for the ALU</li> <li>Else, a memory request is emitted</li> <li>This introduces latency and a bubble in the CPU pipeline</li> <li>When the memory request is resolved; execution resumes</li> <li>The results of \\(a*b\\) is written to cache, and eventually back to main memory later on.</li> </ol>"},{"location":"lecture2/#cpu-caches-in-practice_1","title":"CPU Caches - In practice","text":"<p>In practice:</p> <ul> <li>The CPU fetches entire cache line (Often 64 Bytes) at once (If float: \\(64B / 4B = 16\\) values at once) </li> <li>The CPU can prefetch data: it learns data access patterns and anticipates future memory access.</li> <li>The CPU can execute out-of-order; independent instructions are executed while the memory request is in flight.</li> </ul>"},{"location":"lecture2/#caches-cpu-strided-access","title":"Caches CPU - Strided Access","text":"<p>Consider two NBody 3D implementations:</p> <p>Array Of Structure (AoS)</p> <pre><code>// We allocate N tuples of (x, y, z) positions\nfloat* positions = malloc(sizeof(float) * N * 3);\n</code></pre> <p>Structure Of Array (SoA)</p> <pre><code>// We allocate separate arrays for each components\nfloat* x = malloc(sizeof(float) * N);\nfloat* y = malloc(sizeof(float) * N);\nfloat* z = malloc(sizeof(float) * N);\n</code></pre>"},{"location":"lecture2/#caches-cpu-strided-access_1","title":"Caches CPU - Strided Access","text":"<p>We want to record the number of particles with \\(x \\leq 0.5\\)</p> <p>Array Of Structure (AoS)</p> <pre><code>for (int i = 0; i &lt; N; i += 3)\n  if (positions[i] &lt; 0.5)\n    count++;\n</code></pre> <p>Structure Of Array (SoA)</p> <pre><code>for (int i = 0; i &lt; N; i++)\n  if (x[i] &lt; 0.5)\n    count++;\n</code></pre> <p>Which one is faster; and why ?</p> <p>Which access pattern makes better use of cache lines ?</p>"},{"location":"lecture2/#caches-cpu-strided-access_2","title":"Caches CPU - Strided Access","text":"<p><code>Perf</code> results summed across 100 runs:</p> Time # Instr # L1 Loads # L1 Miss # LLC Loads # LLC Miss AoS ~1.93s ~14 Billion ~3.5 Billion ~1 Million ~400k ~382k SoA ~1.75s ~14 Billion ~3.5 Billion ~300k ~24k ~15k # Cache references (LLC) # Cache miss AoS ~158 Million ~151 Million SoA ~52 Million ~35 Million <p>With AoS more load fail in the L1, leading to LLC accesses.</p> <p>Most LLC loads still results in misses, leading to DRAM access.</p>"},{"location":"lecture2/#compilation-assembly","title":"Compilation &amp; Assembly","text":""},{"location":"lecture2/#compilation-assembly-introduction","title":"Compilation &amp; Assembly - Introduction","text":"<p>C is a compiled language: we must translate the source code to assembly for the CPU</p> <p><code>gcc ./main.c -o main (&lt;flags&gt;)</code></p> <ul> <li>Python is interpreted</li> <li>More flexible but significantly slower</li> <li>C# and Java are compiled to intermediary bytecode and then executed via a virtual machine (or JIT-ed)</li> <li>Balances performance and productivity</li> <li>C/C++/Rust are compiled to assembly code</li> <li>Poor portability, but no intermediary.</li> </ul>"},{"location":"lecture2/#compilation-assembly-simple-loop","title":"Compilation &amp; Assembly - Simple Loop","text":"<pre><code>int sum = 0;\nfor (int i = 0; i &lt; 100000; i++){\n  sum += i;\n}\n</code></pre> <pre><code>main:\n.LFB6:\n    pushq   %rbp                 // We record the stack pointer\n    movq    %rsp, %rbp\n    movl    $0, -4(%rbp)         // Initialize sum\n    movl    $0, -8(%rbp)         // Initialize i\n    jmp .L2\n.L3:\n    movl    -8(%rbp), %eax       // Load sum to a register\n    addl    %eax, -4(%rbp)       // Add i and sum (from memory)\n    addl    $1, -8(%rbp)         // Add 1 to i (from memory)\n.L2:\n    cmpl    $99999, -8(%rbp)     // Check if i &lt; 100 000\n    jle .L3                      // Jump Less Equal\n    movl    $0, %eax             // Set the return value of main\n    popq    %rbp\n    ret                          // Return from main\n</code></pre> <p><code>gcc ./main.c -o main -OO</code></p>"},{"location":"lecture2/#compilation-assembly_1","title":"Compilation &amp; Assembly","text":"<p>Assembly is as close to the metal we usually get, and is architecture dependant:</p> <ul> <li>Intel and AMD use the x86 Instruction Set </li> <li>x86 has multiple extensions (FMA, sse, avx, avx512, etc.)</li> <li>To maximize performance, we should compile our applications on each platform</li> <li>Our binaries are not portable</li> <li>But we can use dedicated instructions</li> <li>Other instructions set exists (ARM, Risc V, etc.)</li> </ul>"},{"location":"lecture2/#compilation-assembly-optimization-passes","title":"Compilation &amp; Assembly - Optimization passes","text":"<p>The compiler is not just a translator:</p> <ul> <li>The compiler can generate optimized instructions from our program</li> <li>Constant values can be propagated, unused values/code removed</li> <li>Operations can be reordered, inlined, vectorized using SIMD, etc.</li> <li>Many, many more optimizations</li> </ul> <p>Those optimizations are enable through flags such as <code>-O1</code>, <code>-O2</code>, -<code>O3</code> which are predefined sets of optimization passes.</p> <p>The flag <code>-march=native</code> allows the compiler to target the current machine for compilation and use all the available ASM extensions.</p>"},{"location":"lecture2/#compilation-assembly-compiler-pipeline","title":"Compilation &amp; Assembly - Compiler Pipeline","text":"<p>There are several compilers with varying performance and features:</p> <ul> <li>GCC and Clang-LLVM (The classics)</li> <li>MSVC (Microsoft), mingw-LLVM, arm-clang (For ARM) and many, many others.</li> </ul>"},{"location":"lecture2/#makefile-basics-introduction","title":"Makefile Basics - Introduction","text":"<p>Make is a scripting tool to automate complex compilation workflows. It works by defining rules inside Makefiles.</p> <pre><code>CC := gcc\nCFLAGS := -g\n\nmain: main.c my_library.c my_library.h\n  $(CC) -o $@ $^ $(CFLAGS)\n</code></pre> <ul> <li><code>main</code> is the target (What we want to build)</li> <li><code>main.c my_library.c my_library.h</code> are the dependencies: rule reruns if any change</li> <li><code>$(CC) -o $@ $&lt; $(CFLAGS)</code> is the recipe</li> <li><code>$@</code> expands to the target name</li> <li><code>$^</code> expands to all dependency</li> </ul>"},{"location":"lecture2/#makefile-basics-phony-rules","title":"Makefile Basics - Phony rules","text":"<p>Makefiles expects that a rule <code>main</code> produces a file called <code>main</code>. However, not all rules produce files:</p> <pre><code>.PHONY: all clean\n\nall: main mylibrary\n\n...\n\nclean:\n  rm -rf *.o\n  rm -rf ./main\n</code></pre> <p>Here, <code>make all</code> will be an alias to build everything, while <code>make clean</code> is a custom rule to clean all build artifacts. Makefile has many, many other functionalities, outside the scope of this course.</p>"},{"location":"lecture2/#makefile-basics-usage","title":"Makefile Basics - Usage","text":"<p>The typical projects looks something like:</p> <pre><code>Project/\n  src/\n    main.c\n    my_library.c\n  include/\n    my_library.h\n  Makefile # We define the Make rules here\n</code></pre> <p><code>make</code> will look for a file in the <code>cwd</code> named <code>Makefile</code> or <code>makefile</code>. You can directly call <code>make all</code>, <code>make clean</code>, etc.</p>"},{"location":"lecture2/#parallelism-basics","title":"Parallelism Basics","text":""},{"location":"lecture2/#parallelism-basics-introduction","title":"Parallelism Basics - Introduction","text":"<p>Compiler optimization is only one side of high peformance computing.</p> <p>If you remember; we saw in <code>LSTOPO</code> that our CPU has many cores:</p> <ul> <li>Every core can perform computations independently of the other</li> <li>Multiple process (Google, vscode, firefox, excel) can run simultaneously on different cores.</li> <li>The kernel manages execution through thread scheduling and time-slicing</li> </ul>"},{"location":"lecture2/#main-thread","title":"Main Thread","text":"<p>Every process has at least one \"thread of execution\", which is an ordered sequence of instructions executed by the CPU.</p>"},{"location":"lecture2/#parallelism-basics-introduction_1","title":"Parallelism Basics - Introduction","text":"<p>What if we could split our programs into multiple threads ?</p> <ul> <li>If we have 1 thread only one computation happens at a time</li> <li>If we have 2 threads, we can potentially double throughput !</li> </ul> <p>In practice, there is some overhead, we must handle dependencies between instructions, etc.</p>"},{"location":"lecture2/#parallelism-basics-types-of-parallelism","title":"Parallelism Basics - Types of parallelism","text":"<p>We consider three main types of parallelism</p> <ul> <li>Single Instruction Multiple Data (SIMD): also called Vectorization</li> <li>single instruction operates simultaneously on multiple data elements.</li> <li>Shared Memory: Multiple threads inside the same memory space</li> <li>Threads share a memory space, enabling fast communication and synchronization.</li> <li>Distributed Memory: Multiple processes</li> <li>Communications are slower, but this model enables scaling across multiple machines.</li> </ul> <p>For this course, we will only focus on SIMD and Shared Memory parallelism.</p>"},{"location":"lecture2/#parallelism-basics-shared-memory","title":"Parallelism Basics - Shared Memory","text":"<p>Consider the following loop:</p> <pre><code>int sum = 0;\nfor (int i = 0; i &lt; 100; i++)\n  sum += i;\n</code></pre> <p>We can slice the iteration space in multiple chunks:</p> <p></p>"},{"location":"lecture2/#parallelism-basics-shared-memory_1","title":"Parallelism Basics - Shared Memory","text":"<p>We split the program into multiple instruction sequences running in parallel.</p> <ul> <li>Every thread operates a sum on a subset of the data</li> <li>We synchronize every thread and combine the partial sums via a global reduction.</li> </ul> <p><code>OpenMP</code> is an HPC tool designed for scenarios like this !</p> <p>It's a simple to use library/compiler pass to parallelize trivial loops.</p>"},{"location":"lecture2/#parallelism-basics-openmp","title":"Parallelism Basics- <code>OpenMP</code>","text":"<p><pre><code>int sum = 0;\n\n#pragma omp parallel for reduction(sum: +)\nfor (int i = 0; i &lt; 100; i++)\n  sum += i;\n</code></pre> <code>gcc ./main.c -fopenmp -O3 -march=native</code></p> <p>This directive automatically distributes the loop iterations across all available CPU cores,  performing a thread-safe reduction on sum.</p>"},{"location":"lecture2/#parallelism-basics-openmp-details","title":"Parallelism Basics - <code>OpenMP</code> details","text":"<p><code>OpenMP</code> defines a set of <code>clause</code> which are operations followed by a set of modifiers.</p> <ul> <li><code>#pragma omp</code>: is the start of all OpenMP clauses</li> <li><code>parallel:</code> enable the creations of multiple threads</li> <li><code>for</code>: toggle the automatic slicing of following loop</li> <li><code>reduction(sum: +)</code>: toggles a reductions clause for sum using the <code>+</code> operation.</li> </ul> <p>This code will be enough for most cases; but <code>OpenMP</code> allows for significantly more complex operations.</p>"},{"location":"lecture2/#parallelism-basics-advanced-openmp-example","title":"Parallelism Basics - Advanced <code>OpenMP</code> Example","text":"<pre><code>float global_min = FLT_MAX;\nint global_min_index = -1;\n#pragma omp parallel\n{\n  float min_value = FLT_MAX;\n  int min_index = -1;\n#pragma omp for nowait schedule(dynamic)\n  for (int i = 0; i &lt; N; i++) {\n    if (T[i] &lt; min_value) {\n      min_value = T[i];\n      min_index = i;\n    }\n  }\n#pragma omp critical\n  {\n    if (min_value &lt; global_min) {\n      global_min = min_value;\n      global_min_index = min_index;\n    }\n  }\n}\n</code></pre>"},{"location":"lecture2/#naive-nbody-3d-strong-scaling-setup","title":"Naive NBody 3D Strong Scaling - Setup","text":"<p>We increase the number of threads while keeping the work size constant.</p> <p><code>OMP_PLACES={0,2,4,6,8,10,12,14} OMP_PROC_BIND=True OMP_NUM_THREADS=8 ./nbody 10000</code> <code>sudo cpupower frequency-set -g performance</code></p> <p>5 Meta repetitions per run, 13th Gen Intel(R) Core(TM) i7-13850HX @5.30 GHz, 32KB/2MB/30MB:L1/L2/L3 15GB DDR5.</p>"},{"location":"lecture2/#naive-nbody-3d-strong-scaling-results","title":"Naive NBody 3D Strong Scaling - Results","text":"<p>Speedup is limited by runtime overhead, concurrency, memory bandwidth, data size, etc.</p>"},{"location":"lecture3/","title":"Building, Testing and Debugging Scientific Software","text":"<p> Download as slides \ud83d\udce5 </p>"},{"location":"lecture3/#objectives","title":"Objectives","text":"<ul> <li>Build systems: Advanced Makefiles, introduction to CMake for managing multi-file and multi-platform projects.</li> <li>Debugging: GDB, Valgrind for detecting memory errors and leaks.</li> <li>Software testing:<ul> <li>Principles: Unit testing, integration testing.</li> <li>Test frameworks in C (e.g., Unity).</li> <li>Importance of testing for regression prevention and validation.</li> </ul> </li> <li>Code documentation: Doxygen.</li> </ul>"},{"location":"lecture3/#makefiles","title":"Makefiles","text":""},{"location":"lecture3/#dependency-management","title":"Dependency Management","text":"<ul> <li>How to determine which files have changed?</li> </ul> <ul> <li>dependencies: <code>main.o</code> depends on changes in <code>lib.h</code></li> </ul>"},{"location":"lecture3/#makefile","title":"Makefile","text":"<ul> <li> <p>A <code>Makefile</code> uses a declarative language to describe targets and their dependencies.</p> </li> <li> <p>It is executed by the <code>make</code> command, which allows building different targets.</p> <ul> <li> <p><code>make</code> uses timestamps to determine which files have changed.</p> </li> <li> <p><code>make</code> evaluates rules recursively to satisfy dependencies.</p> </li> </ul> </li> </ul>"},{"location":"lecture3/#makefile-rule","title":"Makefile Rule","text":"<pre><code>prog: main.c lib.c lib.h\n  clang -o prog main.c lib.c -lm\n\ntarget: dependencies\n\\t  command to build the target from the dependencies\n</code></pre>"},{"location":"lecture3/#separate-compilation","title":"Separate Compilation","text":"<pre><code>prog: main.o lib.o\n  clang -o prog main.o lib.o -lm\n\nmain.o: main.c lib.h\n  clang -c -o main.o main.c\n\nlib.o: lib.c lib.h\n  clang -c -o lib.o lib.c\n</code></pre> <p>If <code>lib.c</code> is modified, which commands will be executed?</p>"},{"location":"lecture3/#phony-targets","title":"Phony Targets","text":"<p>You can add targets that do not correspond to a produced file. For example, it is useful to add a <code>clean</code> target to clean the project.</p> <pre><code>clean:\n  rm -f *.o prog\n.PHONY: clean\n</code></pre> <p><code>.PHONY</code> specifies that the <code>clean</code> rule should always be executed. Declaring all phony targets ensures they are always called (even if a file with the same name is created).</p>"},{"location":"lecture3/#default-rule","title":"Default Rule","text":"<pre><code>make clean\nmake prog\nmake\n</code></pre> <ul> <li>If <code>make</code> is called with a rule, that rule is built.</li> <li>If <code>make</code> is called without arguments, the first rule is built. It is customary to include a default <code>all:</code> rule as the first rule.</li> </ul> <pre><code>all: prog\n\nprog: ...\n</code></pre>"},{"location":"lecture3/#variables","title":"Variables","text":"<pre><code>CC=clang\nCFLAGS=-O2\nLDFLAGS=-lm\n\nprog: main.o lib.o\n  $(CC) -o prog main.o lib.o $(LDFLAGS)\n\nmain.o: main.c lib.h\n  $(CC) $(CFLAGS) -c -o main.o main.c\n\nlib.o: lib.c lib.h\n  $(CC) $(CFLAGS) -c -o lib.o lib.c\n</code></pre> <p>Variables can be overridden when calling <code>make</code>, e.g.,</p> <pre><code>make CC=gcc\n</code></pre>"},{"location":"lecture3/#special-variables","title":"Special Variables","text":"<p><code>$@</code>  target name <code>$^</code>  all dependencies <code>$&lt;</code>  first dependency</p> <pre><code>prog: main.o lib.o\n  $(CC)  -o $@ $^ $(LDFLAGS)\n\nmain.o: main.c lib.h\n  $(CC) $(CFLAGS) -c -o $@ $&lt;\n\nlib.o: lib.c lib.h\n  $(CC) $(CFLAGS) -c -o $@ $&lt; \n</code></pre> <p>The last two rules are very similar...</p>"},{"location":"lecture3/#implicit-rules","title":"Implicit Rules","text":""},{"location":"lecture3/#before","title":"Before","text":"<pre><code>main.o: main.c lib.h\n  $(CC) $(CFLAGS) -c -o $@ $&lt;\n\nlib.o: lib.c lib.h\n  $(CC) $(CFLAGS) -c -o $@ $&lt; \n</code></pre>"},{"location":"lecture3/#with-implicit-rule","title":"With Implicit Rule","text":"<pre><code>%.o: %.c\n  $(CC) $(CFLAGS) -c -o $@ $&lt;\n\nmain.o: lib.h\nlib.o: lib.h\n</code></pre>"},{"location":"lecture3/#other-build-systems","title":"Other Build Systems","text":"<ul> <li> <p>automake / autoconf: automatic generation of complex makefiles and management of system-specific configurations.</p> </li> <li> <p>cmake, scons: successors to Makefile, offering more elegant syntax and new features.</p> </li> </ul>"},{"location":"lecture3/#cmake","title":"CMake","text":""},{"location":"lecture3/#why-cmake","title":"Why CMake?","text":"<ul> <li> <p>Advantages of Makefiles:</p> <ul> <li>Simplicity and transparency.</li> <li>No additional tools required.</li> <li>Direct control over the build process.</li> </ul> </li> <li> <p>Advantages of CMake:</p> <ul> <li>Cross-platform support (Linux, Windows, macOS).</li> <li>Generates build files for multiple build systems (Make, Ninja, etc.).</li> <li>Modular and target-based design.</li> <li>Built-in support for testing, installation, and packaging.</li> </ul> </li> </ul>"},{"location":"lecture3/#general-design-of-cmake","title":"General Design of CMake","text":"<ul> <li> <p>CMake as a Meta-Build System:</p> <ul> <li>Generates build files for different generators (e.g., Make, Ninja).</li> <li>Abstracts platform-specific details.</li> </ul> </li> <li> <p>Workflow:</p> <ol> <li>Write <code>CMakeLists.txt</code> to define the project.</li> <li>Configure the project:</li> </ol> <pre><code>cmake -B build\n</code></pre> <ol> <li>Build the project:</li> </ol> <pre><code>cmake --build build\n# or when using Make as backend\nmake -C build\n</code></pre> </li> </ul> <p>Out-of-source builds are recommended to keep source directories clean.</p>"},{"location":"lecture3/#basic-structure-of-cmakeliststxt","title":"Basic Structure of <code>CMakeLists.txt</code>","text":"<pre><code>cmake_minimum_required(VERSION 3.15)\nproject(MyProject LANGUAGES C)\n\nset(CMAKE_C_STANDARD 11)\n</code></pre> <ul> <li><code>cmake_minimum_required</code>: Specifies the minimum version of CMake required.</li> <li><code>project</code>: Defines the project name and the programming language(s) used.</li> <li><code>set</code>: Sets variables, e.g., C standard version.</li> </ul>"},{"location":"lecture3/#adding-an-executable","title":"Adding an Executable","text":"<pre><code>add_executable(my_executable src/main.c)\n</code></pre> <ul> <li>Creates an executable named <code>my_executable</code>.</li> </ul>"},{"location":"lecture3/#adding-a-shared-library","title":"Adding a Shared Library","text":"<pre><code>add_library(my_library SHARED src/library.c)\n</code></pre> <ul> <li>Creates a shared library named <code>libmy_library.so</code> (on Linux).</li> </ul>"},{"location":"lecture3/#linking-libraries-to-executables","title":"Linking Libraries to Executables","text":"<pre><code>add_library(my_library SHARED src/library.c)\nadd_executable(my_executable src/main.c)\ntarget_link_libraries(my_executable PRIVATE my_library)\n</code></pre> <ul> <li><code>add_library</code>: Creates a shared library.</li> <li><code>add_executable</code>: Creates an executable.</li> <li><code>target_link_libraries</code>: Links the library to the executable.</li> </ul> <p>PRIVATE means that <code>my_executable</code> uses <code>my_library</code>, but <code>my_library</code> does not need to be linked when other targets link to <code>my_executable</code>.</p>"},{"location":"lecture3/#library-dependency-transitivity","title":"Library dependency transitivity","text":"<pre><code>add_library(libA SHARED src/libA.c)\nadd_library(libB SHARED src/libB.c)\ntarget_link_libraries(libB PUBLIC libA)\nadd_executable(my_executable src/main.c)\ntarget_link_libraries(my_executable PRIVATE libB)\n</code></pre> <ul> <li><code>my_executable</code> is linked to <code>libB</code> and also to <code>libA</code> because <code>libB</code> links to <code>libA</code> with <code>PUBLIC</code>.</li> <li>If <code>libB</code> linked to <code>libA</code> with <code>PRIVATE</code>, <code>my_executable</code> would not be linked to <code>libA</code>.</li> <li>If <code>libB</code> linked to <code>libA</code> with <code>INTERFACE</code>, <code>my_executable</code> would be linked to <code>libA</code> but not <code>libB</code>.</li> <li>See this reference for more details.</li> </ul>"},{"location":"lecture3/#global-include-directories","title":"Global Include Directories","text":"<pre><code>include_directories(include)\n</code></pre> <ul> <li>Adds the <code>include</code> directory globally for all targets.</li> <li>Limitation: Can lead to conflicts in larger projects.</li> </ul>"},{"location":"lecture3/#target-specific-include-directories","title":"Target-Specific Include Directories","text":"<pre><code>target_include_directories(my_library\n    PUBLIC include\n)\n</code></pre> <ul> <li>PUBLIC: Include directory is needed when building and using the library.</li> <li>PRIVATE: Include directory is needed only when building the library.</li> <li>INTERFACE: Include directory is needed only when using the library.</li> </ul>"},{"location":"lecture3/#porting-our-minimal-makefile-example-to-cmake","title":"Porting our minimal Makefile example to CMake","text":"<pre><code>cmake_minimum_required(VERSION 3.15)\nproject(MyProject LANGUAGES C)\n\n# Add the executable target\nadd_executable(prog main.c lib.c)\n\n# Specify include directories for the target\ntarget_include_directories(prog \n  PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})\n\n# Add compile options\ntarget_compile_options(prog PRIVATE ${CFLAGS})\n\n# Link libraries if needed\ntarget_link_libraries(prog PRIVATE m)\n</code></pre>"},{"location":"lecture3/#debug-vs-release-builds","title":"Debug vs Release Builds","text":"<ul> <li> <p>Debug Build:</p> <ul> <li>Includes debug symbols for debugging.</li> <li>Example flags: <code>-g</code>, <code>-O0</code>.</li> </ul> </li> <li> <p>Release Build:</p> <ul> <li>Optimized for performance.</li> <li>Example flags: <code>-O3</code>, <code>-DNDEBUG</code>.</li> </ul> </li> </ul>"},{"location":"lecture3/#setting-build-types-in-cmake","title":"Setting Build Types in CMake","text":"<pre><code>if(NOT CMAKE_BUILD_TYPE)\n  set(CMAKE_BUILD_TYPE RelWithDebInfo CACHE STRING \"Build type\" FORCE)\nendif()\n</code></pre> <ul> <li> <p>Build types: <code>Debug</code>, <code>Release</code>, <code>RelWithDebInfo</code>, <code>MinSizeRel</code>.</p> <ul> <li>CACHE: Makes the variable persistent across CMake runs. In out-of-source builds <code>CMakeLists.txt</code> is not re-evaluated on subsequent runs.</li> <li>FORCE: Overrides any previous value.</li> <li>STRING: \"Build type\" provides a description in CMake GUI.</li> </ul> </li> </ul>"},{"location":"lecture3/#adding-compiler-flags","title":"Adding Compiler Flags","text":"<pre><code>target_compile_options(my_library PRIVATE\n    $&lt;$&lt;CONFIG:Debug&gt;:-g -Wall&gt;\n    $&lt;$&lt;CONFIG:Release&gt;:-O3 -DNDEBUG&gt;\n)\n</code></pre> <ul> <li>Generator Expressions: <code>$&lt;CONFIG:Debug&gt;</code> applies flags only for Debug builds.</li> </ul>"},{"location":"lecture3/#installing-targets","title":"Installing Targets","text":"<pre><code>install(TARGETS my_library\n    LIBRARY DESTINATION lib\n    PUBLIC_HEADER DESTINATION include\n)\n</code></pre> <ul> <li>Installs the shared library to the <code>lib</code> directory.</li> <li>Installs public headers to the <code>include</code> directory.</li> </ul>"},{"location":"lecture3/#using-gnuinstalldirs","title":"Using GNUInstallDirs","text":"<pre><code>include(GNUInstallDirs)\n\ninstall(TARGETS my_library\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n)\n</code></pre> <ul> <li>Defines standard GNU library and include directories paths.</li> </ul>"},{"location":"lecture3/#generating-and-building-the-project","title":"Generating and Building the Project","text":"<ol> <li>Configure the Project:</li> </ol> <pre><code>cmake -B build\n</code></pre> <ul> <li> <p>Generates build files in the <code>build</code> directory.</p> </li> <li> <p>Build the Project:</p> </li> </ul> <pre><code>cmake --build build\n# or when using Make as backend\nmake -C build\n</code></pre> <ol> <li>Run the Program:</li> </ol> <pre><code>./build/my_executable\n</code></pre>"},{"location":"lecture3/#best-practices-for-cmake","title":"Best Practices for CMake","text":"<ul> <li> <p>Use Target-Based Commands:</p> <ul> <li>Prefer <code>target_include_directories</code> over <code>include_directories</code>.</li> <li>Prefer <code>target_link_libraries</code> over global linking.</li> </ul> </li> <li> <p>Organize <code>CMakeLists.txt</code>:</p> <ul> <li>Group related targets together.</li> <li>Use comments to explain sections.</li> </ul> </li> <li> <p>Use Modern CMake Features:</p> <ul> <li>Generator expressions for conditional configurations.</li> <li><code>FetchContent</code> for managing external dependencies.</li> </ul> </li> </ul>"},{"location":"lecture3/#debugging-tools","title":"Debugging Tools","text":""},{"location":"lecture3/#buggy-program-example","title":"Buggy program example","text":"<pre><code>/* Linked list of n = 5 nodes\n         .---------.    .---------.           .--------------.\n         | val = 4 |    | val = 3 |           | val = 0      |\n head -&gt; | next  --|--&gt; | next  --|--&gt; ... -&gt; | next =  NULL |\n         '---------'    '---------'           '--------------'\n */\n\n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n\nstruct Node\n{\n  int val;\n  struct Node *next;\n};\n\nint main()\n{\n  int n = 5;\n\n  struct Node *head = init_list(n);\n  // ... do something with the list ...\n  delete(head);\n\n  return 0;\n}\n</code></pre>"},{"location":"lecture3/#linked-list-initialization-and-deletion","title":"Linked List Initialization and Deletion","text":"<pre><code>struct Node *init_list(int n)\n{\n  struct Node *head = NULL;\n  for (int i = 0; i &lt; n; ++i)\n  {\n    struct Node *p = malloc(sizeof *p);\n    assert(p != NULL);\n    p-&gt;val = i;\n    p-&gt;next = head;\n    head = p;\n  }\n  return head;\n}\n\nvoid delete(struct Node *head)\n{\n  while (head)\n  {\n    struct Node *next = head-&gt;next;\n    free(head);\n    head = head-&gt;next;\n  }\n}\n</code></pre>"},{"location":"lecture3/#running-the-program","title":"Running the program...","text":"<pre><code>$ gcc -g -O0 -o buggy buggy.c\n$ ./buggy\nSegmentation fault (core dumped)\n</code></pre>"},{"location":"lecture3/#gdb-gnu-debugger","title":"GDB: GNU Debugger","text":"<ul> <li>Inspect the state of a program at the moment it crashes.</li> <li>Step through the code line by line.</li> <li>Inspect variables and memory.</li> <li>Set breakpoints to pause execution at specific lines.</li> </ul> <p>(Live demonstration)</p> <pre><code>$ gdb ./buggy\nProgram received signal SIGSEGV, Segmentation fault.\n0x000055555555522b in delete (head=0xa45d97b66d0683e8) at buggy.c:28\n28          struct Node *next = head-&gt;next;\n(gdb) x head\n0xa45d97b66d0683e8:     Cannot access memory at address 0xa45d97b66d0683e8\n</code></pre>"},{"location":"lecture3/#valgrind-memory-debugging-and-leak-detection","title":"Valgrind: memory debugging and leak detection","text":"<ul> <li>Detects memory leaks, invalid memory access, and uninitialized memory usage.</li> <li>Runs the code in a virtual sandbox that monitors every memory operation.</li> </ul> <p>(Live demonstration)</p> <pre><code>$ valgrind --leak-check=full ./buggy\n==537945== Invalid read of size 8\n==537945==    at 0x109243: delete (buggy.c:30)\n==537945==    by 0x109282: main (buggy.c:40)\n==537945==  Address 0x4a94188 is 8 bytes inside a block of size 16 free'd\n==537945==    at 0x484988F: free (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)\n==537945==    by 0x10923E: delete (buggy.c:29)\n==537945==    by 0x109282: main (buggy.c:40)\n==537945==  Block was alloc'd at\n==537945==    at 0x4846828: malloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)\n==537945==    by 0x1091B2: init_list (buggy.c:15)\n==537945==    by 0x109272: main (buggy.c:38)\n</code></pre>"},{"location":"lecture3/#other-tools-asan-ubsan","title":"Other tools: ASAN, UBSAN","text":"<ul> <li>AddressSanitizer (ASAN): Detects memory errors such as buffer overflows and use-after-free.</li> <li>UndefinedBehaviorSanitizer (UBSAN): Detects undefined behavior in C/C++ programs.</li> <li>Works on threaded programs and has lower overhead than Valgrind.</li> </ul> <p>(live demonstration) <pre><code>$ gcc -fsanitize=address -g -O0 -o buggy_asan buggy.c\n$ ./buggy_asan\n=================================================================\n==538335==ERROR: AddressSanitizer: heap-use-after-free on address 0x502000000098 at pc 0x5bec7c7343e9 bp 0x7ffdf3015150 sp 0x7ffdf3015140\nREAD of size 8 at 0x502000000098 thread T0\n    #0 0x5bec7c7343e8 in delete /home/poliveira/test-gdb/buggy.c:30\n    #1 0x5bec7c73442c in main /home/poliveira/test-gdb/buggy.c:40\n</code></pre></p>"},{"location":"lecture3/#software-testing","title":"Software Testing","text":""},{"location":"lecture3/#importance-of-software-testing","title":"Importance of Software Testing","text":"<ul> <li>1996: Ariane-5 self-destructed due to an unhandled floating-point exception, resulting in a $500M loss.</li> <li>1998: Mars Climate Orbiter lost due to navigation data expressed in imperial units, resulting in a $327.6M loss.</li> <li>1988-1994: FAA Advanced Automation System project abandoned due to management issues and overly ambitious specifications, resulting in a $2.6B loss.</li> <li>1985-1987: Therac-25 medical accelerator malfunctioned due to a thread concurrency issue, causing five deaths and numerous injuries.</li> </ul>"},{"location":"lecture3/#technical-debt","title":"Technical Debt","text":""},{"location":"lecture3/#software-costs","title":"Software Costs","text":""},{"location":"lecture3/#verification-and-validation-vv","title":"Verification and Validation (V&amp;V)","text":"<ul> <li> <p>Validation: Does the software meet the client's needs?  </p> <ul> <li>\"Are we building the right product?\"</li> </ul> </li> <li> <p>Verification: Does the software work correctly?  </p> <ul> <li>\"Are we building the product right?\"</li> </ul> </li> </ul>"},{"location":"lecture3/#approaches-to-verification","title":"Approaches to Verification","text":"<ul> <li>Formal methods</li> <li>Modeling and simulations</li> <li>Code reviews</li> <li>Testing</li> </ul>"},{"location":"lecture3/#testing-process","title":"Testing Process","text":""},{"location":"lecture3/#v-cycle-model","title":"V Cycle Model","text":""},{"location":"lecture3/#different-types-of-tests","title":"Different Types of Tests","text":"<ul> <li> <p>Unit Tests:</p> <ul> <li>Test individual functions in isolation.</li> <li>Test-driven development (TDD): Focus on writing maintainable, simple, and decoupled code.</li> </ul> </li> <li> <p>Integration Tests:</p> <ul> <li>Test the correct behavior when combining modules.</li> <li>Validate only functional correctness.</li> </ul> </li> <li> <p>Validation Tests:</p> <ul> <li>Test compliance with specifications.</li> <li>Test other characteristics: performance, security, etc.</li> </ul> </li> <li> <p>Acceptance Tests:</p> <ul> <li>Validate requirements with the client.</li> </ul> </li> <li> <p>Regression Tests:</p> <ul> <li>Ensure that fixed bugs do not reappear.</li> </ul> </li> </ul>"},{"location":"lecture3/#black-box-and-white-box-testing","title":"Black-Box and White-Box Testing","text":""},{"location":"lecture3/#black-box-testing-functional","title":"Black-Box Testing (Functional)","text":"<ul> <li>Tests are generated from specifications.</li> <li>Uses assumptions different from the programmer's.</li> <li>Tests are independent of implementation.</li> <li>Difficult to find programming defects.</li> </ul>"},{"location":"lecture3/#white-box-testing-structural","title":"White-Box Testing (Structural)","text":"<ul> <li>Tests are generated from source code.</li> <li>Maximizes coverage by testing all code branches.</li> <li>Difficult to find omission or specification errors.</li> </ul> <p>Both approaches are complementary.</p>"},{"location":"lecture3/#what-to-test","title":"What to Test?","text":"<ul> <li>Running the program on all possible inputs is too costly.</li> <li>Choose a subset of inputs:<ul> <li>Partition inputs into equivalence classes to maximize coverage.</li> <li>Test all code branches.</li> <li>Test edge cases.</li> <li>Test invalid cases.</li> <li>Test combinations (experimental design).</li> </ul> </li> </ul>"},{"location":"lecture3/#example-of-partitioning-13","title":"Example of Partitioning (1/3)","text":""},{"location":"lecture3/#specification","title":"Specification","text":"<pre><code>/* compare returns:\n *   0 if a is equal to b\n *   1 if a is strictly greater than b\n *  -1 if a is strictly less than b\n */\nint compare (int a, int b);\n</code></pre> <p>What inputs should be tested?</p>"},{"location":"lecture3/#equivalence-classes-23","title":"Equivalence Classes (2/3)","text":"Variable Possible Values a {positive, negative, zero} b {positive, negative, zero} result {0, 1, -1}"},{"location":"lecture3/#example-test-cases","title":"Example Test Cases","text":"a b result 10 10 0 20 5 1 3 7 -1 -30 -30 0 -5 -10 1 ... ... ... <p>It is possible to select a subset of classes!</p>"},{"location":"lecture3/#boundary-tests-33","title":"Boundary Tests (3/3)","text":"a b result -2147483648 -1 -1"},{"location":"lecture3/#discussion","title":"Discussion","text":"<ul> <li>Automatic test generation.</li> <li>Test coverage calculation.</li> <li>Mutation testing.</li> <li>Fuzzing.</li> <li>Importance of using automated testing tools.</li> <li>Importance of using continuous integration tools.</li> </ul>"},{"location":"lecture3/#unity-test-framework","title":"Unity Test Framework","text":""},{"location":"lecture3/#introduction-to-unity","title":"Introduction to Unity","text":"<p>Unity Test Framework</p> <ul> <li>Lightweight and simple unit testing framework for C.</li> <li>Designed for embedded systems but can be used in any C project.</li> <li>Provides a set of macros and functions to define and run tests.</li> </ul>"},{"location":"lecture3/#setting-up-unity","title":"Setting Up Unity","text":"<ul> <li> <p>Separate Unity tests into a separate directory, e.g., <code>tests/</code>. </p> </li> <li> <p>Include the Unity header in your test files:</p> </li> </ul> <pre><code>#include \"unity.h\"\n</code></pre> <ul> <li>Requires linking against the Unity library</li> <li>We will link against a static library <code>libunity.a</code>, since Unity uses CMake, we will use FetchContent to add it to our projects.</li> </ul>"},{"location":"lecture3/#writing-tests","title":"Writing Tests","text":"<ul> <li>test_functions use <code>TEST</code> macros provided by Unity to assert conditions.</li> </ul> <pre><code>void test_function_name(void) {\n    ...\n    TEST_ASSERT_EQUAL_INT(expected, actual);\n    TEST_ASSERT_NOT_NULL(pointer);\n    TEST_ASSERT_TRUE(condition);\n    ... \n}\n</code></pre> <ul> <li>Reference for all assertions: Unity Assertions</li> </ul>"},{"location":"lecture3/#example-testing-our-linked-list","title":"Example: testing our linked list","text":"<pre><code>#include \"unity.h\"\n#include \"buggy.h\"\nvoid test_delete_single_node(void) {\n    struct Node *head = init_list(1); \n    TEST_ASSERT_NOT_NULL(head); // head should not be NULL\n    TEST_ASSERT_EQUAL_INT(0, head-&gt;val); // head should be 0\n    delete(head); // should not crash\n    TEST_ASSERT_NULL(head); // head should be NULL after deletion\n}\nvoid test_delete_multiple_nodes(void) {\n    struct Node *head = init_list(5);\n    TEST_ASSERT_EQUAL_INT(4, head-&gt;val); // head should be 4\n    TEST_ASSERT_EQUAL_INT(3, head-&gt;next-&gt;val); \n    delete(head); // should not crash\n    TEST_ASSERT_NULL(head); // head should be NULL after deletion\n}\n</code></pre>"},{"location":"lecture3/#running-tests","title":"Running Tests","text":"<ul> <li>Create a test runner function to execute all tests:</li> </ul> <pre><code>int main(void) ## Boundary Tests{\n    UNITY_BEGIN();\n    RUN_TEST(test_function_name);\n    ...\n    return UNITY_END();\n}\n</code></pre>"},{"location":"lecture3/#setup-and-teardown","title":"SetUp and TearDown","text":"<ul> <li>SetUp and TearDown functions can be defined to run before and after each test.</li> </ul> <pre><code>void setUp(void) {\n    // Code to run before each test\n}\n\nvoid tearDown(void) {\n    // Code to run after each test\n}\n</code></pre>"},{"location":"lecture3/#code-coverage-with-unit-tests","title":"Code Coverage with unit tests","text":"<ul> <li>Use <code>gcov</code> or <code>llvm-cov</code> to measure code coverage of your tests.</li> <li>Compile your code with coverage flags:</li> </ul> <pre><code>gcc --coverage -g -O0 -o test_runner test_runner.c my_code.c -lunity\n</code></pre> <ul> <li> <p>gcov instruments the basic blocks of code to record what is executed during tests.</p> </li> <li> <p>gcovr generate HTML reports showing which parts of the code were covered by tests.</p> </li> </ul>"},{"location":"lecture3/#documentation-with-doxygen","title":"Documentation with Doxygen","text":"<ul> <li>Doxygen is a documentation generator for C, C++, and other languages.</li> <li>It extracts comments from the source code and generates documentation in various formats (HTML, LaTex, etc.).</li> <li>Use special comment blocks to document functions, parameters, return values, and more.</li> <li>Example of a documented function:</li> </ul> <pre><code>/**\n * @brief Initializes a linked list with n nodes.\n * @param n Number of nodes to create.\n * @return Pointer to the head of the linked list\n * @return NULL if memory allocation fails.\n */\nstruct Node *init_list(int n);\n</code></pre> <ul> <li>Generate documentation using the <code>doxygen</code> command with a configuration file (<code>Doxyfile</code>).</li> </ul>"},{"location":"lecture3/#credits-and-bibliography","title":"Credits and Bibliography","text":"<ul> <li>Course \"Automated Software Testing,\" S\u00e9bastien Bardin.</li> <li>CMake Tutorial</li> <li>CMake Best Practices</li> <li>Unity Test Framework</li> <li>Valgrind</li> <li>GDB</li> <li>ASAN/UBSAN</li> <li>Doxygen</li> </ul>"},{"location":"lecture4/","title":"Experimental Design, Profiling, and Performance/Energy Optimization","text":"<p> Download as slides \ud83d\udce5 </p>"},{"location":"lecture4/#plot-example-intro","title":"Plot Example - Intro","text":"<p>In the following slides, you will be shown a series of plots; mainly taken from the PPN course reports of previous students.</p> <p>For each plot:</p> <ul> <li>Try to understand what is represented</li> <li>Explain what you observe</li> <li>Give a definitive conclusion from the data shown</li> </ul> <p>Raise your hands when ready to propose an explanation.</p>"},{"location":"lecture4/#plot-example-1","title":"Plot Example (1)","text":"<p>PPN Example - (No Caption)</p>"},{"location":"lecture4/#plot-example-2","title":"Plot Example (2)","text":"<p>PPN Example - (No Caption)</p>"},{"location":"lecture4/#plot-example-3","title":"Plot Example (3)","text":"<p>PPN Example - (No Caption)</p>"},{"location":"lecture4/#plot-example-4","title":"Plot Example (4)","text":"<p>PPN Example - \"R\u00e9capitulatif des optimisations faites\"</p>"},{"location":"lecture4/#plot-example-5","title":"Plot Example (5)","text":"<p>PPN Example - \"Nouveau trac\u00e9 de la latence cache\"</p>"},{"location":"lecture4/#plot-example-6","title":"Plot Example (6)","text":"<p>Prof Example - (KNM): (a) Speedup map of GA-Adaptive (7k samples) over the Intel MKL hand-tuning for <code>dgetrf</code> (LU), higher is better. (b) Analysis of the slowdown region (performance regression). (c) Analysis of the high speedup region. \\(3,000\\) random solutions were evaluated for each distribution.</p>"},{"location":"lecture4/#plot-example-7","title":"Plot Example (7)","text":"<p>Prof Example - (SPR): Geometric mean Speedup (higher is better)  against the MKL reference configuration on <code>dgetrf</code> (LU), depending on the sampling algorithm. 46x46 validation grid. 7k/15k/30k denotes the samples count. GA-Adaptive outperforms all other sampling strategies for auto-tuning. With 30k samples it achieves a mean speedup of \\(\\times 1.3\\) of the MKL dgetrf kernel.</p>"},{"location":"lecture4/#plot-example-summary","title":"Plot Example - Summary","text":"<p>HPC is a scientific endeavour; data analysis and plotting are first class citizens.</p> <ul> <li>Plots drive decisions</li> <li>Plots make results trustworthy</li> <li>Plots explain complex behaviors</li> </ul> <p>Datasets are large, multi-disciplinary, and often hard to reproduce.</p>"},{"location":"lecture4/#plot-example-what-makes-a-good-plot","title":"Plot Example - What makes a good plot","text":"<p>Ask yourself:</p> <ul> <li>Who am I speaking to ?</li> <li>What's my narrative?</li> <li>Is my plot understandable in ~10 seconds?</li> <li>Is my plot self-contained?</li> <li>Is the context, environment, and methodology clear?</li> </ul>"},{"location":"lecture4/#experimental-methodology","title":"Experimental Methodology","text":""},{"location":"lecture4/#experimental-methodology-workflow","title":"Experimental Methodology - Workflow","text":""},{"location":"lecture4/#statistical-significance-introduction","title":"Statistical significance - Introduction","text":"<p>Computers are noisy, complex systems:</p> <ul> <li>Thread scheduling is non deterministic -&gt; runtime varies between runs.</li> <li>Dynamic CPU frequency (Turbo/Boost)</li> <li>Systems are heterogeneous (CPU/GPU, dual socket, numa effects, E/P cores)</li> <li>Temperature/thermal throttling can alter runtime</li> </ul> <p>How can we make sure our experimental measurements are reliable and conclusive?</p>"},{"location":"lecture4/#statistical-significance-warm-up-effects","title":"Statistical significance - Warm-up effects","text":"<p>Systems need time to reach steady-state:</p> <p></p> <p>On a laptop: \\(\\mathrm{Mean} = 0.315\\ \\mathrm{ms},\\ \\mathrm{CV} = 13.55\\%\\) </p> <p>We need \"warm-up\" iterations to measure stable performance and skip cold caches, page faults, frequency scaling.</p>"},{"location":"lecture4/#statistical-significance-noise-mitigation","title":"Statistical significance - Noise mitigation","text":"<p>Noise can only be mitigated:</p> <ul> <li>Stop all other background processes (other users)</li> <li>Stabilize CPU Frequency (<code>sudo cpupower -g performance</code>)<ul> <li>Make sure laptops are plugged to avoid powersaving policies</li> </ul> </li> <li>Pin threads via <code>taskset</code>, <code>OMP_PLACES</code> and <code>OMP_PROC_BIND</code></li> <li>Consider hyperthreading</li> <li>Use stable compute nodes</li> </ul> <p>Meta-repetitions are essential to mitigate noisy measurements.</p>"},{"location":"lecture4/#statistical-significance-example","title":"Statistical significance - Example","text":"<p>Same experiment on a stabilized benchmarking server:</p> <p></p> <p>On a laptop: \\(\\mathrm{Mean} = 0.315\\ \\mathrm{ms},\\ \\mathrm{CV} = 13.55\\%\\) Stabilized node: \\(\\mathrm{Mean} = 0.582\\ \\mathrm{ms},\\ \\mathrm{CV} = 1.14\\%\\)</p>"},{"location":"lecture4/#note","title":"Note","text":"<p>Timing on a laptop is always subpar</p>"},{"location":"lecture4/#statistical-significance-mean-median-variance","title":"Statistical significance - Mean, Median, Variance","text":"<p>Single-run measurements are misleading; we need statistics.</p> <ul> <li>Mean runtime \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\)</li> <li>Median: less sensitive to outliers than the mean</li> <li>Variance/standard deviation: Measure of uncertainty</li> <li>Relative metrics are useful: Coefficient of variation (\\(CV = \\frac{\\sigma}{\\bar{x}} \\times 100 \\%\\))</li> </ul> <p>We usually give both the mean and standard deviation when giving performance results. Plots usually show \\(\\bar{x} \\pm 1 \\sigma\\) as a shaded region around the mean to represent uncertainty.</p>"},{"location":"lecture4/#note_1","title":"Note","text":"<p>Distribution plots can be useful: stable measurements are often close to Gaussian,    even if systematic noise may lead to skewed or heavy-tailed distributions.</p>"},{"location":"lecture4/#statistical-significance-confidence-intervals","title":"Statistical significance - Confidence Intervals","text":"<p>Plotting variance/uncertainty through confidence intervals can change interpretation.  </p>"},{"location":"lecture4/#statistical-significance-confidence-intervals_1","title":"Statistical significance - Confidence Intervals","text":"<p>How to decide how many repetitions we should perform ?</p> <ul> <li>Usually, the costlier the kernels, the less meta-repetitions are expected</li> <li>Short or really short kernels should have more metas to reduce the influence of noise</li> </ul> <p>Remember that:</p> \\[CI_{0.95} \\approx \\bar{x} \\pm 1.96 \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] <p>More repetitions increase confidence, but returns diminish: CI width \\(\\propto \\tfrac{1}{\\sqrt{n}}\\)</p>"},{"location":"lecture4/#note_2","title":"Note","text":"<p>Confidence intervals are a bit less common in plots than \\(\\pm 1 \\sigma\\) but can also be used !</p>"},{"location":"lecture4/#statistical-significance-p-score-hypothesis-testing","title":"Statistical significance - p-score &amp; Hypothesis testing","text":"<p>In HPC, mean/median and variance often suffice, but hypothesis testing can become handy in some contexts.</p> <ul> <li>Null hypothesis (\\(H_0\\)): GPU and CPU have the same performance for small matrixes<ul> <li>Differences in measurements are only due to noise</li> </ul> </li> <li> <p>Alternative hypothesis: CPU is faster for small matrixes</p> </li> <li> <p>p-value is the probability that \\(H_0\\) explains a phenomenon.</p> </li> <li>If \\(p &lt; 0.05\\), we can safely reject \\(H_0\\) (Statistically significant difference)</li> </ul> <p>Example: \\(\\bar{x}_{GPU} = 5.0 \\mathrm{s}\\), \\(\\sigma_{GPU} = 0.20\\), \\(\\bar{x}_{CPU} = 4.8 \\mathrm{s}\\), \\(\\sigma_{CPU} = 0.4\\), Two-sample t-test with 10 samples \\(p = 0.02\\).</p> <p>The measured differences between CPU and GPU execution time are statistically significant.</p>"},{"location":"lecture4/#experimental-methodology-reproducibility","title":"Experimental Methodology \u2013 Reproducibility","text":"<p>Reproducibility is a very hot topic (Reproducibility crisis in science):</p> <ul> <li>Data and protocols are first-class citizens: as important as the plots themselves  </li> <li>Transparency matters: make data, scripts, and parameters accessible  </li> <li>Enables others to verify, build on, and trust your results</li> </ul>"},{"location":"lecture4/#note_3","title":"Note","text":"<p>Beware of your mindset: your results should be credible and honest before being \"good\".</p> <p>\"Our results are unstable, we have yet to understand why, this is what we tried\"   is a completely valid answer</p>"},{"location":"lecture4/#plotting-tools","title":"Plotting Tools","text":""},{"location":"lecture4/#plotting-tools-cheetsheet","title":"Plotting tools - Cheetsheet","text":"Name Use pandas Storing and saving tabular data numpy Numerical arrays, manipulating data matplotlib Basic 2D plots, full control seaborn Statistical plots, higher-level API logging Logging experiment progress/results OpenCV Image processing, animations/videos ffmpeg Generating and encoding videos <p>Lookup the quick reference plotting gallery in the annex! Both <code>matplotlib</code> and <code>seaborn</code> provide extensive online galleries.</p> <p>[Live Example of the matplotlib gallery https://matplotlib.org/stable/gallery/index.html]</p>"},{"location":"lecture4/#plotting-tools-matplotlib","title":"Plotting tools - Matplotlib","text":"<p>Matplotlib is one of the most widely used plotting libraries. A figure is built hierarchically from nested elements:</p> <pre><code>- Figure (The canvas)\n  - (Subfigures)\n    - Axes (One or more subplots)\n      - Axis (x/y/z scales, ticks, labels)\n      - Artists (Lines, markers, text, patches, etc.)\n</code></pre> <ul> <li>Data is plotted using axes-level functions like <code>ax.plot</code>, <code>ax.histogram</code></li> <li>Customization occurs at both the Figure and Axes levels</li> <li>Complex multi plots layout occur at the Figure level</li> </ul>"},{"location":"lecture4/#plotting-tools-matplotlib_1","title":"Plotting tools - Matplotlib","text":""},{"location":"lecture4/#plotting-tools-matplotlib_2","title":"Plotting tools - Matplotlib","text":"<pre><code>import matplotlib.pyplot as plt\n\nx = [0, 1, 2, 3]\ny = [2.8, 5.7, 12.5, 14]\n\n# Create a new figure, single axis\n# Size is 8 inches by 8 inches, and constrained layout\nfig, ax = plt.subplots(figsize=(8, 8), layout=\"constrained\")\n\n# Plot a simple line\nax.plot(x, y, color=\"red\", label=\"My Algorithm\")\n\n# Customize the axes\nax.set_xlabel(\"Iteration\") # Name of the X axis\nax.set_ylabel(\"Time (s)\") # Name of the y axis\n# Title of the plot\nax.set_title(\"Evolution of Time with the number of iteration\")\n\nax.margins(0, 0) # Remove white spaces around the figure\nax.legend(loc=\"upper right\") # Draw the legend in the upper right corner\n\nfig.savefig(\"my_plot.png\", dpi=300) # Higher DPI -&gt; bigger image\nplt.close() # End the plot and release resources\n</code></pre>"},{"location":"lecture4/#plotting-tools-matplotlib-multi-axis","title":"Plotting tools - Matplotlib (Multi axis)","text":"<p>We can easily have multiple plots on the same figure:</p> <pre><code>nrows = 5, ncols = 1\nfig, axs = plt.subplots(5, 1, figsize(8 * ncols, 3 * nrows))\n\nax = axs[0]\nax.plot()\n...\n\nax = axs[1]\nax.plot()\n...\n\nfig.tight_layout() # Alternative to constrained layout\nfig.savefig(\"my_multiplot.png\", dpi=300)\n</code></pre> <p>Each axis is its own plot, with its own legend and artists.</p>"},{"location":"lecture4/#note_4","title":"Note","text":"<p>Use the reference (https://matplotlib.org/stable/api/index.html) and gallery (https://matplotlib.org/stable/gallery/index.html) extensively !</p>"},{"location":"lecture4/#plotting-tools-seaborn","title":"Plotting tools - Seaborn","text":"<p>Seaborn is an extension of Matplotlib dedicated to statistical visualization:</p> <p></p> <p>It's useful for histograms, bar charts, kdeplots, scatterplots, and is overall a very good companion library.</p>"},{"location":"lecture4/#plotting-tools-seaborn_1","title":"Plotting tools - Seaborn","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(...) # Read the dataframe from somewhere\n\nfig, ax = plt.subplots(figsize=(8, 8), layout=\"constrained\")\n\n# We must pass the axis to plot on as an argument\nsns.kdeplot(data=df, x=\"Time\", label=\"Algorithm\", color=\"red\", fill=True, ax=ax)\n\nax.set_title(\"Distribution of Execution time for the algorithm\")\nax.margins(0, 0)\nax.set_xlabel(\"Time (s)\", fontweight=\"bold\")\nax.set_ylabel(\"Density\", fontweight=\"bold\")\n\nax.set_xticks(np.linspace(df[\"Time\"].min(), df[\"Time\"].max(), 10)\n# Format the x axis ticks: `3.25s`\nax.xaxis.set_major_formatter(StrMethodFormatter(\"{x:.2f}s\"))\n\nfig.savefig(\"my_distribution.png\")\n</code></pre> <p>https://matplotlib.org/stable/gallery/ticks/tick-formatters.html</p>"},{"location":"lecture4/#profiling","title":"Profiling","text":""},{"location":"lecture4/#profiling-motivation","title":"Profiling - Motivation","text":"<ul> <li>HPC codes are massive, complex and heterogeneous</li> <li>Humans are bad at predicting bottlenecks </li> <li>Don\u2019t blindly optimize everything</li> <li>Profiling guides optimization</li> </ul> <p>Remember: Always profile first.</p>"},{"location":"lecture4/#profiling-amdahls-law","title":"Profiling - Amdahl's law","text":"\\[ \\mathrm{Speedup} = \\frac{1}{1 - f + \\frac{f}{S}} \\] <p>Where f is the fraction of program improved, and S is the speedup on that fraction.</p> <p>Example:</p> <ul> <li>I have optimized 80% of my application, with a speedup of x10</li> <li>In total, my application is now \\(\\frac{1}{0.2 + (0.8 / 10)} = 3.57 \\times\\) faster</li> </ul> <p>The 20% are a bottleneck !</p>"},{"location":"lecture4/#profiling-steps","title":"Profiling - Steps","text":"<ol> <li>Where (Hotspots) ?<ul> <li>What functions are we spending time/energy in ?</li> <li>What call-tree are we spending time/energy in ?</li> </ul> </li> <li>Why ?<ul> <li>Arithmetic density, memory access patterns</li> <li>Cache misses, branch misspredictions, vectorization efficieny (Hardware counters)</li> </ul> </li> <li>What goal ?<ul> <li>Should I optimize for speed ? For energy ? Memory footprint ?</li> <li>What about cold storage size/compression ?</li> <li>Do I have constraints (i.e. limited memory) ?</li> <li>Should I optimize or switch algorithm ?</li> </ul> </li> </ol>"},{"location":"lecture4/#profiling-time","title":"Profiling - Time","text":"<p>It's rather easy to benchmark a single function using a (high-resolution, monotonic) clock:</p> <pre><code>begin = time.now()\nmy_function()\nend = time.now()\nelapsed = end - begin\n</code></pre> <p>Quick-and-dirty way to profile part of my program.</p>"},{"location":"lecture4/#profiling-time-stability","title":"Profiling - Time (Stability)","text":"<p>But we have to account for noise:</p> <pre><code>for _ in range(NWarmup):\n  my_function()\n\ntimes = []\nfor i in range(NMeta):\n  begin = time.perf_counter()\n  my_function()\n  times.append((i, time.perf_counter() - begin))\n\ndf = pd.DataFrame(times, columns=[\"Iteration\", \"Time\"])\n\nmedian = np.median(df[\"Time\"])\nstd = np.std(df[\"Time\"])\nprint(f\"Time: {median} +/- {std}\")\n...\n# Plot through seaborn !\nsns.plot(data=df, x=\"Iteration\", y=\"Time\", ax=ax)\n...\n</code></pre> <p>We must check that our measures are acceptable!</p>"},{"location":"lecture4/#profilers-introduction","title":"Profilers - Introduction","text":"<p>Full application -&gt; Thousands of functions to measure !</p> <ul> <li>Profilers are tools to automate this</li> <li>Two main types:</li> <li>Sampling: Pause the program and log where the program is   (Costly functions -&gt; More samples !)</li> <li>Instrumentation: Modify the program to automatically add timers</li> </ul> <p>Profilers can also check for thread usage, vectorization, memory access, etc.</p>"},{"location":"lecture4/#perf-record","title":"Perf - Record","text":"<p>Linux Perf is a powerful and versatile profiler:</p> <pre><code>gcc ... -g -fno-omit-frame-pointer\nperf record -g -- ./mytransform ./pipelines/big.pipeline\nLoaded image: images/image1.bmp (3660x4875, 3 channels)\n[ perf record: Woken up 3 times to write data ]\n[ perf record: Captured and wrote 0.484 MB perf.data (2941 samples) ]\n\n\nperf report\n</code></pre> <p></p> <p>It's a great tool to quickly get call stacks with few dependencies.</p>"},{"location":"lecture4/#profiling-hardware-counters","title":"Profiling - Hardware counters","text":"<p>In reality, perf is not just a profiler !</p> <ul> <li>The Linux Perf API can be used to access many hardware counters</li> <li>Perf record is just one usage of perf</li> </ul> <p>Most CPUs/GPUs have hardware counters that monitors different events:</p> <ul> <li>Number of cycles</li> <li>Number of retired instructions</li> <li>Number of memory access</li> <li>RAPL energy counters</li> </ul>"},{"location":"lecture4/#profiling-perf-for-hardware-counters","title":"Profiling - Perf for Hardware counters","text":"<pre><code>perf stat -e cycles,instructions python3 ./scripts/run_bls.py ...\n...\n\n Performance counter stats:\n   749,352,412,722      cpu_core/cycles/                                                      \n 3,142,707,494,308      cpu_core/instructions/           #    4.19  insn per cycle               \n\n      32.363472139 seconds time elapsed\n     225.351168000 seconds user\n       0.111367000 seconds sys\n</code></pre> <ul> <li>4.19 instruction per cycle -&gt; Very good vectorization</li> <li>time elapsed -&gt; \"Wall clock time\"</li> <li>seconds user -&gt; CPU time in user space -&gt; \\(225 / 30 \\approx 7\\) threads !</li> </ul>"},{"location":"lecture4/#profiling-perf-for-hardware-counters_1","title":"Profiling - Perf for Hardware counters","text":"<pre><code>perf stat -e cache-references,cache-misses python3 ./scripts/run_bls.py ...\n...\n\n Performance counter stats:    \n       394,258,269      cpu_core/cache-references/                                            \n        36,823,151      cpu_core/cache-misses/           #    9.34% of all cache refs         \n\n      32.363472139 seconds time elapsed\n     225.351168000 seconds user\n       0.111367000 seconds sys\n</code></pre> <ul> <li>394,258,269 references to the LLC (On Intel)</li> <li>36,283,151 LLC misses -&gt; 9.3% miss rate </li> </ul>"},{"location":"lecture4/#profiling-perf-for-hardware-counters_2","title":"Profiling - Perf for Hardware counters","text":"<pre><code>perf stat -e branches,branch-misses python3 ./scripts/run_bls.py ...\n...\n\n Performance counter stats:    \n   761,974,570,065      cpu_core/branches/                                                    \n       248,674,718      cpu_core/branch-misses/          #    0.03% of all branches       \n\n      32.363472139 seconds time elapsed\n     225.351168000 seconds user\n       0.111367000 seconds sys\n</code></pre> <ul> <li>761,974,570,065 execution flow breaks (ifs, returns, loops, etc.)</li> <li>248,674,718 branch misses -&gt; Good branch prediction! (\\(0.9%\\) miss rate)</li> </ul>"},{"location":"lecture4/#perf-record-with-other-events","title":"Perf - Record with other events","text":"<p>We can also use <code>perf record</code> with other events !</p> <pre><code>perf record -e \"cache-references,cache-misses,branches,branch-misses\" -g -- ./mytransform ./pipelines/big.pipeline\nLoaded image: images/image1.bmp (3660x4875, 3 channels)\n[ perf record: Woken up 7 times to write data ]\n[ perf record: Captured and wrote 1.709 MB perf.data (10260 samples) ]\n\nperf report\n</code></pre> <p></p>"},{"location":"lecture4/#intel-vtune","title":"Intel VTune","text":"<p>Perf is a bit \"barebone\": many profilers build upon perf like Intel VTune</p> <p></p>"},{"location":"lecture4/#vtune-cpu-usage","title":"VTune - CPU Usage","text":""},{"location":"lecture4/#vtune-collections-mode","title":"VTune - Collections Mode","text":"<p>VTune has multiple collection mode:</p> <p></p>"},{"location":"lecture4/#vtune-hpc-performance","title":"VTune - HPC Performance","text":"<p>VTune has multiple collection mode:</p> <p></p>"},{"location":"lecture4/#other-profilers","title":"Other profilers","text":"<ul> <li>MAQAO is a profiler developped by the LIPARAD</li> <li>AMD, NVIDIA and ARM have their own profilers for their platforms</li> <li>And many, many others (likwid, gprof, etc.) </li> </ul> <p>Usually, we combine a \"quick\" profiler like gprof/perf record with a more indepth one when needed.</p>"},{"location":"lecture4/#profiling-energy","title":"Profiling - Energy","text":"<p>Energy is a growing concern:</p> <ul> <li>One HPC cluster consume millions of dollars in electricity yearly</li> <li>ChatGPT and other LLM are computationally intensive:</li> <li>Nvidia GPUs consumes lots of energy</li> </ul> <p>On the flip side, measuring energy is harder than measuring time.</p> <p>Many actors still focus on execution time only -&gt; Energy is perceived as \"Second rank\"</p>"},{"location":"lecture4/#profiling-rapl","title":"Profiling - RAPL","text":"<p>Running Average Power Limit (RAPL) is an x86 hardware counter that monitors energy consumption:</p> <ul> <li>Energy is tracked at different level</li> <li>Core, Ram, Package, GPU, etc.</li> <li>It does not account for secondary power consummers (Fans, Water cooling, etc.) </li> <li>RAPL is not event based: The entire machine is measured ! (Background processes, etc.)</li> </ul> <p>It requires sudo permissions to access (compared to a clock)</p> <pre><code>perf stat -a -j -e power/energy-pkg,power/energy-cores &lt;app&gt;\n{\"counter-value\" : \"88.445740\", \"unit\" : \"Joules\", \"event\" : \"power/energy-pkg/\", \"event-runtime\" : 10002168423, \"pcnt-running\" : 100.00}\n{\"counter-value\" : \"10.848633\", \"unit\" : \"Joules\", \"event\" : \"power/energy-cores/\", \"event-runtime\" : 10002166697, \"pcnt-running\" : 100.00}\n</code></pre>"},{"location":"lecture4/#profiling-watt-meter-yokogawa","title":"Profiling - Watt-Meter (Yokogawa)","text":"<p>Hardware solutions are also available to monitor energy consumption. They typically have a slow sampling resolution (\\(\\approx 1s\\)) and are harder to scale to entire clusters.</p> <p>On the flip side, they give precise power measurements compared to RAPL.</p>"},{"location":"lecture4/#profiling-rapl-accuracy","title":"Profiling - RAPL accuracy","text":"<p>In practice, RAPL underestimates power consumption, but trends are correctly matched.</p>"},{"location":"lecture4/#live-demo","title":"Live Demo","text":""},{"location":"lecture4/#experiment-example","title":"Experiment example","text":"<p>Annex/run_experiment.sh</p> <p>Annex/model_convergence.py</p>"},{"location":"lecture5/","title":"HPC for AI &amp; Environmental impact of computation","text":"<p> Download as slides \ud83d\udce5 </p>"},{"location":"lecture5/#introduction-to-ai-applications","title":"Introduction to AI applications","text":""},{"location":"lecture5/#ai-renaissance-neural-networks","title":"AI Renaissance: Neural Networks","text":"<ul> <li> <p>2012: AI renaissance brought by increased data     availability and computation ressources</p> <ul> <li>breakthroughs in multiple domains</li> <li>many innovations: algorithms, specialized processors, optimizations</li> </ul> </li> <li> <p>Most systems use neural networks:</p> <ul> <li>Training (stochastic gradient descent + backpropagation)</li> <li>Inference (forward pass)</li> </ul> </li> <li> <p>For both, the bottleneck is matrix multiplication</p> </li> </ul>"},{"location":"lecture5/#objectives","title":"Objectives","text":"<ul> <li>Explain why dense linear algebra (GEMM) dominates NN compute</li> <li>Core SGEMM kernel ideas and common optimizations</li> <li>Use Roofline model to identify bottlenecks</li> </ul>"},{"location":"lecture5/#short-introduction-to-neural-networks","title":"Short introduction to Neural Networks","text":"<ul> <li>Neural networks are composed of layers of neurons</li> <li>Each neuron computes a weighted sum of its inputs followed by a non-linear activation function \\(f\\)</li> </ul> \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\rightarrow \\textbf{neuron} \\rightarrow y \\] \\[ y = f\\left(\\sum_{i} w_i x_i + b\\right) \\] <ul> <li> <p>Common activation functions: ReLU, sigmoid, ...</p> </li> <li> <p>Perceptron: single layer of neurons (1958 Rosenblatt)</p> </li> </ul>"},{"location":"lecture5/#architectures","title":"Architectures","text":"<ul> <li> <p>Different architectures for different tasks:</p> <ul> <li>Fully connected layers</li> <li>Convolutional layers</li> <li>Recursive layers</li> <li>Transformers (attention mechanism)</li> </ul> <p></p> </li> </ul>"},{"location":"lecture5/#inference","title":"Inference","text":"<ul> <li>Inference: use the trained model to make predictions on new data</li> <li> <p>Forward pass through the network:</p> <ul> <li> <p>For each layer, compute the weighted sum and apply activation function</p> </li> <li> <p>The weighted sum is a matrix-vector multiplication for fully connected layers and convolutions (often implemented as GEMM).</p> </li> </ul> </li> </ul>"},{"location":"lecture5/#two-layer-network","title":"Two layer network","text":"<p>Layer 1:</p> <ul> <li>\\(X\\): input data [K \u00d7 B]  \u2192 K features, B batch size</li> <li>\\(W_1\\): weights [H \u00d7 K]  \u2192 H hidden units</li> <li>\\(b_1\\): bias [H \u00d7 1]</li> </ul> <p>Layer 2:</p> <ul> <li>\\(W_2\\): weights [O \u00d7 H]  \u2192 O outputs</li> <li>\\(b_2\\): bias [O \u00d7 1]</li> </ul> <p>ReLU \\(f(x) = max(0,x)\\), \\(f'(x) = 1_{x&gt;0}\\)</p>"},{"location":"lecture5/#forward-inference","title":"Forward inference","text":"<ul> <li>Layer 1 Pre-activation hidden (GEMM, H\u00d7K \u00d7 K\u00d7B \u2192 H\u00d7B)</li> </ul> \\[Z_1 = W_1 \u00b7 X + B_1\\] <ul> <li>Layer 1 Activation - ReLU (elementwise)</li> </ul> \\[H = f(Z_1)\\] <ul> <li>Layer 2 Output pre-activation (GEMM, O\u00d7H \u00d7 H\u00d7B \u2192 O\u00d7B)</li> </ul> \\[Z_2 = W_2 \u00b7 H + B_2\\] <ul> <li>Layer 2 Activation - ReLU (elementwise)</li> </ul> \\[Y = f(Z_2)\\] <ul> <li>Forward is dominated by the two large GEMMs Z1 and Z2.</li> </ul>"},{"location":"lecture5/#training","title":"Training","text":"<ul> <li>Training: adjust weights \\(W\\) and biases \\(b\\) to minimize a loss function \\(L\\) over a training dataset</li> <li> <p>Use backpropagation to compute gradients on each layer (chain rule)</p> </li> <li> <p>Example with one neuron and MSE loss:</p> </li> </ul> \\[ y = f(w_1 x_1 + w_2 x_2 + b) \\] \\[ L = (y - y_{true})^2 \\] \\[ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_1} = 2(y - y_{true}) \\cdot f'(w_1 x_1 + w_2 x_2 + b) \\cdot x_1 \\] <ul> <li>Backward pass can be efficiently implemented using automatic differentiation and matrix multiplications.</li> </ul>"},{"location":"lecture5/#stochastic-gradient-descent","title":"Stochastic Gradient Descent","text":"<ul> <li>Use stochastic gradient descent to update weights:</li> </ul> \\[ w_1 \\leftarrow w_1 - \\eta \\cdot \\frac{\\partial L}{\\partial w_1} \\] \\[ w_2 \\leftarrow w_2 - \\eta \\cdot \\frac{\\partial L}{\\partial w_2} \\] \\[ b \\leftarrow b - \\eta \\cdot \\frac{\\partial L}{\\partial b} \\] <ul> <li>\\(\\eta\\) is the learning rate</li> <li>Repeat for many epochs over the training dataset</li> </ul>"},{"location":"lecture5/#training_1","title":"Training","text":"<ol> <li>Forward pass to compute \\(H\\) and \\(Y\\)</li> <li>Compute loss \\(L(Y, Y_{true})\\)</li> <li>Backward pass to compute gradients.</li> </ol> <p>The backward pass is also dominated by GEMMs.</p>"},{"location":"lecture5/#frameworks","title":"Frameworks","text":"<ul> <li> <p>Popular frameworks: TensorFlow, PyTorch, JAX, ...</p> </li> <li> <p>High-level APIs for defining models, automatic differentiation, GPU acceleration</p> </li> </ul> <pre><code># Simple 2-layer NN in PyTorch\nimport torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return x\n</code></pre>"},{"location":"lecture5/#sgemm","title":"SGEMM","text":"<p>Single-precision General Matrix-Matrix multiplication (SGEMM):</p> \\[ RES = A \\times B + C \\] <p></p>"},{"location":"lecture5/#naive-sgemm-implementation-pseudocode","title":"Naive SGEMM implementation (pseudocode)","text":"<pre><code>// Initialize RES to C\nfor (i = 0; i &lt; M; i++)\n    for (j = 0; j &lt; N; j++)\n        RES[i][j] = C[i][j];\n\n// Matrix multiply\nfor (i = 0; i &lt; M; i++) {\n    for (j = 0; j &lt; N; j++) {\n        for (k = 0; k &lt; K; k++) {\n            RES[i][j] += A[i][k] * B[k][j];\n        }\n}\n</code></pre> <ul> <li>FLOPS: \\(2 \\times M \\times N \\times K\\)</li> <li>min. Memory: \\(4\\) bytes \\(\\times (M \\times K + K \\times N + M \\times N)\\)</li> </ul>"},{"location":"lecture5/#locality-issues-in-naive-sgemm","title":"Locality issues in naive SGEMM","text":"\\[ {\\color{green}\\text{order in memory} \\rightarrow} \\] \\[ \\begin{bmatrix} \\color{red} b_{11} &amp; b_{12} &amp; b_{13} &amp; b_{14} \\\\ \\color{red} b_{21} &amp; b_{22} &amp; b_{23} &amp; b_{24} \\\\ \\color{red} b_{31} &amp; b_{32} &amp; b_{33} &amp; b_{34} \\\\ \\color{red} b_{41} &amp; b_{42} &amp; b_{43} &amp; b_{44} \\\\ \\end{bmatrix} \\] <ul> <li> <p>Stride in accessing B (column-major)</p> <ul> <li>Poor spatial locality</li> <li>Difficult to vectorize</li> <li>Cache misses for large matrices (reuse distance too large)</li> </ul> </li> <li> <p>Low arithmetic intensity: \\(\\approx 0.5\\) FLOP/byte for large matrices</p> </li> </ul>"},{"location":"lecture5/#reordering-loops-ikj","title":"Reordering loops (i,k,j)","text":"<ul> <li>Sums <code>RES[i][j] += A[i][k] * B[k][j];</code> are independent \u2192 reorder loops:</li> </ul> <pre><code>for (i = 0; i &lt; M; i++) \n    for (k = 0; k &lt; K; k++) \n        for (j = 0; j &lt; N; j++) \n            RES[i][j] += A[i][k] * B[k][j];\n</code></pre> <ul> <li> <p><code>A[i][k]</code> does not depend on <code>j</code> \u2192 load once, reuse N times</p> </li> <li> <p><code>RES</code> and <code>B</code> accesses are now stride-1 (row-major)</p> </li> </ul> <pre><code>for (i = 0; i &lt; M; i++) \n    for (k = 0; k &lt; K; k++) {\n        const float temp = A[i][k];\n        for (j = 0; j &lt; N; j++) \n             RES[i][j] += temp * B[k][j];\n        }\n</code></pre> <ul> <li>Better spatial locality and easier to vectorize</li> </ul>"},{"location":"lecture5/#vectorization","title":"Vectorization","text":"<p>Inner loop assembly for (i,k,j) ordering with AVX (8 <code>float</code> in a vector):</p> <pre><code>.loop:                                   # Inner loop\n    vmovss  xmm0, DWORD PTR A[i][k]      # Load A[i][k]\n    vbroadcastss ymm0, xmm0              # Broadcast scalar to all lanes\n    vmovaps ymm1, YMMWORD PTR B[k][j]    # Load B[k][j:j+8]\n    vfmadd231ps ymm2, ymm1, ymm0         # Fused multiply-add\n    vmovaps YMMWORD PTR RES[i][j], ymm2  # Store RES[i][j:j+8]\n    add     j, 8                         # Increment j by 8 (vector width)\n    cmp     j, N                         # Compare j with N\n    jl      .loop                        # Loop if j &lt; N\n</code></pre>"},{"location":"lecture5/#problems-with-ikj-ordering","title":"Problems with (i,k,j) ordering","text":"<ul> <li> <p>Temporal locality analysis:</p> <ul> <li>GOOD: \\(A[i][k]\\) reused in the inner loop, reuse distance \\(1\\).</li> <li>MEDIUM : For a given \\((i,j)\\), each \\(RES[i][j]\\) revisited once per k. So reuse distance \\(K\\) (one full row).<ul> <li>To keep RES in cache between uses you would need cache \\(\\ge N \\times 4B\\)</li> </ul> </li> <li>BAD : For a given \\((k,j)\\), \\(B[k][j]\\) used once per i. So reuse distance \\(K \\times N\\) (entire B matrix).<ul> <li>To keep B in cache between uses you would need cache \\(\\ge K \\times N \\times 4B\\)</li> </ul> </li> </ul> </li> <li> <p>Still poor temporal locality for large matrices </p> </li> <li> <p>Solution: tiling / blocking to increase reuse</p> </li> </ul>"},{"location":"lecture5/#blocking-tiling","title":"Blocking (tiling)","text":"<ul> <li>Idea: operate on sub-matrices blocks that fit in cache</li> </ul> \\[  \\begin{bmatrix} \\textcolor{red}{A_{11}} &amp; A_{12} \\\\ A_{21} &amp; A_{22} \\\\ \\end{bmatrix} \\times \\begin{bmatrix} \\textcolor{blue}{B_{11}} &amp; B_{12} \\\\ B_{21} &amp; B_{22} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\textcolor{red}{A_{11}}\\textcolor{blue}{B_{11}} + A_{12}B_{21} &amp; A _{11}B_{12} + A_{12}B_{22} \\\\ A_{21}B_{11} + A_{22}B_{21} &amp; A_{21}B_{12} + A_{22}B_{22} \\\\ \\end{bmatrix} \\] <pre><code>#define BS 64 // Block size\n// Loop over blocks\nfor (ii = 0; ii &lt; M; ii += BS)\n    for (kk = 0; kk &lt; K; kk += BS)\n        for (jj = 0; jj &lt; N; jj += BS)\n\n            // Operate on blocks A[ii:ii+BS, kk:kk+BS],\n            // B[kk:kk+BS, jj:jj+BS], RES[ii:ii+BS, jj:jj+BS]\n            for (i = ii; i &lt; min(ii+BS, M); i++)\n                for (k = kk; k &lt; min(kk+BS, K); k++)\n                    for (j = jj; j &lt; min(jj+BS, N); j++)\n                        RES[i][j] += A[i][k] * B[k][j];\n</code></pre>"},{"location":"lecture5/#parallelization","title":"Parallelization","text":"<ul> <li>Each block operation is independent \u2192 parallelize over blocks</li> </ul> <pre><code>#pragma omp parallel for collapse(3)\nfor (ii = 0; ii &lt; M; ii += BS)\n    for (jj = 0; jj &lt; N; jj += BS)\n        for (kk = 0; kk &lt; K; kk += BS)\n            // Block multiplication as before\n</code></pre> <ul> <li>Each thread works on its own block \u2192 no false sharing</li> <li>Synchronization only at the end of the parallel region</li> <li>NUMA considerations: pin threads to cores, allocate memory close to threads</li> <li>Load balancing: static scheduling usually works well for large matrices</li> </ul>"},{"location":"lecture5/#libraries-autotuners","title":"Libraries &amp; autotuners","text":"<ul> <li> <p>Highly optimized SGEMM implementations exist:</p> <ul> <li> <p>OpenBLAS, Intel MKL for CPU</p> </li> <li> <p>NVIDIA cuBLAS for GPU</p> </li> </ul> </li> <li> <p>Implementations use blocking, vectorization, parallelization, and many architecture-specific optimizations</p> </li> <li> <p>Libraries are carefully tuned for different sizes and shapes of matrices.</p> </li> <li> <p>Autotuners (e.g., ATLAS, TVM, MLKAPS) can generate optimized code for specific hardware and problem sizes.</p> </li> </ul>"},{"location":"lecture5/#roofline-model-definitions","title":"Roofline model - Definitions","text":"<ul> <li> <p>Hypothesis: performance is limited by either compute or memory bandwidth</p> <ul> <li>performance: FLOP/s (vertical axis)</li> <li>memory bandwidth: Bytes/s</li> <li>arithmetic intensity: FLOP/byte (horizontal axis)</li> </ul> </li> <li> <p>Simple visual model to understand bottlenecks</p> </li> </ul>"},{"location":"lecture5/#roofline-model-bounds","title":"Roofline model - Bounds","text":"<ul> <li>Compute bound: horizontal line at peak FLOP/s</li> <li>Memory bound: sloped line with slope = memory bandwidth<ul> <li>\\(\\frac{\\text{Flop/s}}{\\text{Flop/Byte}} = \\text{Byte/s}\\) </li> </ul> </li> </ul>"},{"location":"lecture5/#roofline-model-sgemm-analysis","title":"Roofline model - SGEMM analysis","text":"<ul> <li>Interactive demonstration and analysis</li> </ul>"},{"location":"lecture5/#environmental-impact-of-computation","title":"Environmental impact of computation","text":""},{"location":"lecture5/#introduction","title":"Introduction","text":"<ul> <li> <p>Major ecological crisis: French roadmap targets carbon     neutrality in 2050 (Strat\u00e9gie Nationale Bas Carbone).</p> </li> <li> <p>Requires a 40% energy consumption reduction.</p> </li> <li> <p>HPC part of the solution: modeling and improving complex     systems</p> </li> </ul>"},{"location":"lecture5/#hpc-part-of-the-problem","title":"HPC part of the problem","text":"<ul> <li> <p>Frontier system at ORNL</p> <ul> <li> <p>More than \\(10^{18}\\) floating point operations per second</p> </li> <li> <p>Consumes 21MW: the energy of a small town (\\(16\\,000\\) french houses)</p> </li> </ul> </li> </ul> <p></p>"},{"location":"lecture5/#environmental-impact-of-computation_1","title":"Environmental impact of computation","text":"<ul> <li> <p>The ICT sector consumes \\(\\approx\\) 5% of the energy     wordwide</p> </li> <li> <p>It accounts for 1.8% - 2.8% of emitted GHG [Freitag, 2021]:</p> <ul> <li> <p>Accounts for embodied emissions.</p> </li> <li> <p>Shadow energy during the whole life-cycle: mining, fabrication, transportation, recycling.</p> </li> </ul> </li> <li> <p>GHG emmissions are only one of the sustainability issues</p> <ul> <li> <p>rare-earth mining and waste disposal (eg. Agbogbloshie).</p> <ul> <li>human-right abuses, health issues, pollution.</li> </ul> </li> </ul> </li> <li> <p>This presentation focus on energy consumption of HPC</p> </li> </ul>"},{"location":"lecture5/#what-about-renewable-energies","title":"What about renewable energies?","text":"<ul> <li> <p>Low-carbon electricity is a limited ressource</p> </li> <li> <p>Decarbonation \\(\\rightarrow\\) huge increase in electricity demand</p> <ul> <li> <p>Heating, Transportation, Industry</p> </li> <li> <p>Computing will compete for low-carbon electricity.</p> </li> </ul> </li> </ul>"},{"location":"lecture5/#energy-consumption-of-hpc","title":"Energy consumption of HPC","text":""},{"location":"lecture5/#evolution-of-processing-units-batten-2023","title":"Evolution of processing units [Batten, 2023]","text":""},{"location":"lecture5/#dennards-scaling-1970-2005","title":"Dennard's scaling 1970-2005","text":"\\[\\begin{aligned}         \\text{CMOS Power} &amp;  &amp; P = \\underbrace{1/2.C.V^2.f}_{P_{\\text{dynamic}}} + \\underbrace{V.I_{\\text{leak}}}_{P_{\\text{static}}} \\end{aligned}\\] <p>For each generation, transistors dimensions reduced by 30%,</p> <ul> <li> <p>Voltage and capacitance reduced by 30%</p> </li> <li> <p>Frequency increases: \\(\\times 1.4 \\approx 1/0.7\\)</p> </li> <li> <p>Surface halved: \\(0.5 \\approx 0.7 \\times 0.7\\)</p> </li> <li> <p>Power halved: \\(\\Delta P = 0.7 \\times 0.7^2 \\times 1/0.7 \\approx 0.5\\)</p> </li> </ul> <p>Power per surface unit remains constant but manufacturers double number of transistors and frequency increases:</p> <ul> <li> <p>Power efficiency doubles every 1.57 years</p> </li> <li> <p>Total power increases</p> </li> </ul>"},{"location":"lecture5/#multicore-2005-2020","title":"Multicore 2005-2020","text":"<ul> <li> <p>At current scale, leak currents start increasing     (\\(P_{\\textrm{static}} \\nearrow\\)). Power wall slows Dennard's scaling.</p> </li> <li> <p>Computing demand \\(\\rightarrow\\) parallelism and specialization.</p> </li> <li> <p>Number of cores increases exponentially since 2005.</p> </li> <li> <p>Power efficiency still improving:</p> <ul> <li> <p>selectively turning-off inactive transistors;</p> </li> <li> <p>architecture design optimizations;</p> </li> <li> <p>software optimizations.</p> </li> </ul> </li> </ul>"},{"location":"lecture5/#ai-accelerators-2020-2024","title":"AI Accelerators 2020-2024","text":"<ul> <li> <p>For domain specific applications, such as AI, specialized     accelerators are used</p> <ul> <li> <p>Memory and compute units tuned for a specific problem (matrix     multiplication) ;</p> </li> <li> <p>Faster and better power efficiency: GPU, TPU, FPGA, ASIC.</p> </li> </ul> </li> </ul>"},{"location":"lecture5/#analysis-of-top-100-hpc-systems","title":"Analysis of TOP-100 HPC systems","text":"<p>Efficiency and Peak computation exponential increase.</p>"},{"location":"lecture5/#rebound-effects","title":"Rebound effects","text":"<ul> <li> <p>In 1865, Jevons shows that steam engine improvements translate into     increased coal consumption.</p> </li> <li> <p>In HPC, efficiency gains contribute to the rising computation     demand.</p> <ul> <li>net increase of the total power consumption.</li> </ul> </li> <li> <p>Rebound effects for data-centers [Masanet, 2020]</p> <ul> <li>6% increase in energy consumption from 2010 to 2018\\     (255 % increase in nodes).</li> </ul> </li> <li> <p>Indirect rebound effects: computation advances can     contribute to the acceleration of other fields.</p> </li> </ul>"},{"location":"lecture5/#ai-energy-and-computation-costs","title":"AI energy and computation costs","text":""},{"location":"lecture5/#training-cost-doubles-every-34-months-openai-2020","title":"Training cost doubles every 3.4 months [OpenAI, 2020]","text":""},{"location":"lecture5/#should-we-study-training-or-inference","title":"Should we study training or inference?","text":"<ul> <li> <p>Training: huge cost but done once</p> <ul> <li> <p>GPT3, 175 billion parameters, \\(\\approx\\) 314 ZettaFLOP</p> </li> <li> <p>GPT4, 1.7 trillion parameters</p> </li> </ul> </li> <li> <p>Inference: millions of users and requests</p> <ul> <li>80-90% cost of a deployed AI system is spend on inference     [NVIDIA, 2019]</li> </ul> </li> </ul>"},{"location":"lecture5/#inference-cost-diminishing-returns-for-computer-vision","title":"Inference cost - Diminishing returns for computer vision","text":"<p>Exponential increase in compute for linear accuracy gain [Desislavov, 2023 / Schwartz, 2019]</p>"},{"location":"lecture5/#more-frugal-computing","title":"More frugal computing?","text":""},{"location":"lecture5/#smaller-precision-smaller-models-for-ai","title":"Smaller precision / Smaller models for AI","text":"<p>LLM success of smaller models (Llama, Chinchilla) fine-tuned for specific tasks with LoRA.</p>"},{"location":"lecture5/#tradeoff-model-complexity-cost-explainability","title":"Tradeoff: Model complexity - Cost - Explainability","text":"<ul> <li> <p>Inference cost grows with model complexity</p> </li> <li> <p>Simpler models are often more interpretable</p> <ul> <li>Traditional science also prefers simpler models</li> </ul> </li> <li> <p>DNN not necessary for all tasks</p> </li> </ul>"},{"location":"lecture5/#dvfs-study-of-lu-decomposition","title":"DVFS study of LU decomposition","text":"<ul> <li>Knights Mill 72 cores</li> <li>Intel MKL dgetrf</li> <li>\\(n \\in [1000,3000]\\)</li> <li>RAPL estimation</li> </ul> <p>(Thomas Roglin, M1 UVSQ/INTEL internship 2023)</p>"},{"location":"lecture5/#save-energy-by-computing-slower-1ghz","title":"Save energy by computing slower: 1GHz","text":""},{"location":"lecture5/#when-accounting-for-the-whole-system","title":"When accounting for the whole system","text":"<ul> <li>Model: RAPL + 40W</li> <li>System power dominates at low frequencies</li> </ul>"},{"location":"lecture5/#race-to-idle-26-ghz-compute-faster-and-turn-off-machine","title":"Race to idle: 2.6 GHz compute faster and turn off machine","text":""},{"location":"lecture5/#need-for-an-interdisciplinary-discussion","title":"Need for an interdisciplinary discussion","text":"<ul> <li> <p>AI / HPC can contribute towards sustainability (eg. acceleration of     weather forecast models) ...  but its energy cost must be reduced</p> </li> <li> <p>Efficiency:</p> <ul> <li> <p>Improve hardware and software</p> </li> <li> <p>Use smaller models / smaller precision</p> </li> </ul> <p>... subject to rebound effects</p> </li> <li> <p>Frugality in computing:</p> <ul> <li> <p>Balance computation cost vs. outcomes for each task</p> </li> <li> <p>Choose the right sized model</p> </li> <li> <p>Assess the environmental impact</p> </li> </ul> </li> </ul>"},{"location":"lecture5/#exemple-e-health-solution-in-tanzania-dacremont-2021","title":"Exemple: e-health solution in Tanzania [d'Acremont, 2021]","text":"<p>Treatment of febrile children illnesess in dispensaries.</p> <ul> <li> <p>IMCI: Paper-based decision tree WHO</p> </li> <li> <p>e-POCT CART tree tailored to real data on a standalone     tablet</p> <ul> <li> <p>Final CART tree easy to interpret and manually checked</p> </li> <li> <p>Randomized-trial \\(\\rightarrow\\) better clinical outcomes and     antibiotic prescription reduction</p> </li> </ul> </li> <li> <p>Sophisticated AI that continuously collects patient data and adapts     the algorithm ?</p> <ul> <li> <p>Increase in hardware and computation costs.</p> </li> <li> <p>Loss in explainability and verification of the algorithm.</p> </li> </ul> </li> </ul>"},{"location":"lecture5/#references-hpc-for-ai-applications","title":"References - HPC for AI applications","text":"<ul> <li>S. Boehm Optimizing, How to Optimize a CUDA Matmul Kernel</li> </ul>"},{"location":"lecture5/#references-environmental-impact-of-computation","title":"References - Environmental impact of computation","text":"<ul> <li> <p>Jones, Nicola (2018) \u2018How to stop data centres from gobbling up the world\u2019s electricity\u2019. Nature, 561(7722), pp. 163\u2013167.</p> </li> <li> <p>Freitag, Charlotte, Berners-Lee, Mike, Widdicks, Kelly, Knowles, Bran, et al. (2021) \u2018The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations\u2019. Patterns, 2(9), p. 100340. online</p> </li> <li> <p>Masanet, Eric, Shehabi, Arman, Lei, Nuoa, Smith, Sarah and Koomey, Jonathan (2020) \u2018Recalibrating global data center energy-use estimates\u2019. Science, 367(6481), pp. 984\u2013986.</p> </li> <li> <p>Schwartz, Roy, Dodge, Jesse, Smith, Noah A. and Etzioni, Oren (2019) \u2018Green AI\u2019. arXiv:1907.10597</p> </li> <li> <p>Amodei, Dario, Hernandez, Danny, Sastry, Girish, Clark, Jack, et al. (2018) \u2018AI and compute. OpenAI\u2019. https://openai.com/blog/ai-and-compute/</p> </li> <li> <p>D'Acremont presentation: https://youtu.be/oKcy_cY0QOw</p> </li> </ul>"},{"location":"annex/bash_cheatsheet/","title":"Bash Cheatsheet","text":"Goal Command Variants Create a directory <code>mkdir &lt;path&gt;</code> <code>mkdir -p &lt;path&gt;</code> to ignore errors Go inside a directory <code>cd &lt;path&gt;</code> <code>cd ..</code> to go up one level, <code>cd ~</code> to go to your home List all files <code>ls (&lt;path&gt;)</code> <code>ls -lah (&lt;path&gt;)</code> for pretty print with human-readable numbers. Show hidden files Print cwd <code>pwd</code> Convert to absolute path <code>realpath (&lt;path&gt;)</code> Print text <code>echo &lt;text&gt;</code> <code>echo $&lt;VARIABLE&gt;</code> to print a variable Redirect output to file <code>&gt;</code> Example: <code>echo \"Bonjour\" &gt; test.txt</code> Print file content <code>cat &lt;path&gt;</code> For big files: <code>less &lt;path&gt;</code> Delete a file <code>rm &lt;path&gt;</code> Delete a directory <code>rmdir &lt;path&gt;</code> Delete a non empty directory <code>rm -rf &lt;path&gt;</code> Create empty file <code>touch &lt;path&gt;</code> Copy a file <code>cp &lt;input&gt; &lt;output&gt;</code> <code>cp -r &lt;input&gt; &lt;output&gt;</code> to copy folders recursively"},{"location":"annex/install_fedora/","title":"Installing Fedora (Step-by-Step Guide)","text":"<p>This guide explains how to install Fedora and set up the required tools for this course. Following these steps will give you an environment suitable for the entire Master\u2019s program.</p> <p>Note that dual booting is not covered in this guide.</p>"},{"location":"annex/install_fedora/#requirements","title":"Requirements","text":"<p>To install Fedora, you will need to:</p> <ul> <li>Make a complete backup of all your data (Installation will permanently delete everything already on the computer.)</li> <li>Use a USB drive; around 10GB should be sufficient. Note that the drive will be completely erased.</li> <li>A working computer to write Fedora on USB.</li> </ul>"},{"location":"annex/install_fedora/#downloading-fedora","title":"Downloading Fedora","text":"<p>First, go to the Fedora website (fedoraproject.org), then download and install the Fedora Media Writer. Make sure you have plugged in your USB stick, then follow these steps:</p>"},{"location":"annex/install_fedora/#fedora-version","title":"Fedora Version","text":"<p>Here, I recommend choosing either Fedora Workstation or KDE Plasma Desktop:</p> <ul> <li>Fedora Workstation is the official GNOME version of Fedora: It is very stable and lightweight. The desktop is more akin to macOS.</li> <li>KDE Plasma Desktop is the official KDE Spin of Fedora. KDE is less stable and can be very heavy, but it is also more similar to Windows and its look-and-feel are easier to customize.</li> </ul> <p>Note that you can switch between the two without reinstalling Fedora, although it requires some additional steps.</p>"},{"location":"annex/install_fedora/#writing","title":"Writing","text":"<p>Here, you should make sure that:</p> <ul> <li>You select the correct hardware architecture for the target computer. It is most likely Intel/AMD 64-bit.</li> <li>Select your USB drive; here, <code>VendorCo</code> should be the name of your drive.</li> </ul> <p>Danger</p> <p>Remember that the USB drive will be completely erased when you press 'Download and Write'!</p> <p>Wait for the download to complete, then safely remove the USB stick and plug it into the computer on which you want to install Fedora.</p> <p>Danger</p> <p>Remember to back up your important files!</p>"},{"location":"annex/install_fedora/#booting-on-fedora","title":"Booting on Fedora","text":"<p>Now, you will need to boot from the Fedora drive to begin the installation.  When you start your device, you should see something akin to PRESS DEL OR F2 TO ENTER BIOS SETTING. Press the corresponding key when you see this screen until your BIOS appears. You can restart your computer if necessary.</p> <p>On some hardware, F12 or ESC may directly show a temporary boot menu.</p> <p>You should see something similar to this. Note that it may look very different, as this menu is hardware dependent.</p> <p>You should look for something named Boot Priorities or Boot Order, and modify the boot order so that the first item is either:</p> <ul> <li>The name of your USB Drive</li> <li>OR Something like \"Fedora Live Image\"</li> <li>OR \"Boot from USB\"</li> </ul> <p>Make sure to save the settings if necessary, then exit the BIOS. Do not modify anything else in this menu unless you know what you are doing.</p>"},{"location":"annex/install_fedora/#installing-fedora","title":"Installing Fedora","text":"<p>Now, your PC should reboot into Fedora. It is possible that a GRUB menu appears with multiple options; you should select the first one named \"Install Fedora\" or \"Start Fedora Workstation Live\".</p> <p>You should then select \"Install Fedora to Hard Drive\".</p> <p>Then:</p> <ul> <li>Select the language of your choice, although I recommend English (United States).</li> <li>Select the correct keyboard layout:<ul> <li>us for QWERTY</li> <li>fr for AZERTY</li> </ul> </li> <li>Click Next.</li> </ul> <p>Select the disk where you want to install Fedora, and make sure to select Use entire disk. Then click Next.</p> <p>We will skip encryption for this guide; simply click Next.</p> <p>The next page will summarize the changes and ask you to confirm before starting the installation. You may be asked to create a new user. Make sure to remember the password!</p> <p>Danger</p> <p>Make sure your device is plugged in if it is a laptop!</p> <p>When the installation is finished, power off the device, then remove the USB drive, and then power it on. Do not unplug the USB drive while the installation is ongoing.</p> <p>Congratulations, Fedora is now installed! You can log in with the user account you created and begin installing the required course packages.</p>"},{"location":"annex/oh-my-zsh/","title":"Installing oh-my-zsh","text":"<p>Zsh is a powerful shell that can replace Bash. It provides better auto-completion, improved globbing, and an extensible configuration system.</p> <p>oh-my-zsh is a community-driven framework for managing Zsh configuration. It comes with themes and plugins that make your console more productive.</p>"},{"location":"annex/oh-my-zsh/#1-install-zsh","title":"1) Install Zsh","text":"Fedora<pre><code>sudo dnf install zsh\n</code></pre> Ubuntu/Debian<pre><code>sudo apt update\nsudo apt install zsh\n</code></pre>"},{"location":"annex/oh-my-zsh/#2-install-oh-my-zsh","title":"2) Install oh-my-zsh","text":"<p>Run the official installation script <pre><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre></p> <p>Or look up the command at https://ohmyz.sh/#install</p>"},{"location":"annex/oh-my-zsh/#3-set-zsh-as-the-default-shell","title":"3) Set Zsh as the default shell","text":"<pre><code>chsh -s $(which zsh)\n</code></pre> <p>Note that you may have to log back in for changes to take effect.</p>"},{"location":"annex/oh-my-zsh/#4-add-plugins","title":"4) Add plugins","text":"<p>Open the <code>~/.zshrc</code> file and find this line: <pre><code>plugins=(git)\n</code></pre></p> <p>And change it to: <pre><code>plugins=(git dnf pip zsh-syntax-highlighting zsh-autosuggestions)\n</code></pre></p> <p>Then run Fedora<pre><code>sudo dnf install zsh-syntax-highlighting zsh-autosuggestions\n</code></pre></p>"},{"location":"annex/oh-my-zsh/#5-optional-set-up-powerlevel-10k","title":"5) Optional: Set up powerlevel-10k","text":"<p>powerlevel-10k is a nice theme for oh-my-zsh which I highly recommend.</p> <p>You can find up-to-date installation instructions at <code>https://github.com/romkatv/powerlevel10k</code>.</p>"},{"location":"annex/example_experiment/model_convergence/","title":"model_convergence.py","text":"Setup the experiment<pre><code># Compared to just using print(...), we can automatically report the timestamp\n# when we output !\ndef setup_logging(args):\n    logging.basicConfig(\n        filename=log_file,\n        level=logging.INFO,\n        format=\"(%(asctime)s)[%(levelname)s]: %(message)s\",\n    )\n\ndef parse_args():\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\"Experimental Pipeline\")\n    parser.add_argument(\n        \"-i\", \"--input\", type=Path, required=True, help=\"Path to training data\"\n    )\n    parser.add_argument(\n        \"-o\", \"--output\", type=Path, required=True, help=\"Output folder\"\n    )\n    parser.add_argument(\"--force\", action=\"store_true\", help=\"Prevent reloading existing data\")\n    args = parser.parse_args()\n\n    if not args.input.exists():\n        parser.error(f\"Input path {args.input} does not exist.\")\n\n    if not args.output.exists():\n        args.output.mkdir(parents=True, exist_ok=True)\n\n    return parser.parse_args()\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    setup_logging(args)\n    main(args)\n</code></pre> Main with optional reloading<pre><code>def main(args):\n    # Try to reload the data first\n    logging.info(\"Starting experimental pipeline\")\n    if not args.force and not args.output / \"raw/results.csv\".exists():\n        logging.info(\"Running experiment\")\n        results = run_experiment(args)\n        logging.info(\"Experiment completed and results saved\")\n    else:\n        logging.info(\"Loading existing results\")\n        results = pd.read_csv(args.output / \"raw/results.csv\")\n\n    plot(results)\n</code></pre> Data collection<pre><code>def run_experiment(args):\n    data = pd.read_csv(args.input)\n\n    train_x, test_x, train_y, test_y = train_test_split(\n        data.drop(columns=[\"target\"]), data[\"target\"], test_size=0.2, random_state=42\n    )\n    results = []\n    output_path = args.output / \"raw/results.csv\"\n\n    for perc in np.linspace(0.1, 1.0, 10):\n        sample_size = int(len(train_x) * perc)\n        sample_indices = np.random.choice(len(train_x), size=sample_size, replace=False)\n\n        X = train_x.iloc[sample_indices].copy()\n        y = train_y.iloc[sample_indices]\n\n        model = LGBMRegressor()\n        model.fit(X, y)\n\n        preds = model.predict(test_x)\n        mse = mean_squared_error(test_y, preds)\n\n        results.append((int(perc * len(data)), mse))\n        # We dump to file regularly so that we won't lose data if the script crashes\n        res = pd.DataFrame(results, columns=[\"nsamples\", \"mse\"])\n        res.to_csv(output_path)\n    return res\n</code></pre> Plotting<pre><code>def plot(args, results):\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    sns.lineplot(data=results, x=\"nsamples\", y=\"mse\", marker=\"o\", ax=ax)\n    ax.set_title(\"Evolution of MSE with the number of training samples\")\n\n    ax.set_xlabel(\"Number of Training Samples Used\")\n    ax.set_xticks(\n        np.linspace(results[\"nsamples\"].min()),\n            results[\"nsamples\"].max()\n            10\n    )\n\n    ax.set_ylabel(\"Mean Squared Error (MSE)\")\n    ax.set_yticks(\n        np.linspace(results[\"mse\"].min()),\n            results[\"mse\"].max()\n            10\n    )\n    ax.grid(\"y\")\n\n    ax.margins(0)\n    output_dir = args.output / \"results\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    fig.savefig(output_dir / \"results_plot.png\")\n    plt.close()\n</code></pre>"},{"location":"annex/example_experiment/run_experiment/","title":"run_experiment.sh","text":"<p>This Bash script will run an experiment and perform logging:</p> Argument parsing<pre><code>#!/bin/bash\n\n# We enable these flags so that\n# The script stops on error\nset -e\nset -o pipefail\n\nif [ \"$#\" -ne 2 ]; then\n    echo \"Usage: $0 &lt;input data&gt; &lt;output_dir&gt;\"\n    exit 1\nfi\n\nINPUT_DATA=$1\nOUTPUT_DIR=$2\nmkdir -p \"$OUTPUT_DIR\"\n</code></pre> Main experiment block<pre><code># We run the experiment inside a subshell so that we can log everything through tee\n(\n    echo \"Seeding environment...\"\n    source ./setup_env.sh\n\n    # We collect information about the system and dump them to the the output folder\n    # This acts as a \"label\" for the run\n    echo \"Collecting system information...\"\n    cp /proc/cpuinfo \"$OUTPUT_DIR/cpuinfo.txt\"\n    timestamp &gt;&gt; \"$OUTPUT_DIR/timestamp.txt\"\n    uname -a &gt;&gt; \"$OUTPUT_DIR/uname.txt\"\n    echo $(python3 --version) &gt; \"$OUTPUT_DIR/python_version.txt\"\n    echo $(pip freeze) &gt; \"$OUTPUT_DIR/requirements.txt\"\n\n    echo \"Running experiment...\"\n    python3 run_experiment.py -i \"$INPUT_DATA\" -o \"$OUTPUT_DIR/results\"\n\n) 2&gt;&amp;1 | tee \"$OUTPUT_DIR/raw_logs.log\"\n</code></pre>"}]}